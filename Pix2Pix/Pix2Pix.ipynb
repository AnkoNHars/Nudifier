{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perso import *\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import skimage.io\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageIterator():\n",
    "    def __init__(self, path, batch_size=32, shuffle = False, ext = '.png', max_n = 9999999):\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path\n",
    "        self.shuffle = shuffle\n",
    "        self._n = 0\n",
    "        self.max_n = getLastFromDir(path)\n",
    "        if self.max_n > max_n: self.max_n = max_n\n",
    "        self.ext = ext\n",
    "        self.epochs = 0\n",
    "    \n",
    "    def __next__(self):\n",
    "        data = []\n",
    "        for x in range(self.batch_size):\n",
    "            img = skimage.io.imread(self.path+'/{}'.format(self._n)+self.ext)\n",
    "            img = (img - 127.5) / 127.5\n",
    "            data.append(img)\n",
    "            self._n +=1\n",
    "            if self._n > self.max_n: \n",
    "                self._n = 0\n",
    "                self.epochs += 1\n",
    "        return np.asarray(data)\n",
    "class SiameseGanImageIterator():\n",
    "    def __init__(self,generator, back_path, persons_path, batch_size= 16,**kwargs):\n",
    "        \n",
    "        self.back_path = back_path\n",
    "        self.persons_path = persons_path \n",
    "        \n",
    "        self.max_n = min(getLastFromDir(back_path), getLastFromDir(persons_path))\n",
    "        self.generator = generator\n",
    "        self.batch_size = batch_size\n",
    "        self._n = 0\n",
    "        self.ext = '.png'\n",
    "    \n",
    "    def add_n(self):\n",
    "        self._n += 1 \n",
    "        if self._n > self.max_n:\n",
    "            self._n = 0\n",
    "        \n",
    "    def next(self):\n",
    "        backs = []\n",
    "        persons = []\n",
    "        for x in range(self.batch_size):\n",
    "            while True:\n",
    "                try:\n",
    "                    back = (skimage.io.imread(self.back_path+'/{}'.format(self._n)+self.ext) -127.5) / 127.5\n",
    "                    person = (skimage.io.imread(self.persons_path+'/{}'.format(self._n)+self.ext) - 127.5) / 127.5\n",
    "                    break\n",
    "                except: self.add_n()\n",
    "            backs.append(back)\n",
    "            persons.append(person)\n",
    "            self.add_n()\n",
    "        return np.asarray(persons), np.asarray(backs)\n",
    "            \n",
    "        \n",
    "    def get_valid_imgs(self):\n",
    "        persons, backs = self.next()\n",
    "        return [persons, backs]\n",
    "    \n",
    "    def get_fake_imgs(self):\n",
    "        persons, backs = self.next()\n",
    "        persons = self.generator.predict(backs)\n",
    "        return [persons, backs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2Pix():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 640\n",
    "        self.img_cols = 640\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'nude'\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        valid = self.discriminator([fake_A, img_B])\n",
    "\n",
    "        self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        self.combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=optimizer)\n",
    "        #self.combined_dressed = Model(inputs=img_B, outputs=valid)\n",
    "        #self.combined_dressed.compile(loss='mae', optimizer=optimizer)\n",
    "        \n",
    "        \n",
    "        print(self.generator.summary())\n",
    "        print(self.discriminator.summary())\n",
    "        print(self.combined.summary())\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True, strides=2):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=strides, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0, size=2):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=size)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "        \n",
    "        \"\"\"d5 = d_layer(combined_imgs, self.df, bn=False, f_size=2)\n",
    "        d6 = d_layer(d5, self.df*2, f_size=2)\n",
    "        d7 = d_layer(d6, self.df*4, f_size=2)\n",
    "        d8 = d_layer(d7, self.df*8, f_size=2)\n",
    "        \n",
    "        d = Concatenate(axis=-1)([d4, d8])\"\"\"\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A, img_B], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        self.iterator = SiameseGanImageIterator(self.generator, 'data/640x640/cropped/nude/face', \n",
    "                                                'data/640x640/cropped/nude/person', batch_size = batch_size)\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        steps_per_epoch = int(self.iterator.max_n / batch_size)\n",
    "        print(steps_per_epoch)\n",
    "        for epoch in range(epochs):\n",
    "            for s in range(steps_per_epoch):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                valid_imgs = self.iterator.get_valid_imgs()\n",
    "                fake_imgs = self.iterator.get_fake_imgs()\n",
    "                d_loss_real = self.discriminator.train_on_batch(valid_imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(fake_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                g_loss = self.combined.train_on_batch(valid_imgs, [valid, valid_imgs[0]])\n",
    "                #valid_imgs = self.iterator.get_dressed_imgs()\n",
    "                #g_loss_dressed = self.combined_dressed.train_on_batch(valid_imgs[1], valid)\n",
    "                #g_loss = 0.5 * np.add(g_loss_nude, g_loss_dressed)\n",
    "                \n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                # Plot the progress\n",
    "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                        s, steps_per_epoch,\n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if s % sample_interval == 0:\n",
    "                    self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        print('sampling')\n",
    "        valids = self.iterator.get_valid_imgs()\n",
    "        back = valids[1]\n",
    "        truth = valids[0]\n",
    "        generated = self.generator.predict(back)[0]\n",
    "        img = show_encoded_array(np.concatenate([back[0],generated, truth[0]], axis=1))\n",
    "        img.save('sample/{}.png'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 320, 320, 64) 3136        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 320, 320, 64) 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 160, 160, 128 131200      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 160, 160, 128 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 160, 160, 128 512         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 80, 80, 256)  524544      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 80, 80, 256)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 80, 80, 256)  1024        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 40, 40, 512)  2097664     batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 40, 40, 512)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 40, 40, 512)  2048        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 20, 20, 512)  4194816     batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 20, 20, 512)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 20, 512)  2048        leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 10, 10, 512)  4194816     batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 10, 10, 512)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 10, 10, 512)  2048        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 5, 5, 512)    4194816     batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 5, 5, 512)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 5, 5, 512)    2048        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 10, 10, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 10, 10, 512)  4194816     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 10, 10, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10, 10, 1024) 0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 20, 20, 1024) 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 20, 20, 512)  8389120     up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 20, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 20, 20, 1024) 0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 40, 40, 1024) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 40, 40, 512)  8389120     up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 40, 40, 512)  2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 40, 40, 1024) 0           batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 80, 80, 1024) 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 80, 80, 256)  4194560     up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 80, 80, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 80, 80, 512)  0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 160, 160, 512 0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 160, 160, 128 1048704     up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 160, 160, 128 512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 160, 160, 256 0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 320, 320, 256 0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 320, 320, 64) 262208      up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 320, 320, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 320, 320, 128 0           batch_normalization_15[0][0]     \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 640, 640, 128 0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 640, 640, 3)  6147        up_sampling2d_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 41,843,331\n",
      "Trainable params: 41,834,499\n",
      "Non-trainable params: 8,832\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 640, 640, 6)  0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 320, 320, 64) 6208        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 320, 320, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 160, 160, 128 131200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 160, 160, 128 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 160, 160, 128 512         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 80, 80, 256)  524544      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 80, 80, 256)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 80, 80, 256)  1024        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 40, 40, 512)  2097664     batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 40, 40, 512)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 40, 40, 512)  2048        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 40, 40, 1)    8193        batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 5,540,994\n",
      "Trainable params: 2,769,601\n",
      "Non-trainable params: 2,771,393\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 640, 640, 3)  41843331    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 40, 40, 1)    2771393     model_2[1][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 44,614,724\n",
      "Trainable params: 41,834,499\n",
      "Non-trainable params: 2,780,225\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean/anaconda3/envs/kerasenv/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "gan = Pix2Pix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1367\n",
      "[Epoch 0/200] [Batch 0/1367] [D loss: 22.637815, acc:  22%] [G loss: 128.505127] time: 0:00:22.772950\n",
      "sampling\n",
      "[Epoch 0/200] [Batch 1/1367] [D loss: 23.409107, acc:   5%] [G loss: 59.379639] time: 0:00:28.659630\n",
      "[Epoch 0/200] [Batch 2/1367] [D loss: 17.193813, acc:   7%] [G loss: 54.358345] time: 0:00:33.146419\n",
      "[Epoch 0/200] [Batch 3/1367] [D loss: 7.773759, acc:   9%] [G loss: 46.856621] time: 0:00:37.654842\n",
      "[Epoch 0/200] [Batch 4/1367] [D loss: 2.599147, acc:  13%] [G loss: 34.961414] time: 0:00:42.131064\n",
      "[Epoch 0/200] [Batch 5/1367] [D loss: 1.096805, acc:  36%] [G loss: 30.236580] time: 0:00:46.611550\n",
      "[Epoch 0/200] [Batch 6/1367] [D loss: 0.755301, acc:  43%] [G loss: 31.747610] time: 0:00:51.099247\n",
      "[Epoch 0/200] [Batch 7/1367] [D loss: 0.626936, acc:  47%] [G loss: 22.930212] time: 0:00:55.586290\n",
      "[Epoch 0/200] [Batch 8/1367] [D loss: 0.573940, acc:  46%] [G loss: 25.649952] time: 0:01:00.081318\n",
      "[Epoch 0/200] [Batch 9/1367] [D loss: 0.533100, acc:  49%] [G loss: 29.641657] time: 0:01:04.577899\n",
      "[Epoch 0/200] [Batch 10/1367] [D loss: 0.498005, acc:  48%] [G loss: 26.811722] time: 0:01:09.078253\n",
      "[Epoch 0/200] [Batch 11/1367] [D loss: 0.496242, acc:  50%] [G loss: 32.959106] time: 0:01:13.568287\n",
      "[Epoch 0/200] [Batch 12/1367] [D loss: 0.475783, acc:  47%] [G loss: 25.437517] time: 0:01:18.078584\n",
      "[Epoch 0/200] [Batch 13/1367] [D loss: 0.470840, acc:  49%] [G loss: 31.789591] time: 0:01:22.612214\n",
      "[Epoch 0/200] [Batch 14/1367] [D loss: 0.464169, acc:  48%] [G loss: 28.645063] time: 0:01:27.142961\n",
      "[Epoch 0/200] [Batch 15/1367] [D loss: 0.500989, acc:  45%] [G loss: 26.213654] time: 0:01:31.666557\n",
      "[Epoch 0/200] [Batch 16/1367] [D loss: 0.480236, acc:  47%] [G loss: 25.999710] time: 0:01:36.198651\n",
      "[Epoch 0/200] [Batch 17/1367] [D loss: 0.443428, acc:  47%] [G loss: 28.198322] time: 0:01:40.724607\n",
      "[Epoch 0/200] [Batch 18/1367] [D loss: 0.441291, acc:  51%] [G loss: 26.825300] time: 0:01:45.279602\n",
      "[Epoch 0/200] [Batch 19/1367] [D loss: 0.417749, acc:  48%] [G loss: 19.677021] time: 0:01:49.787877\n",
      "[Epoch 0/200] [Batch 20/1367] [D loss: 0.401031, acc:  52%] [G loss: 25.302998] time: 0:01:54.336635\n",
      "[Epoch 0/200] [Batch 21/1367] [D loss: 0.411738, acc:  41%] [G loss: 28.284657] time: 0:01:58.886937\n",
      "[Epoch 0/200] [Batch 22/1367] [D loss: 0.388420, acc:  50%] [G loss: 21.909748] time: 0:02:03.461321\n",
      "[Epoch 0/200] [Batch 23/1367] [D loss: 0.378873, acc:  49%] [G loss: 18.251328] time: 0:02:08.018381\n",
      "[Epoch 0/200] [Batch 24/1367] [D loss: 0.369828, acc:  48%] [G loss: 21.539721] time: 0:02:12.583311\n",
      "[Epoch 0/200] [Batch 25/1367] [D loss: 0.351919, acc:  52%] [G loss: 15.985259] time: 0:02:17.109073\n",
      "[Epoch 0/200] [Batch 26/1367] [D loss: 0.362520, acc:  52%] [G loss: 26.289961] time: 0:02:21.647609\n",
      "[Epoch 0/200] [Batch 27/1367] [D loss: 0.342516, acc:  51%] [G loss: 20.019844] time: 0:02:26.206424\n",
      "[Epoch 0/200] [Batch 28/1367] [D loss: 0.363791, acc:  52%] [G loss: 26.857639] time: 0:02:30.766360\n",
      "[Epoch 0/200] [Batch 29/1367] [D loss: 0.308534, acc:  54%] [G loss: 16.335085] time: 0:02:35.347832\n",
      "[Epoch 0/200] [Batch 30/1367] [D loss: 0.321252, acc:  52%] [G loss: 18.378870] time: 0:02:39.908687\n",
      "[Epoch 0/200] [Batch 31/1367] [D loss: 0.325616, acc:  49%] [G loss: 15.544127] time: 0:02:44.461466\n",
      "[Epoch 0/200] [Batch 32/1367] [D loss: 0.306904, acc:  43%] [G loss: 14.122348] time: 0:02:49.020696\n",
      "[Epoch 0/200] [Batch 33/1367] [D loss: 0.338046, acc:  45%] [G loss: 13.471907] time: 0:02:53.625033\n",
      "[Epoch 0/200] [Batch 34/1367] [D loss: 0.281504, acc:  53%] [G loss: 12.914240] time: 0:02:58.219215\n",
      "[Epoch 0/200] [Batch 35/1367] [D loss: 0.355043, acc:  43%] [G loss: 17.035376] time: 0:03:02.851963\n",
      "[Epoch 0/200] [Batch 36/1367] [D loss: 0.412493, acc:  45%] [G loss: 25.765667] time: 0:03:07.434332\n",
      "[Epoch 0/200] [Batch 37/1367] [D loss: 0.339253, acc:  51%] [G loss: 33.958412] time: 0:03:12.034565\n",
      "[Epoch 0/200] [Batch 38/1367] [D loss: 0.291420, acc:  71%] [G loss: 22.956429] time: 0:03:16.597378\n",
      "[Epoch 0/200] [Batch 39/1367] [D loss: 0.293048, acc:  53%] [G loss: 16.376320] time: 0:03:21.152643\n",
      "[Epoch 0/200] [Batch 40/1367] [D loss: 0.295045, acc:  53%] [G loss: 40.631065] time: 0:03:25.716011\n",
      "[Epoch 0/200] [Batch 41/1367] [D loss: 0.340922, acc:  51%] [G loss: 24.213318] time: 0:03:30.292298\n",
      "[Epoch 0/200] [Batch 42/1367] [D loss: 0.448271, acc:  56%] [G loss: 35.924267] time: 0:03:34.905684\n",
      "[Epoch 0/200] [Batch 43/1367] [D loss: 0.303627, acc:  54%] [G loss: 24.327557] time: 0:03:39.508660\n",
      "[Epoch 0/200] [Batch 44/1367] [D loss: 0.293754, acc:  71%] [G loss: 22.151484] time: 0:03:44.112639\n",
      "[Epoch 0/200] [Batch 45/1367] [D loss: 0.309130, acc:  53%] [G loss: 19.020702] time: 0:03:48.684634\n",
      "[Epoch 0/200] [Batch 46/1367] [D loss: 0.275097, acc:  59%] [G loss: 12.777740] time: 0:03:53.222640\n",
      "[Epoch 0/200] [Batch 47/1367] [D loss: 0.312309, acc:  53%] [G loss: 12.902804] time: 0:03:57.778365\n",
      "[Epoch 0/200] [Batch 48/1367] [D loss: 0.210416, acc:  82%] [G loss: 9.623590] time: 0:04:02.364580\n",
      "[Epoch 0/200] [Batch 49/1367] [D loss: 0.396162, acc:  42%] [G loss: 14.204562] time: 0:04:06.917037\n",
      "[Epoch 0/200] [Batch 50/1367] [D loss: 0.389446, acc:  29%] [G loss: 15.581210] time: 0:04:11.491161\n",
      "[Epoch 0/200] [Batch 51/1367] [D loss: 0.311842, acc:  50%] [G loss: 14.334058] time: 0:04:16.053134\n",
      "[Epoch 0/200] [Batch 52/1367] [D loss: 0.238903, acc:  74%] [G loss: 10.995169] time: 0:04:20.622968\n",
      "[Epoch 0/200] [Batch 53/1367] [D loss: 0.260352, acc:  52%] [G loss: 18.238522] time: 0:04:25.237174\n",
      "[Epoch 0/200] [Batch 54/1367] [D loss: 0.310479, acc:  50%] [G loss: 20.167116] time: 0:04:29.825520\n",
      "[Epoch 0/200] [Batch 55/1367] [D loss: 0.226047, acc:  79%] [G loss: 14.183435] time: 0:04:34.390044\n",
      "[Epoch 0/200] [Batch 56/1367] [D loss: 0.235745, acc:  76%] [G loss: 10.499543] time: 0:04:38.962312\n",
      "[Epoch 0/200] [Batch 57/1367] [D loss: 0.290142, acc:  53%] [G loss: 12.833931] time: 0:04:43.533917\n",
      "[Epoch 0/200] [Batch 58/1367] [D loss: 0.368059, acc:  22%] [G loss: 16.739262] time: 0:04:48.111089\n",
      "[Epoch 0/200] [Batch 59/1367] [D loss: 0.295767, acc:  49%] [G loss: 10.170006] time: 0:04:52.672887\n",
      "[Epoch 0/200] [Batch 60/1367] [D loss: 0.277740, acc:  49%] [G loss: 13.596473] time: 0:04:57.241243\n",
      "[Epoch 0/200] [Batch 61/1367] [D loss: 0.293065, acc:  50%] [G loss: 17.989481] time: 0:05:01.816463\n",
      "[Epoch 0/200] [Batch 62/1367] [D loss: 0.448981, acc:  14%] [G loss: 21.663496] time: 0:05:06.388331\n",
      "[Epoch 0/200] [Batch 63/1367] [D loss: 0.325385, acc:  52%] [G loss: 22.108065] time: 0:05:11.004334\n",
      "[Epoch 0/200] [Batch 64/1367] [D loss: 0.297650, acc:  49%] [G loss: 16.982143] time: 0:05:15.594511\n",
      "[Epoch 0/200] [Batch 65/1367] [D loss: 0.267401, acc:  54%] [G loss: 17.093559] time: 0:05:20.186317\n",
      "[Epoch 0/200] [Batch 66/1367] [D loss: 0.339802, acc:  28%] [G loss: 19.164072] time: 0:05:24.777895\n",
      "[Epoch 0/200] [Batch 67/1367] [D loss: 0.312706, acc:  43%] [G loss: 12.320273] time: 0:05:29.373955\n",
      "[Epoch 0/200] [Batch 68/1367] [D loss: 0.331392, acc:  69%] [G loss: 8.898455] time: 0:05:33.948695\n",
      "[Epoch 0/200] [Batch 69/1367] [D loss: 0.257301, acc:  72%] [G loss: 11.512051] time: 0:05:38.522425\n",
      "[Epoch 0/200] [Batch 70/1367] [D loss: 0.359798, acc:  30%] [G loss: 16.353491] time: 0:05:43.095688\n",
      "[Epoch 0/200] [Batch 71/1367] [D loss: 0.294744, acc:  50%] [G loss: 15.401944] time: 0:05:47.638595\n",
      "[Epoch 0/200] [Batch 72/1367] [D loss: 0.218973, acc:  78%] [G loss: 19.385626] time: 0:05:52.230251\n",
      "[Epoch 0/200] [Batch 73/1367] [D loss: 0.293375, acc:  55%] [G loss: 10.974306] time: 0:05:56.800837\n",
      "[Epoch 0/200] [Batch 74/1367] [D loss: 0.238071, acc:  80%] [G loss: 8.726688] time: 0:06:01.369014\n",
      "[Epoch 0/200] [Batch 75/1367] [D loss: 0.240671, acc:  75%] [G loss: 13.106966] time: 0:06:05.931263\n",
      "[Epoch 0/200] [Batch 76/1367] [D loss: 0.261119, acc:  41%] [G loss: 13.107744] time: 0:06:10.474024\n",
      "[Epoch 0/200] [Batch 77/1367] [D loss: 0.274718, acc:  71%] [G loss: 14.289488] time: 0:06:15.081043\n",
      "[Epoch 0/200] [Batch 78/1367] [D loss: 0.376296, acc:  33%] [G loss: 20.113111] time: 0:06:19.671129\n",
      "[Epoch 0/200] [Batch 79/1367] [D loss: 0.231331, acc:  79%] [G loss: 12.890356] time: 0:06:24.232056\n",
      "[Epoch 0/200] [Batch 80/1367] [D loss: 0.268654, acc:  54%] [G loss: 11.911176] time: 0:06:28.800880\n",
      "[Epoch 0/200] [Batch 81/1367] [D loss: 0.245103, acc:  67%] [G loss: 10.578559] time: 0:06:33.367958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 82/1367] [D loss: 0.319428, acc:  38%] [G loss: 13.850019] time: 0:06:37.953496\n",
      "[Epoch 0/200] [Batch 83/1367] [D loss: 0.240091, acc:  75%] [G loss: 13.174192] time: 0:06:42.538977\n",
      "[Epoch 0/200] [Batch 84/1367] [D loss: 0.240179, acc:  71%] [G loss: 14.927591] time: 0:06:47.121509\n",
      "[Epoch 0/200] [Batch 85/1367] [D loss: 0.345234, acc:  44%] [G loss: 22.199867] time: 0:06:51.745876\n",
      "[Epoch 0/200] [Batch 86/1367] [D loss: 0.331913, acc:  44%] [G loss: 20.853296] time: 0:06:56.346605\n",
      "[Epoch 0/200] [Batch 87/1367] [D loss: 0.283995, acc:  70%] [G loss: 14.193029] time: 0:07:00.948589\n",
      "[Epoch 0/200] [Batch 88/1367] [D loss: 0.346637, acc:  22%] [G loss: 19.257807] time: 0:07:05.572764\n",
      "[Epoch 0/200] [Batch 89/1367] [D loss: 0.245552, acc:  60%] [G loss: 20.076326] time: 0:07:10.166843\n",
      "[Epoch 0/200] [Batch 90/1367] [D loss: 0.295564, acc:  51%] [G loss: 10.743284] time: 0:07:14.796531\n",
      "[Epoch 0/200] [Batch 91/1367] [D loss: 0.325827, acc:  26%] [G loss: 12.163098] time: 0:07:19.384900\n",
      "[Epoch 0/200] [Batch 92/1367] [D loss: 0.281580, acc:  50%] [G loss: 15.851821] time: 0:07:23.996120\n",
      "[Epoch 0/200] [Batch 93/1367] [D loss: 0.341967, acc:  40%] [G loss: 21.226936] time: 0:07:28.586655\n",
      "[Epoch 0/200] [Batch 94/1367] [D loss: 0.254631, acc:  69%] [G loss: 13.119809] time: 0:07:33.215098\n",
      "[Epoch 0/200] [Batch 95/1367] [D loss: 0.288405, acc:  50%] [G loss: 9.485352] time: 0:07:37.804285\n",
      "[Epoch 0/200] [Batch 96/1367] [D loss: 0.349932, acc:  43%] [G loss: 16.481108] time: 0:07:42.412459\n",
      "[Epoch 0/200] [Batch 97/1367] [D loss: 0.224545, acc:  76%] [G loss: 14.667061] time: 0:07:47.031537\n",
      "[Epoch 0/200] [Batch 98/1367] [D loss: 0.230060, acc:  62%] [G loss: 14.537337] time: 0:07:51.567834\n",
      "[Epoch 0/200] [Batch 99/1367] [D loss: 0.300480, acc:  30%] [G loss: 12.969520] time: 0:07:56.155827\n",
      "[Epoch 0/200] [Batch 100/1367] [D loss: 0.275341, acc:  70%] [G loss: 12.698940] time: 0:08:00.751370\n",
      "[Epoch 0/200] [Batch 101/1367] [D loss: 0.251912, acc:  72%] [G loss: 13.786133] time: 0:08:05.329073\n",
      "[Epoch 0/200] [Batch 102/1367] [D loss: 0.287341, acc:  32%] [G loss: 13.703582] time: 0:08:09.901817\n",
      "[Epoch 0/200] [Batch 103/1367] [D loss: 0.294469, acc:  52%] [G loss: 15.368075] time: 0:08:14.500733\n",
      "[Epoch 0/200] [Batch 104/1367] [D loss: 0.266967, acc:  51%] [G loss: 15.145343] time: 0:08:19.084351\n",
      "[Epoch 0/200] [Batch 105/1367] [D loss: 0.329322, acc:  33%] [G loss: 15.369912] time: 0:08:23.664716\n",
      "[Epoch 0/200] [Batch 106/1367] [D loss: 0.303245, acc:  45%] [G loss: 13.637430] time: 0:08:28.237048\n",
      "[Epoch 0/200] [Batch 107/1367] [D loss: 0.237845, acc:  60%] [G loss: 13.166844] time: 0:08:32.790951\n",
      "[Epoch 0/200] [Batch 108/1367] [D loss: 0.248022, acc:  53%] [G loss: 11.869463] time: 0:08:37.387936\n",
      "[Epoch 0/200] [Batch 109/1367] [D loss: 0.283357, acc:  36%] [G loss: 13.166360] time: 0:08:41.968548\n",
      "[Epoch 0/200] [Batch 110/1367] [D loss: 0.329921, acc:  25%] [G loss: 12.334002] time: 0:08:46.551219\n",
      "[Epoch 0/200] [Batch 111/1367] [D loss: 0.294663, acc:  45%] [G loss: 9.806296] time: 0:08:51.105809\n",
      "[Epoch 0/200] [Batch 112/1367] [D loss: 0.278920, acc:  50%] [G loss: 8.646808] time: 0:08:55.674124\n",
      "[Epoch 0/200] [Batch 113/1367] [D loss: 0.290470, acc:  67%] [G loss: 9.648317] time: 0:09:00.231718\n",
      "[Epoch 0/200] [Batch 114/1367] [D loss: 0.330651, acc:  57%] [G loss: 13.362921] time: 0:09:04.817206\n",
      "[Epoch 0/200] [Batch 115/1367] [D loss: 0.290195, acc:  50%] [G loss: 15.713862] time: 0:09:09.414383\n",
      "[Epoch 0/200] [Batch 116/1367] [D loss: 0.320828, acc:  27%] [G loss: 10.444798] time: 0:09:14.023578\n",
      "[Epoch 0/200] [Batch 117/1367] [D loss: 0.276268, acc:  49%] [G loss: 13.388252] time: 0:09:18.633549\n",
      "[Epoch 0/200] [Batch 118/1367] [D loss: 0.314102, acc:  45%] [G loss: 13.982704] time: 0:09:23.233065\n",
      "[Epoch 0/200] [Batch 119/1367] [D loss: 0.383416, acc:  18%] [G loss: 15.838906] time: 0:09:27.820783\n",
      "[Epoch 0/200] [Batch 120/1367] [D loss: 0.293902, acc:  62%] [G loss: 9.949436] time: 0:09:32.390215\n",
      "[Epoch 0/200] [Batch 121/1367] [D loss: 0.261312, acc:  51%] [G loss: 9.396017] time: 0:09:36.960730\n",
      "[Epoch 0/200] [Batch 122/1367] [D loss: 0.306031, acc:  28%] [G loss: 9.397186] time: 0:09:41.528138\n",
      "[Epoch 0/200] [Batch 123/1367] [D loss: 0.256690, acc:  55%] [G loss: 7.561801] time: 0:09:46.130732\n",
      "[Epoch 0/200] [Batch 124/1367] [D loss: 0.259095, acc:  55%] [G loss: 16.574259] time: 0:09:50.726739\n",
      "[Epoch 0/200] [Batch 125/1367] [D loss: 0.238194, acc:  57%] [G loss: 13.541983] time: 0:09:55.308750\n",
      "[Epoch 0/200] [Batch 126/1367] [D loss: 0.252510, acc:  56%] [G loss: 11.022368] time: 0:09:59.899845\n",
      "[Epoch 0/200] [Batch 127/1367] [D loss: 0.312041, acc:  44%] [G loss: 10.870543] time: 0:10:04.470734\n",
      "[Epoch 0/200] [Batch 128/1367] [D loss: 0.243426, acc:  71%] [G loss: 8.904824] time: 0:10:09.046933\n",
      "[Epoch 0/200] [Batch 129/1367] [D loss: 0.282136, acc:  63%] [G loss: 14.161296] time: 0:10:13.631374\n",
      "[Epoch 0/200] [Batch 130/1367] [D loss: 0.280490, acc:  65%] [G loss: 15.849421] time: 0:10:18.195340\n",
      "[Epoch 0/200] [Batch 131/1367] [D loss: 0.296050, acc:  44%] [G loss: 9.232528] time: 0:10:22.770665\n",
      "[Epoch 0/200] [Batch 132/1367] [D loss: 0.360967, acc:  19%] [G loss: 13.033880] time: 0:10:27.351046\n",
      "[Epoch 0/200] [Batch 133/1367] [D loss: 0.286142, acc:  36%] [G loss: 10.400541] time: 0:10:31.951687\n",
      "[Epoch 0/200] [Batch 134/1367] [D loss: 0.318002, acc:  30%] [G loss: 11.697889] time: 0:10:36.563657\n",
      "[Epoch 0/200] [Batch 135/1367] [D loss: 0.290387, acc:  27%] [G loss: 10.943231] time: 0:10:41.142137\n",
      "[Epoch 0/200] [Batch 136/1367] [D loss: 0.228037, acc:  75%] [G loss: 6.347709] time: 0:10:45.710507\n",
      "[Epoch 0/200] [Batch 137/1367] [D loss: 0.347150, acc:  19%] [G loss: 12.476852] time: 0:10:50.270247\n",
      "[Epoch 0/200] [Batch 138/1367] [D loss: 0.258693, acc:  73%] [G loss: 7.615002] time: 0:10:54.853210\n",
      "[Epoch 0/200] [Batch 139/1367] [D loss: 0.314051, acc:  67%] [G loss: 11.045300] time: 0:10:59.435509\n",
      "[Epoch 0/200] [Batch 140/1367] [D loss: 0.267132, acc:  72%] [G loss: 8.030105] time: 0:11:04.015128\n",
      "[Epoch 0/200] [Batch 141/1367] [D loss: 0.283772, acc:  46%] [G loss: 7.449655] time: 0:11:08.623049\n",
      "[Epoch 0/200] [Batch 142/1367] [D loss: 0.250318, acc:  37%] [G loss: 8.227632] time: 0:11:13.227248\n",
      "[Epoch 0/200] [Batch 143/1367] [D loss: 0.303609, acc:  46%] [G loss: 13.296762] time: 0:11:17.842715\n",
      "[Epoch 0/200] [Batch 144/1367] [D loss: 0.288073, acc:  54%] [G loss: 14.725997] time: 0:11:22.463888\n",
      "[Epoch 0/200] [Batch 145/1367] [D loss: 0.313265, acc:  48%] [G loss: 13.545331] time: 0:11:27.064921\n",
      "[Epoch 0/200] [Batch 146/1367] [D loss: 0.329403, acc:  67%] [G loss: 12.600666] time: 0:11:31.618908\n",
      "[Epoch 0/200] [Batch 147/1367] [D loss: 0.347178, acc:  61%] [G loss: 22.766157] time: 0:11:36.212182\n",
      "[Epoch 0/200] [Batch 148/1367] [D loss: 0.337458, acc:  50%] [G loss: 27.247114] time: 0:11:40.779991\n",
      "[Epoch 0/200] [Batch 149/1367] [D loss: 0.245198, acc:  55%] [G loss: 12.621299] time: 0:11:45.350479\n",
      "[Epoch 0/200] [Batch 150/1367] [D loss: 0.361280, acc:  29%] [G loss: 17.379288] time: 0:11:49.927504\n",
      "[Epoch 0/200] [Batch 151/1367] [D loss: 0.257061, acc:  56%] [G loss: 10.785558] time: 0:11:54.511125\n",
      "[Epoch 0/200] [Batch 152/1367] [D loss: 0.245452, acc:  52%] [G loss: 8.591814] time: 0:11:59.096273\n",
      "[Epoch 0/200] [Batch 153/1367] [D loss: 0.258798, acc:  39%] [G loss: 12.948251] time: 0:12:03.665048\n",
      "[Epoch 0/200] [Batch 154/1367] [D loss: 0.293174, acc:  26%] [G loss: 6.042770] time: 0:12:08.197599\n",
      "[Epoch 0/200] [Batch 155/1367] [D loss: 0.268696, acc:  31%] [G loss: 8.698063] time: 0:12:12.769853\n",
      "[Epoch 0/200] [Batch 156/1367] [D loss: 0.275538, acc:  28%] [G loss: 7.787413] time: 0:12:17.343027\n",
      "[Epoch 0/200] [Batch 157/1367] [D loss: 0.247235, acc:  53%] [G loss: 8.345666] time: 0:12:21.914785\n",
      "[Epoch 0/200] [Batch 158/1367] [D loss: 0.275152, acc:  27%] [G loss: 11.279013] time: 0:12:26.476449\n",
      "[Epoch 0/200] [Batch 159/1367] [D loss: 0.272421, acc:  29%] [G loss: 9.863966] time: 0:12:31.043732\n",
      "[Epoch 0/200] [Batch 160/1367] [D loss: 0.250625, acc:  54%] [G loss: 8.850220] time: 0:12:35.632543\n",
      "[Epoch 0/200] [Batch 161/1367] [D loss: 0.279639, acc:  70%] [G loss: 7.857288] time: 0:12:40.184574\n",
      "[Epoch 0/200] [Batch 162/1367] [D loss: 0.219371, acc:  75%] [G loss: 6.041677] time: 0:12:44.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 163/1367] [D loss: 0.316149, acc:  58%] [G loss: 14.392893] time: 0:12:49.307352\n",
      "[Epoch 0/200] [Batch 164/1367] [D loss: 0.333653, acc:  22%] [G loss: 14.897768] time: 0:12:53.879871\n",
      "[Epoch 0/200] [Batch 165/1367] [D loss: 0.252498, acc:  64%] [G loss: 7.832727] time: 0:12:58.458907\n",
      "[Epoch 0/200] [Batch 166/1367] [D loss: 0.270311, acc:  65%] [G loss: 9.152681] time: 0:13:03.014885\n",
      "[Epoch 0/200] [Batch 167/1367] [D loss: 0.233377, acc:  55%] [G loss: 10.176543] time: 0:13:07.593596\n",
      "[Epoch 0/200] [Batch 168/1367] [D loss: 0.299023, acc:  47%] [G loss: 14.613612] time: 0:13:12.188128\n",
      "[Epoch 0/200] [Batch 169/1367] [D loss: 0.216326, acc:  83%] [G loss: 10.442964] time: 0:13:16.755574\n",
      "[Epoch 0/200] [Batch 170/1367] [D loss: 0.277350, acc:  35%] [G loss: 10.071964] time: 0:13:21.342910\n",
      "[Epoch 0/200] [Batch 171/1367] [D loss: 0.280505, acc:  35%] [G loss: 8.754486] time: 0:13:25.937541\n",
      "[Epoch 0/200] [Batch 172/1367] [D loss: 0.245427, acc:  38%] [G loss: 7.535445] time: 0:13:30.514644\n",
      "[Epoch 0/200] [Batch 173/1367] [D loss: 0.302180, acc:  27%] [G loss: 9.363438] time: 0:13:35.141317\n",
      "[Epoch 0/200] [Batch 174/1367] [D loss: 0.250195, acc:  67%] [G loss: 9.173465] time: 0:13:39.729963\n",
      "[Epoch 0/200] [Batch 175/1367] [D loss: 0.314328, acc:  40%] [G loss: 12.113820] time: 0:13:44.342025\n",
      "[Epoch 0/200] [Batch 176/1367] [D loss: 0.274353, acc:  71%] [G loss: 7.184202] time: 0:13:48.896727\n",
      "[Epoch 0/200] [Batch 177/1367] [D loss: 0.369634, acc:  62%] [G loss: 11.524117] time: 0:13:53.471793\n",
      "[Epoch 0/200] [Batch 178/1367] [D loss: 0.347360, acc:  73%] [G loss: 7.184432] time: 0:13:58.013497\n",
      "[Epoch 0/200] [Batch 179/1367] [D loss: 0.370435, acc:  66%] [G loss: 6.138398] time: 0:14:02.586709\n",
      "[Epoch 0/200] [Batch 180/1367] [D loss: 0.410601, acc:  58%] [G loss: 13.436585] time: 0:14:07.171509\n",
      "[Epoch 0/200] [Batch 181/1367] [D loss: 0.419737, acc:  63%] [G loss: 9.203055] time: 0:14:11.731197\n",
      "[Epoch 0/200] [Batch 182/1367] [D loss: 0.336931, acc:  66%] [G loss: 13.041043] time: 0:14:16.301666\n",
      "[Epoch 0/200] [Batch 183/1367] [D loss: 0.288029, acc:  47%] [G loss: 11.453825] time: 0:14:20.883881\n",
      "[Epoch 0/200] [Batch 184/1367] [D loss: 0.282449, acc:  33%] [G loss: 12.302000] time: 0:14:25.448946\n",
      "[Epoch 0/200] [Batch 185/1367] [D loss: 0.513698, acc:  21%] [G loss: 10.694699] time: 0:14:29.981776\n",
      "[Epoch 0/200] [Batch 186/1367] [D loss: 0.653499, acc:  10%] [G loss: 6.022452] time: 0:14:34.543251\n",
      "[Epoch 0/200] [Batch 187/1367] [D loss: 0.705101, acc:  13%] [G loss: 6.298877] time: 0:14:39.118510\n",
      "[Epoch 0/200] [Batch 188/1367] [D loss: 0.503342, acc:  26%] [G loss: 10.209126] time: 0:14:43.673448\n",
      "[Epoch 0/200] [Batch 189/1367] [D loss: 0.259780, acc:  53%] [G loss: 6.408788] time: 0:14:48.252798\n",
      "[Epoch 0/200] [Batch 190/1367] [D loss: 0.286124, acc:  64%] [G loss: 7.717874] time: 0:14:52.815169\n",
      "[Epoch 0/200] [Batch 191/1367] [D loss: 0.326229, acc:  70%] [G loss: 9.850049] time: 0:14:57.364089\n",
      "[Epoch 0/200] [Batch 192/1367] [D loss: 0.318476, acc:  69%] [G loss: 9.159222] time: 0:15:01.939676\n",
      "[Epoch 0/200] [Batch 193/1367] [D loss: 0.494897, acc:  69%] [G loss: 9.366274] time: 0:15:06.498208\n",
      "[Epoch 0/200] [Batch 194/1367] [D loss: 0.510414, acc:  69%] [G loss: 7.492421] time: 0:15:11.077386\n",
      "[Epoch 0/200] [Batch 195/1367] [D loss: 0.499147, acc:  59%] [G loss: 14.220017] time: 0:15:15.644399\n",
      "[Epoch 0/200] [Batch 196/1367] [D loss: 0.302845, acc:  67%] [G loss: 11.568759] time: 0:15:20.230528\n",
      "[Epoch 0/200] [Batch 197/1367] [D loss: 0.257783, acc:  73%] [G loss: 7.356951] time: 0:15:24.803726\n",
      "[Epoch 0/200] [Batch 198/1367] [D loss: 0.283462, acc:  48%] [G loss: 9.579719] time: 0:15:29.389140\n",
      "[Epoch 0/200] [Batch 199/1367] [D loss: 0.285607, acc:  47%] [G loss: 6.129140] time: 0:15:33.930440\n",
      "[Epoch 0/200] [Batch 200/1367] [D loss: 0.254044, acc:  73%] [G loss: 5.701277] time: 0:15:38.501912\n",
      "sampling\n",
      "[Epoch 0/200] [Batch 201/1367] [D loss: 0.294052, acc:  45%] [G loss: 9.365886] time: 0:15:44.463141\n",
      "[Epoch 0/200] [Batch 202/1367] [D loss: 0.281179, acc:  44%] [G loss: 9.190598] time: 0:15:49.036803\n",
      "[Epoch 0/200] [Batch 203/1367] [D loss: 0.254085, acc:  50%] [G loss: 7.125252] time: 0:15:53.615350\n",
      "[Epoch 0/200] [Batch 204/1367] [D loss: 0.271005, acc:  49%] [G loss: 11.933181] time: 0:15:58.227822\n",
      "[Epoch 0/200] [Batch 205/1367] [D loss: 0.254340, acc:  53%] [G loss: 12.411824] time: 0:16:02.841194\n",
      "[Epoch 0/200] [Batch 206/1367] [D loss: 0.305898, acc:  26%] [G loss: 10.144698] time: 0:16:07.424226\n",
      "[Epoch 0/200] [Batch 207/1367] [D loss: 0.267646, acc:  54%] [G loss: 9.863985] time: 0:16:11.999792\n",
      "[Epoch 0/200] [Batch 208/1367] [D loss: 0.310076, acc:  27%] [G loss: 13.588336] time: 0:16:16.570345\n",
      "[Epoch 0/200] [Batch 209/1367] [D loss: 0.237871, acc:  73%] [G loss: 8.665650] time: 0:16:21.134816\n",
      "[Epoch 0/200] [Batch 210/1367] [D loss: 0.251090, acc:  71%] [G loss: 7.197488] time: 0:16:25.705221\n",
      "[Epoch 0/200] [Batch 211/1367] [D loss: 0.251282, acc:  54%] [G loss: 7.622509] time: 0:16:30.284899\n",
      "[Epoch 0/200] [Batch 212/1367] [D loss: 0.263223, acc:  31%] [G loss: 6.264053] time: 0:16:34.836937\n",
      "[Epoch 0/200] [Batch 213/1367] [D loss: 0.282461, acc:  29%] [G loss: 7.108501] time: 0:16:39.417485\n",
      "[Epoch 0/200] [Batch 214/1367] [D loss: 0.250086, acc:  31%] [G loss: 4.629739] time: 0:16:43.975553\n",
      "[Epoch 0/200] [Batch 215/1367] [D loss: 0.285593, acc:  32%] [G loss: 9.064922] time: 0:16:48.551210\n",
      "[Epoch 0/200] [Batch 216/1367] [D loss: 0.220902, acc:  78%] [G loss: 4.248402] time: 0:16:53.098547\n",
      "[Epoch 0/200] [Batch 217/1367] [D loss: 0.314808, acc:  76%] [G loss: 6.227432] time: 0:16:57.665502\n",
      "[Epoch 0/200] [Batch 218/1367] [D loss: 0.293696, acc:  78%] [G loss: 5.734649] time: 0:17:02.224750\n",
      "[Epoch 0/200] [Batch 219/1367] [D loss: 0.281873, acc:  66%] [G loss: 10.102479] time: 0:17:06.806143\n",
      "[Epoch 0/200] [Batch 220/1367] [D loss: 0.279054, acc:  49%] [G loss: 11.071271] time: 0:17:11.387896\n",
      "[Epoch 0/200] [Batch 221/1367] [D loss: 0.298017, acc:  26%] [G loss: 13.542489] time: 0:17:15.981786\n",
      "[Epoch 0/200] [Batch 222/1367] [D loss: 0.296550, acc:  34%] [G loss: 12.854750] time: 0:17:20.545466\n",
      "[Epoch 0/200] [Batch 223/1367] [D loss: 0.323310, acc:  44%] [G loss: 9.116241] time: 0:17:25.116016\n",
      "[Epoch 0/200] [Batch 224/1367] [D loss: 0.372664, acc:  68%] [G loss: 13.678987] time: 0:17:29.697938\n",
      "[Epoch 0/200] [Batch 225/1367] [D loss: 0.488549, acc:  75%] [G loss: 6.658436] time: 0:17:34.252426\n",
      "[Epoch 0/200] [Batch 226/1367] [D loss: 0.452833, acc:  76%] [G loss: 5.723275] time: 0:17:38.791288\n",
      "[Epoch 0/200] [Batch 227/1367] [D loss: 0.336131, acc:  72%] [G loss: 7.026612] time: 0:17:43.396551\n",
      "[Epoch 0/200] [Batch 228/1367] [D loss: 0.255273, acc:  74%] [G loss: 7.013554] time: 0:17:47.992248\n",
      "[Epoch 0/200] [Batch 229/1367] [D loss: 0.282862, acc:  31%] [G loss: 6.254625] time: 0:17:52.530932\n",
      "[Epoch 0/200] [Batch 230/1367] [D loss: 0.337170, acc:  32%] [G loss: 7.143191] time: 0:17:57.095676\n",
      "[Epoch 0/200] [Batch 231/1367] [D loss: 0.620795, acc:  26%] [G loss: 14.287665] time: 0:18:01.691390\n",
      "[Epoch 0/200] [Batch 232/1367] [D loss: 0.819069, acc:  22%] [G loss: 13.951681] time: 0:18:06.311025\n",
      "[Epoch 0/200] [Batch 233/1367] [D loss: 0.616460, acc:  42%] [G loss: 16.440710] time: 0:18:10.902606\n",
      "[Epoch 0/200] [Batch 234/1367] [D loss: 0.329130, acc:  43%] [G loss: 14.039924] time: 0:18:15.500235\n",
      "[Epoch 0/200] [Batch 235/1367] [D loss: 0.274812, acc:  65%] [G loss: 12.114721] time: 0:18:20.109191\n",
      "[Epoch 0/200] [Batch 236/1367] [D loss: 0.447517, acc:  55%] [G loss: 15.734698] time: 0:18:24.725801\n",
      "[Epoch 0/200] [Batch 237/1367] [D loss: 0.548723, acc:  59%] [G loss: 24.220053] time: 0:18:29.322860\n",
      "[Epoch 0/200] [Batch 238/1367] [D loss: 0.403350, acc:  68%] [G loss: 10.365185] time: 0:18:33.890901\n",
      "[Epoch 0/200] [Batch 239/1367] [D loss: 0.334580, acc:  64%] [G loss: 15.369569] time: 0:18:38.451357\n",
      "[Epoch 0/200] [Batch 240/1367] [D loss: 0.241263, acc:  73%] [G loss: 10.952743] time: 0:18:43.020617\n",
      "[Epoch 0/200] [Batch 241/1367] [D loss: 0.256811, acc:  66%] [G loss: 15.897554] time: 0:18:47.602588\n",
      "[Epoch 0/200] [Batch 242/1367] [D loss: 0.220982, acc:  64%] [G loss: 6.240892] time: 0:18:52.157751\n",
      "[Epoch 0/200] [Batch 243/1367] [D loss: 0.237734, acc:  43%] [G loss: 10.825268] time: 0:18:56.729230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 244/1367] [D loss: 0.267674, acc:  33%] [G loss: 6.790002] time: 0:19:01.288771\n",
      "[Epoch 0/200] [Batch 245/1367] [D loss: 0.259138, acc:  52%] [G loss: 15.767702] time: 0:19:05.881109\n",
      "[Epoch 0/200] [Batch 246/1367] [D loss: 0.228579, acc:  74%] [G loss: 6.083947] time: 0:19:10.455341\n",
      "[Epoch 0/200] [Batch 247/1367] [D loss: 0.241541, acc:  71%] [G loss: 9.383999] time: 0:19:15.022820\n",
      "[Epoch 0/200] [Batch 248/1367] [D loss: 0.278420, acc:  64%] [G loss: 11.497594] time: 0:19:19.590226\n",
      "[Epoch 0/200] [Batch 249/1367] [D loss: 0.275877, acc:  51%] [G loss: 11.048237] time: 0:19:24.140961\n",
      "[Epoch 0/200] [Batch 250/1367] [D loss: 0.244482, acc:  61%] [G loss: 13.091981] time: 0:19:28.719792\n",
      "[Epoch 0/200] [Batch 251/1367] [D loss: 0.223923, acc:  63%] [G loss: 13.326828] time: 0:19:33.314964\n",
      "[Epoch 0/200] [Batch 252/1367] [D loss: 0.299003, acc:  30%] [G loss: 12.909592] time: 0:19:37.870681\n",
      "[Epoch 0/200] [Batch 253/1367] [D loss: 0.246620, acc:  41%] [G loss: 8.929177] time: 0:19:42.442055\n",
      "[Epoch 0/200] [Batch 254/1367] [D loss: 0.218154, acc:  75%] [G loss: 7.841392] time: 0:19:47.007764\n",
      "[Epoch 0/200] [Batch 255/1367] [D loss: 0.242559, acc:  41%] [G loss: 8.137450] time: 0:19:51.589914\n",
      "[Epoch 0/200] [Batch 256/1367] [D loss: 0.229150, acc:  43%] [G loss: 6.189399] time: 0:19:56.174359\n",
      "[Epoch 0/200] [Batch 257/1367] [D loss: 0.250728, acc:  42%] [G loss: 10.169756] time: 0:20:00.765272\n",
      "[Epoch 0/200] [Batch 258/1367] [D loss: 0.244202, acc:  57%] [G loss: 10.480891] time: 0:20:05.360422\n",
      "[Epoch 0/200] [Batch 259/1367] [D loss: 0.270773, acc:  70%] [G loss: 7.718883] time: 0:20:09.922372\n",
      "[Epoch 0/200] [Batch 260/1367] [D loss: 0.270663, acc:  69%] [G loss: 10.976748] time: 0:20:14.462195\n",
      "[Epoch 0/200] [Batch 261/1367] [D loss: 0.265621, acc:  72%] [G loss: 8.841957] time: 0:20:19.045318\n",
      "[Epoch 0/200] [Batch 262/1367] [D loss: 0.267026, acc:  67%] [G loss: 7.770503] time: 0:20:23.617269\n",
      "[Epoch 0/200] [Batch 263/1367] [D loss: 0.280859, acc:  71%] [G loss: 8.272412] time: 0:20:28.205233\n",
      "[Epoch 0/200] [Batch 264/1367] [D loss: 0.251919, acc:  65%] [G loss: 22.299246] time: 0:20:32.823612\n",
      "[Epoch 0/200] [Batch 265/1367] [D loss: 0.245031, acc:  70%] [G loss: 10.341382] time: 0:20:37.391251\n",
      "[Epoch 0/200] [Batch 266/1367] [D loss: 0.267277, acc:  49%] [G loss: 13.135621] time: 0:20:41.952903\n",
      "[Epoch 0/200] [Batch 267/1367] [D loss: 0.213488, acc:  66%] [G loss: 7.221802] time: 0:20:46.538838\n",
      "[Epoch 0/200] [Batch 268/1367] [D loss: 0.324060, acc:  31%] [G loss: 14.389889] time: 0:20:51.121601\n",
      "[Epoch 0/200] [Batch 269/1367] [D loss: 0.298923, acc:  42%] [G loss: 12.150503] time: 0:20:55.698985\n",
      "[Epoch 0/200] [Batch 270/1367] [D loss: 0.296475, acc:  32%] [G loss: 13.037050] time: 0:21:00.285332\n",
      "[Epoch 0/200] [Batch 271/1367] [D loss: 0.213492, acc:  64%] [G loss: 5.313096] time: 0:21:04.848587\n",
      "[Epoch 0/200] [Batch 272/1367] [D loss: 0.281291, acc:  34%] [G loss: 6.172285] time: 0:21:09.428742\n",
      "[Epoch 0/200] [Batch 273/1367] [D loss: 0.376684, acc:  27%] [G loss: 8.165165] time: 0:21:14.010547\n",
      "[Epoch 0/200] [Batch 274/1367] [D loss: 0.431710, acc:  25%] [G loss: 9.622160] time: 0:21:18.574445\n",
      "[Epoch 0/200] [Batch 275/1367] [D loss: 0.383159, acc:  43%] [G loss: 10.866542] time: 0:21:23.165577\n",
      "[Epoch 0/200] [Batch 276/1367] [D loss: 0.371365, acc:  30%] [G loss: 13.911890] time: 0:21:27.753313\n",
      "[Epoch 0/200] [Batch 277/1367] [D loss: 0.266726, acc:  44%] [G loss: 11.395502] time: 0:21:32.313484\n",
      "[Epoch 0/200] [Batch 278/1367] [D loss: 0.397882, acc:  64%] [G loss: 20.406240] time: 0:21:36.887680\n",
      "[Epoch 0/200] [Batch 279/1367] [D loss: 0.662124, acc:  71%] [G loss: 10.623885] time: 0:21:41.460387\n",
      "[Epoch 0/200] [Batch 280/1367] [D loss: 1.238550, acc:  66%] [G loss: 13.704999] time: 0:21:46.036288\n",
      "[Epoch 0/200] [Batch 281/1367] [D loss: 1.177382, acc:  70%] [G loss: 6.877979] time: 0:21:50.617422\n",
      "[Epoch 0/200] [Batch 282/1367] [D loss: 0.465481, acc:  65%] [G loss: 12.758071] time: 0:21:55.205230\n",
      "[Epoch 0/200] [Batch 283/1367] [D loss: 0.257839, acc:  43%] [G loss: 12.290497] time: 0:21:59.802754\n",
      "[Epoch 0/200] [Batch 284/1367] [D loss: 0.378244, acc:  37%] [G loss: 10.525461] time: 0:22:04.365141\n",
      "[Epoch 0/200] [Batch 285/1367] [D loss: 0.320516, acc:  36%] [G loss: 12.414097] time: 0:22:08.941889\n",
      "[Epoch 0/200] [Batch 286/1367] [D loss: 0.263223, acc:  40%] [G loss: 16.735584] time: 0:22:13.526001\n",
      "[Epoch 0/200] [Batch 287/1367] [D loss: 0.250442, acc:  43%] [G loss: 14.659769] time: 0:22:18.107186\n",
      "[Epoch 0/200] [Batch 288/1367] [D loss: 0.270397, acc:  54%] [G loss: 22.512863] time: 0:22:22.693925\n",
      "[Epoch 0/200] [Batch 289/1367] [D loss: 0.221157, acc:  47%] [G loss: 6.842383] time: 0:22:27.266988\n",
      "[Epoch 0/200] [Batch 290/1367] [D loss: 0.245944, acc:  62%] [G loss: 17.040674] time: 0:22:31.877221\n",
      "[Epoch 0/200] [Batch 291/1367] [D loss: 0.272643, acc:  51%] [G loss: 10.560591] time: 0:22:36.488300\n",
      "[Epoch 0/200] [Batch 292/1367] [D loss: 0.206778, acc:  65%] [G loss: 16.789639] time: 0:22:41.080296\n",
      "[Epoch 0/200] [Batch 293/1367] [D loss: 0.237648, acc:  48%] [G loss: 22.019524] time: 0:22:45.640791\n",
      "[Epoch 0/200] [Batch 294/1367] [D loss: 0.213705, acc:  68%] [G loss: 12.664179] time: 0:22:50.218119\n",
      "[Epoch 0/200] [Batch 295/1367] [D loss: 0.246700, acc:  42%] [G loss: 8.658535] time: 0:22:54.793157\n",
      "[Epoch 0/200] [Batch 296/1367] [D loss: 0.243295, acc:  43%] [G loss: 7.630634] time: 0:22:59.355348\n",
      "[Epoch 0/200] [Batch 297/1367] [D loss: 0.249857, acc:  40%] [G loss: 9.488679] time: 0:23:03.902491\n",
      "[Epoch 0/200] [Batch 298/1367] [D loss: 0.201640, acc:  76%] [G loss: 11.189327] time: 0:23:08.441842\n",
      "[Epoch 0/200] [Batch 299/1367] [D loss: 0.222622, acc:  75%] [G loss: 7.665300] time: 0:23:13.026872\n",
      "[Epoch 0/200] [Batch 300/1367] [D loss: 0.250696, acc:  70%] [G loss: 9.674828] time: 0:23:17.618156\n",
      "[Epoch 0/200] [Batch 301/1367] [D loss: 0.251808, acc:  71%] [G loss: 8.312513] time: 0:23:22.201323\n",
      "[Epoch 0/200] [Batch 302/1367] [D loss: 0.257921, acc:  68%] [G loss: 12.317615] time: 0:23:26.823683\n",
      "[Epoch 0/200] [Batch 303/1367] [D loss: 0.245952, acc:  70%] [G loss: 9.717400] time: 0:23:31.405021\n",
      "[Epoch 0/200] [Batch 304/1367] [D loss: 0.218147, acc:  63%] [G loss: 4.024745] time: 0:23:35.938580\n",
      "[Epoch 0/200] [Batch 305/1367] [D loss: 0.243420, acc:  38%] [G loss: 8.198667] time: 0:23:40.504465\n",
      "[Epoch 0/200] [Batch 306/1367] [D loss: 0.187374, acc:  60%] [G loss: 11.002625] time: 0:23:45.077969\n",
      "[Epoch 0/200] [Batch 307/1367] [D loss: 0.240545, acc:  60%] [G loss: 13.432673] time: 0:23:49.658562\n",
      "[Epoch 0/200] [Batch 308/1367] [D loss: 0.220939, acc:  75%] [G loss: 14.488184] time: 0:23:54.214656\n",
      "[Epoch 0/200] [Batch 309/1367] [D loss: 0.231495, acc:  73%] [G loss: 9.407425] time: 0:23:58.759930\n",
      "[Epoch 0/200] [Batch 310/1367] [D loss: 0.271561, acc:  71%] [G loss: 7.453298] time: 0:24:03.314260\n",
      "[Epoch 0/200] [Batch 311/1367] [D loss: 0.219302, acc:  75%] [G loss: 8.708213] time: 0:24:07.901324\n",
      "[Epoch 0/200] [Batch 312/1367] [D loss: 0.241683, acc:  75%] [G loss: 7.315961] time: 0:24:12.475744\n",
      "[Epoch 0/200] [Batch 313/1367] [D loss: 0.227964, acc:  64%] [G loss: 13.003914] time: 0:24:17.049818\n",
      "[Epoch 0/200] [Batch 314/1367] [D loss: 0.242568, acc:  72%] [G loss: 7.206440] time: 0:24:21.611633\n",
      "[Epoch 0/200] [Batch 315/1367] [D loss: 0.252794, acc:  71%] [G loss: 5.840331] time: 0:24:26.180536\n",
      "[Epoch 0/200] [Batch 316/1367] [D loss: 0.245628, acc:  73%] [G loss: 7.521875] time: 0:24:30.754597\n",
      "[Epoch 0/200] [Batch 317/1367] [D loss: 0.306124, acc:  55%] [G loss: 8.174664] time: 0:24:35.324873\n",
      "[Epoch 0/200] [Batch 318/1367] [D loss: 0.299054, acc:  55%] [G loss: 11.434122] time: 0:24:39.894270\n",
      "[Epoch 0/200] [Batch 319/1367] [D loss: 0.276035, acc:  67%] [G loss: 6.051356] time: 0:24:44.476575\n",
      "[Epoch 0/200] [Batch 320/1367] [D loss: 0.270554, acc:  65%] [G loss: 8.502545] time: 0:24:49.055404\n",
      "[Epoch 0/200] [Batch 321/1367] [D loss: 0.280492, acc:  39%] [G loss: 6.618361] time: 0:24:53.617080\n",
      "[Epoch 0/200] [Batch 322/1367] [D loss: 0.231902, acc:  42%] [G loss: 8.153433] time: 0:24:58.211788\n",
      "[Epoch 0/200] [Batch 323/1367] [D loss: 0.298027, acc:  42%] [G loss: 21.627239] time: 0:25:02.778364\n",
      "[Epoch 0/200] [Batch 324/1367] [D loss: 0.312823, acc:  39%] [G loss: 9.019233] time: 0:25:07.349020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 325/1367] [D loss: 0.326539, acc:  35%] [G loss: 13.931264] time: 0:25:11.931440\n",
      "[Epoch 0/200] [Batch 326/1367] [D loss: 0.348671, acc:  31%] [G loss: 11.740917] time: 0:25:16.489312\n",
      "[Epoch 0/200] [Batch 327/1367] [D loss: 0.274197, acc:  40%] [G loss: 8.092691] time: 0:25:21.070484\n",
      "[Epoch 0/200] [Batch 328/1367] [D loss: 0.258818, acc:  54%] [G loss: 9.121635] time: 0:25:25.683067\n",
      "[Epoch 0/200] [Batch 329/1367] [D loss: 0.235704, acc:  61%] [G loss: 6.775208] time: 0:25:30.234294\n",
      "[Epoch 0/200] [Batch 330/1367] [D loss: 0.206312, acc:  72%] [G loss: 8.665713] time: 0:25:34.812078\n",
      "[Epoch 0/200] [Batch 331/1367] [D loss: 0.329946, acc:  67%] [G loss: 13.280827] time: 0:25:39.394583\n",
      "[Epoch 0/200] [Batch 332/1367] [D loss: 0.382868, acc:  61%] [G loss: 10.750318] time: 0:25:43.983359\n",
      "[Epoch 0/200] [Batch 333/1367] [D loss: 0.338246, acc:  63%] [G loss: 9.674364] time: 0:25:48.564069\n",
      "[Epoch 0/200] [Batch 334/1367] [D loss: 0.444545, acc:  57%] [G loss: 11.686063] time: 0:25:53.091270\n",
      "[Epoch 0/200] [Batch 335/1367] [D loss: 0.370192, acc:  80%] [G loss: 5.277380] time: 0:25:57.639857\n",
      "[Epoch 0/200] [Batch 336/1367] [D loss: 0.343963, acc:  49%] [G loss: 7.196899] time: 0:26:02.178779\n",
      "[Epoch 0/200] [Batch 337/1367] [D loss: 0.220424, acc:  62%] [G loss: 7.161088] time: 0:26:06.728907\n",
      "[Epoch 0/200] [Batch 338/1367] [D loss: 0.372822, acc:  42%] [G loss: 11.813899] time: 0:26:11.293312\n",
      "[Epoch 0/200] [Batch 339/1367] [D loss: 0.455528, acc:  43%] [G loss: 14.266569] time: 0:26:15.879752\n",
      "[Epoch 0/200] [Batch 340/1367] [D loss: 0.313788, acc:  47%] [G loss: 12.618808] time: 0:26:20.471653\n",
      "[Epoch 0/200] [Batch 341/1367] [D loss: 0.212256, acc:  68%] [G loss: 13.114107] time: 0:26:25.046950\n",
      "[Epoch 0/200] [Batch 342/1367] [D loss: 0.227615, acc:  73%] [G loss: 12.673011] time: 0:26:29.607449\n",
      "[Epoch 0/200] [Batch 343/1367] [D loss: 0.610898, acc:  73%] [G loss: 8.856102] time: 0:26:34.140999\n",
      "[Epoch 0/200] [Batch 344/1367] [D loss: 0.978226, acc:  60%] [G loss: 14.193470] time: 0:26:38.728297\n",
      "[Epoch 0/200] [Batch 345/1367] [D loss: 0.587034, acc:  71%] [G loss: 8.436463] time: 0:26:43.305026\n",
      "[Epoch 0/200] [Batch 346/1367] [D loss: 0.269146, acc:  73%] [G loss: 13.227205] time: 0:26:47.870882\n",
      "[Epoch 0/200] [Batch 347/1367] [D loss: 0.278693, acc:  42%] [G loss: 11.062038] time: 0:26:52.452737\n",
      "[Epoch 0/200] [Batch 348/1367] [D loss: 0.534392, acc:  19%] [G loss: 8.543295] time: 0:26:57.001808\n",
      "[Epoch 0/200] [Batch 349/1367] [D loss: 0.540814, acc:  18%] [G loss: 8.897167] time: 0:27:01.573106\n",
      "[Epoch 0/200] [Batch 350/1367] [D loss: 0.358747, acc:  38%] [G loss: 10.493447] time: 0:27:06.160675\n",
      "[Epoch 0/200] [Batch 351/1367] [D loss: 0.194093, acc:  67%] [G loss: 14.098168] time: 0:27:10.754173\n",
      "[Epoch 0/200] [Batch 352/1367] [D loss: 0.237029, acc:  63%] [G loss: 10.668982] time: 0:27:15.356696\n",
      "[Epoch 0/200] [Batch 353/1367] [D loss: 0.407529, acc:  62%] [G loss: 13.270382] time: 0:27:19.919381\n",
      "[Epoch 0/200] [Batch 354/1367] [D loss: 0.255455, acc:  62%] [G loss: 12.575344] time: 0:27:24.527576\n",
      "[Epoch 0/200] [Batch 355/1367] [D loss: 0.188116, acc:  82%] [G loss: 14.549811] time: 0:27:29.105236\n",
      "[Epoch 0/200] [Batch 356/1367] [D loss: 0.197403, acc:  70%] [G loss: 8.988721] time: 0:27:33.687765\n",
      "[Epoch 0/200] [Batch 357/1367] [D loss: 0.205881, acc:  70%] [G loss: 11.196157] time: 0:27:38.249563\n",
      "[Epoch 0/200] [Batch 358/1367] [D loss: 0.151342, acc:  89%] [G loss: 6.169692] time: 0:27:42.833036\n",
      "[Epoch 0/200] [Batch 359/1367] [D loss: 0.141898, acc:  91%] [G loss: 7.641542] time: 0:27:47.424498\n",
      "[Epoch 0/200] [Batch 360/1367] [D loss: 0.221024, acc:  74%] [G loss: 7.926758] time: 0:27:51.991642\n",
      "[Epoch 0/200] [Batch 361/1367] [D loss: 0.169874, acc:  82%] [G loss: 8.579290] time: 0:27:56.559716\n",
      "[Epoch 0/200] [Batch 362/1367] [D loss: 0.164752, acc:  77%] [G loss: 9.173122] time: 0:28:01.121356\n",
      "[Epoch 0/200] [Batch 363/1367] [D loss: 0.282611, acc:  67%] [G loss: 12.785606] time: 0:28:05.706410\n",
      "[Epoch 0/200] [Batch 364/1367] [D loss: 0.317723, acc:  64%] [G loss: 12.545532] time: 0:28:10.290025\n",
      "[Epoch 0/200] [Batch 365/1367] [D loss: 0.279870, acc:  62%] [G loss: 10.719110] time: 0:28:14.870681\n",
      "[Epoch 0/200] [Batch 366/1367] [D loss: 0.684264, acc:  25%] [G loss: 16.159754] time: 0:28:19.478348\n",
      "[Epoch 0/200] [Batch 367/1367] [D loss: 0.721170, acc:  21%] [G loss: 13.089679] time: 0:28:24.039177\n",
      "[Epoch 0/200] [Batch 368/1367] [D loss: 0.429426, acc:  36%] [G loss: 17.981897] time: 0:28:28.601482\n",
      "[Epoch 0/200] [Batch 369/1367] [D loss: 0.341606, acc:  66%] [G loss: 13.118631] time: 0:28:33.157517\n",
      "[Epoch 0/200] [Batch 370/1367] [D loss: 0.819861, acc:  66%] [G loss: 11.446261] time: 0:28:37.734158\n",
      "[Epoch 0/200] [Batch 371/1367] [D loss: 0.713949, acc:  61%] [G loss: 11.278018] time: 0:28:42.341005\n",
      "[Epoch 0/200] [Batch 372/1367] [D loss: 0.234358, acc:  74%] [G loss: 8.163604] time: 0:28:46.937110\n",
      "[Epoch 0/200] [Batch 373/1367] [D loss: 0.217616, acc:  62%] [G loss: 21.381773] time: 0:28:51.552338\n",
      "[Epoch 0/200] [Batch 374/1367] [D loss: 0.208265, acc:  69%] [G loss: 13.699349] time: 0:28:56.173100\n",
      "[Epoch 0/200] [Batch 375/1367] [D loss: 0.195898, acc:  83%] [G loss: 10.771801] time: 0:29:00.766912\n",
      "[Epoch 0/200] [Batch 376/1367] [D loss: 0.265714, acc:  66%] [G loss: 12.052474] time: 0:29:05.363270\n",
      "[Epoch 0/200] [Batch 377/1367] [D loss: 0.155182, acc:  82%] [G loss: 10.401011] time: 0:29:09.920926\n",
      "[Epoch 0/200] [Batch 378/1367] [D loss: 0.191428, acc:  81%] [G loss: 9.757687] time: 0:29:14.484688\n",
      "[Epoch 0/200] [Batch 379/1367] [D loss: 0.194838, acc:  87%] [G loss: 7.519503] time: 0:29:19.068909\n",
      "[Epoch 0/200] [Batch 380/1367] [D loss: 0.246384, acc:  60%] [G loss: 10.500620] time: 0:29:23.641805\n",
      "[Epoch 0/200] [Batch 381/1367] [D loss: 0.196579, acc:  64%] [G loss: 8.888895] time: 0:29:28.221213\n",
      "[Epoch 0/200] [Batch 382/1367] [D loss: 0.185945, acc:  73%] [G loss: 7.745486] time: 0:29:32.783666\n",
      "[Epoch 0/200] [Batch 383/1367] [D loss: 0.176007, acc:  82%] [G loss: 13.175901] time: 0:29:37.361696\n",
      "[Epoch 0/200] [Batch 384/1367] [D loss: 0.248062, acc:  71%] [G loss: 7.805500] time: 0:29:41.948848\n",
      "[Epoch 0/200] [Batch 385/1367] [D loss: 0.293477, acc:  70%] [G loss: 13.942325] time: 0:29:46.525835\n",
      "[Epoch 0/200] [Batch 386/1367] [D loss: 0.205626, acc:  82%] [G loss: 8.447234] time: 0:29:51.106521\n",
      "[Epoch 0/200] [Batch 387/1367] [D loss: 0.195354, acc:  76%] [G loss: 32.649746] time: 0:29:55.711859\n",
      "[Epoch 0/200] [Batch 388/1367] [D loss: 0.229573, acc:  49%] [G loss: 8.105511] time: 0:30:00.296230\n",
      "[Epoch 0/200] [Batch 389/1367] [D loss: 0.249973, acc:  52%] [G loss: 13.864046] time: 0:30:04.901798\n",
      "[Epoch 0/200] [Batch 390/1367] [D loss: 0.217342, acc:  66%] [G loss: 9.331185] time: 0:30:09.497207\n",
      "[Epoch 0/200] [Batch 391/1367] [D loss: 0.142293, acc:  90%] [G loss: 12.925032] time: 0:30:14.089679\n",
      "[Epoch 0/200] [Batch 392/1367] [D loss: 0.153125, acc:  78%] [G loss: 11.755359] time: 0:30:18.679222\n",
      "[Epoch 0/200] [Batch 393/1367] [D loss: 0.478335, acc:  61%] [G loss: 17.478743] time: 0:30:23.261992\n",
      "[Epoch 0/200] [Batch 394/1367] [D loss: 0.613958, acc:  77%] [G loss: 8.749867] time: 0:30:27.821699\n",
      "[Epoch 0/200] [Batch 395/1367] [D loss: 0.242901, acc:  58%] [G loss: 13.402794] time: 0:30:32.401133\n",
      "[Epoch 0/200] [Batch 396/1367] [D loss: 1.193767, acc:  15%] [G loss: 10.663538] time: 0:30:36.978881\n",
      "[Epoch 0/200] [Batch 397/1367] [D loss: 1.251494, acc:  11%] [G loss: 12.258710] time: 0:30:41.541492\n",
      "[Epoch 0/200] [Batch 398/1367] [D loss: 0.214558, acc:  63%] [G loss: 13.520417] time: 0:30:46.127578\n",
      "[Epoch 0/200] [Batch 399/1367] [D loss: 0.231766, acc:  70%] [G loss: 28.617599] time: 0:30:50.709902\n",
      "[Epoch 0/200] [Batch 400/1367] [D loss: 0.239996, acc:  59%] [G loss: 15.707472] time: 0:30:55.320503\n",
      "sampling\n",
      "[Epoch 0/200] [Batch 401/1367] [D loss: 0.229305, acc:  70%] [G loss: 15.078422] time: 0:31:01.435862\n",
      "[Epoch 0/200] [Batch 402/1367] [D loss: 0.210402, acc:  71%] [G loss: 13.744569] time: 0:31:06.018960\n",
      "[Epoch 0/200] [Batch 403/1367] [D loss: 0.243145, acc:  72%] [G loss: 12.561583] time: 0:31:10.594791\n",
      "[Epoch 0/200] [Batch 404/1367] [D loss: 0.362979, acc:  38%] [G loss: 9.667291] time: 0:31:15.171413\n",
      "[Epoch 0/200] [Batch 405/1367] [D loss: 0.364879, acc:  57%] [G loss: 8.879457] time: 0:31:19.738506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 406/1367] [D loss: 0.245213, acc:  72%] [G loss: 8.827468] time: 0:31:24.307722\n",
      "[Epoch 0/200] [Batch 407/1367] [D loss: 0.210905, acc:  79%] [G loss: 5.861099] time: 0:31:28.843992\n",
      "[Epoch 0/200] [Batch 408/1367] [D loss: 0.243526, acc:  51%] [G loss: 3.269996] time: 0:31:33.385069\n",
      "[Epoch 0/200] [Batch 409/1367] [D loss: 0.128901, acc:  93%] [G loss: 5.589230] time: 0:31:37.932920\n",
      "[Epoch 0/200] [Batch 410/1367] [D loss: 0.145992, acc:  79%] [G loss: 10.963527] time: 0:31:42.511890\n",
      "[Epoch 0/200] [Batch 411/1367] [D loss: 0.119609, acc:  85%] [G loss: 4.380083] time: 0:31:47.084238\n",
      "[Epoch 0/200] [Batch 412/1367] [D loss: 0.165220, acc:  80%] [G loss: 8.157821] time: 0:31:51.634656\n",
      "[Epoch 0/200] [Batch 413/1367] [D loss: 0.132566, acc:  93%] [G loss: 13.727651] time: 0:31:56.207085\n",
      "[Epoch 0/200] [Batch 414/1367] [D loss: 0.215682, acc:  52%] [G loss: 9.784692] time: 0:32:00.778471\n",
      "[Epoch 0/200] [Batch 415/1367] [D loss: 0.116822, acc:  91%] [G loss: 9.465447] time: 0:32:05.365745\n",
      "[Epoch 0/200] [Batch 416/1367] [D loss: 0.208958, acc:  74%] [G loss: 7.432499] time: 0:32:09.959428\n",
      "[Epoch 0/200] [Batch 417/1367] [D loss: 0.264739, acc:  71%] [G loss: 9.418196] time: 0:32:14.537548\n",
      "[Epoch 0/200] [Batch 418/1367] [D loss: 0.180823, acc:  81%] [G loss: 11.120002] time: 0:32:19.135876\n",
      "[Epoch 0/200] [Batch 419/1367] [D loss: 0.164922, acc:  90%] [G loss: 8.347990] time: 0:32:23.747148\n",
      "[Epoch 0/200] [Batch 420/1367] [D loss: 0.277566, acc:  56%] [G loss: 6.628400] time: 0:32:28.348705\n",
      "[Epoch 0/200] [Batch 421/1367] [D loss: 0.176263, acc:  85%] [G loss: 6.378853] time: 0:32:32.919710\n",
      "[Epoch 0/200] [Batch 422/1367] [D loss: 0.317142, acc:  65%] [G loss: 6.590598] time: 0:32:37.488426\n",
      "[Epoch 0/200] [Batch 423/1367] [D loss: 0.342552, acc:  59%] [G loss: 13.631920] time: 0:32:42.036449\n",
      "[Epoch 0/200] [Batch 424/1367] [D loss: 0.316166, acc:  49%] [G loss: 11.329823] time: 0:32:46.601390\n",
      "[Epoch 0/200] [Batch 425/1367] [D loss: 0.215869, acc:  78%] [G loss: 9.998097] time: 0:32:51.168202\n",
      "[Epoch 0/200] [Batch 426/1367] [D loss: 0.272114, acc:  56%] [G loss: 4.903374] time: 0:32:55.747919\n",
      "[Epoch 0/200] [Batch 427/1367] [D loss: 0.498009, acc:  38%] [G loss: 12.417063] time: 0:33:00.327535\n",
      "[Epoch 0/200] [Batch 428/1367] [D loss: 0.713188, acc:  32%] [G loss: 14.725403] time: 0:33:04.912231\n",
      "[Epoch 0/200] [Batch 429/1367] [D loss: 0.310714, acc:  53%] [G loss: 16.335642] time: 0:33:09.505042\n",
      "[Epoch 0/200] [Batch 430/1367] [D loss: 0.313475, acc:  70%] [G loss: 13.809786] time: 0:33:14.083947\n",
      "[Epoch 0/200] [Batch 431/1367] [D loss: 0.220463, acc:  72%] [G loss: 13.659595] time: 0:33:18.672332\n",
      "[Epoch 0/200] [Batch 432/1367] [D loss: 0.233266, acc:  78%] [G loss: 6.062515] time: 0:33:23.221276\n",
      "[Epoch 0/200] [Batch 433/1367] [D loss: 0.178393, acc:  73%] [G loss: 5.097446] time: 0:33:27.800751\n",
      "[Epoch 0/200] [Batch 434/1367] [D loss: 0.182032, acc:  74%] [G loss: 5.952290] time: 0:33:32.358691\n",
      "[Epoch 0/200] [Batch 435/1367] [D loss: 0.126937, acc:  94%] [G loss: 8.537474] time: 0:33:36.955301\n",
      "[Epoch 0/200] [Batch 436/1367] [D loss: 0.414528, acc:  20%] [G loss: 8.776073] time: 0:33:41.532878\n",
      "[Epoch 0/200] [Batch 437/1367] [D loss: 0.367830, acc:  29%] [G loss: 8.258425] time: 0:33:46.092166\n",
      "[Epoch 0/200] [Batch 438/1367] [D loss: 0.163253, acc:  91%] [G loss: 5.872783] time: 0:33:50.663780\n",
      "[Epoch 0/200] [Batch 439/1367] [D loss: 0.276409, acc:  60%] [G loss: 13.322176] time: 0:33:55.254887\n",
      "[Epoch 0/200] [Batch 440/1367] [D loss: 0.205600, acc:  72%] [G loss: 11.035229] time: 0:33:59.835150\n",
      "[Epoch 0/200] [Batch 441/1367] [D loss: 0.280867, acc:  68%] [G loss: 8.055540] time: 0:34:04.408223\n",
      "[Epoch 0/200] [Batch 442/1367] [D loss: 0.212317, acc:  59%] [G loss: 7.829667] time: 0:34:08.996880\n",
      "[Epoch 0/200] [Batch 443/1367] [D loss: 0.266071, acc:  37%] [G loss: 9.043839] time: 0:34:13.543324\n",
      "[Epoch 0/200] [Batch 444/1367] [D loss: 0.131689, acc:  74%] [G loss: 7.174648] time: 0:34:18.090112\n",
      "[Epoch 0/200] [Batch 445/1367] [D loss: 0.341716, acc:  75%] [G loss: 8.836909] time: 0:34:22.650818\n",
      "[Epoch 0/200] [Batch 446/1367] [D loss: 0.669239, acc:  76%] [G loss: 6.019753] time: 0:34:27.203981\n",
      "[Epoch 0/200] [Batch 447/1367] [D loss: 0.265945, acc:  58%] [G loss: 13.525263] time: 0:34:31.784789\n",
      "[Epoch 0/200] [Batch 448/1367] [D loss: 0.705031, acc:  18%] [G loss: 11.114889] time: 0:34:36.324803\n",
      "[Epoch 0/200] [Batch 449/1367] [D loss: 0.735209, acc:  21%] [G loss: 7.560205] time: 0:34:40.895906\n",
      "[Epoch 0/200] [Batch 450/1367] [D loss: 0.158993, acc:  81%] [G loss: 6.910206] time: 0:34:45.472254\n",
      "[Epoch 0/200] [Batch 451/1367] [D loss: 0.332312, acc:  71%] [G loss: 13.257682] time: 0:34:50.050281\n",
      "[Epoch 0/200] [Batch 452/1367] [D loss: 0.310480, acc:  77%] [G loss: 5.233963] time: 0:34:54.627840\n",
      "[Epoch 0/200] [Batch 453/1367] [D loss: 0.176651, acc:  87%] [G loss: 8.372959] time: 0:34:59.191239\n",
      "[Epoch 0/200] [Batch 454/1367] [D loss: 0.304194, acc:  36%] [G loss: 8.325129] time: 0:35:03.747054\n",
      "[Epoch 0/200] [Batch 455/1367] [D loss: 0.212752, acc:  69%] [G loss: 6.508940] time: 0:35:08.313287\n",
      "[Epoch 0/200] [Batch 456/1367] [D loss: 0.158338, acc:  68%] [G loss: 9.710031] time: 0:35:12.884516\n",
      "[Epoch 0/200] [Batch 457/1367] [D loss: 0.237454, acc:  63%] [G loss: 16.887447] time: 0:35:17.470932\n",
      "[Epoch 0/200] [Batch 458/1367] [D loss: 0.298364, acc:  60%] [G loss: 17.133879] time: 0:35:22.060659\n",
      "[Epoch 0/200] [Batch 459/1367] [D loss: 0.204640, acc:  76%] [G loss: 13.422803] time: 0:35:26.660207\n",
      "[Epoch 0/200] [Batch 460/1367] [D loss: 0.207802, acc:  66%] [G loss: 14.506244] time: 0:35:31.229147\n",
      "[Epoch 0/200] [Batch 461/1367] [D loss: 0.156462, acc:  80%] [G loss: 7.669836] time: 0:35:35.813983\n",
      "[Epoch 0/200] [Batch 462/1367] [D loss: 0.105381, acc:  87%] [G loss: 10.242101] time: 0:35:40.390810\n",
      "[Epoch 0/200] [Batch 463/1367] [D loss: 0.061028, acc:  97%] [G loss: 11.866305] time: 0:35:44.925225\n",
      "[Epoch 0/200] [Batch 464/1367] [D loss: 0.247227, acc:  59%] [G loss: 19.037201] time: 0:35:49.495670\n",
      "[Epoch 0/200] [Batch 465/1367] [D loss: 0.167032, acc:  87%] [G loss: 10.656939] time: 0:35:54.076881\n",
      "[Epoch 0/200] [Batch 466/1367] [D loss: 0.066296, acc:  97%] [G loss: 11.226261] time: 0:35:58.667122\n",
      "[Epoch 0/200] [Batch 467/1367] [D loss: 0.156346, acc:  79%] [G loss: 11.150529] time: 0:36:03.244788\n",
      "[Epoch 0/200] [Batch 468/1367] [D loss: 0.204043, acc:  72%] [G loss: 12.293695] time: 0:36:07.806480\n",
      "[Epoch 0/200] [Batch 469/1367] [D loss: 0.076500, acc:  89%] [G loss: 11.248845] time: 0:36:12.375827\n",
      "[Epoch 0/200] [Batch 470/1367] [D loss: 0.157805, acc:  75%] [G loss: 12.709532] time: 0:36:16.968173\n",
      "[Epoch 0/200] [Batch 471/1367] [D loss: 0.238154, acc:  64%] [G loss: 10.893783] time: 0:36:21.546725\n",
      "[Epoch 0/200] [Batch 472/1367] [D loss: 0.083963, acc:  94%] [G loss: 10.630309] time: 0:36:26.138967\n",
      "[Epoch 0/200] [Batch 473/1367] [D loss: 0.261435, acc:  78%] [G loss: 9.355038] time: 0:36:30.741565\n",
      "[Epoch 0/200] [Batch 474/1367] [D loss: 0.454693, acc:  73%] [G loss: 10.942112] time: 0:36:35.315308\n",
      "[Epoch 0/200] [Batch 475/1367] [D loss: 0.313851, acc:  55%] [G loss: 9.137617] time: 0:36:39.881067\n",
      "[Epoch 0/200] [Batch 476/1367] [D loss: 0.223086, acc:  63%] [G loss: 14.359617] time: 0:36:44.474868\n",
      "[Epoch 0/200] [Batch 477/1367] [D loss: 0.231967, acc:  57%] [G loss: 13.088900] time: 0:36:49.075923\n",
      "[Epoch 0/200] [Batch 478/1367] [D loss: 0.188798, acc:  58%] [G loss: 5.743740] time: 0:36:53.634150\n",
      "[Epoch 0/200] [Batch 479/1367] [D loss: 0.165810, acc:  81%] [G loss: 5.471869] time: 0:36:58.204455\n",
      "[Epoch 0/200] [Batch 480/1367] [D loss: 0.464061, acc:  20%] [G loss: 7.223634] time: 0:37:02.763385\n",
      "[Epoch 0/200] [Batch 481/1367] [D loss: 0.362265, acc:  23%] [G loss: 9.543830] time: 0:37:07.335088\n",
      "[Epoch 0/200] [Batch 482/1367] [D loss: 0.268261, acc:  35%] [G loss: 7.153512] time: 0:37:11.902834\n",
      "[Epoch 0/200] [Batch 483/1367] [D loss: 0.125950, acc:  89%] [G loss: 5.521297] time: 0:37:16.455994\n",
      "[Epoch 0/200] [Batch 484/1367] [D loss: 0.272738, acc:  49%] [G loss: 5.660895] time: 0:37:21.017054\n",
      "[Epoch 0/200] [Batch 485/1367] [D loss: 0.360078, acc:  29%] [G loss: 22.269869] time: 0:37:25.597694\n",
      "[Epoch 0/200] [Batch 486/1367] [D loss: 0.203951, acc:  73%] [G loss: 12.130018] time: 0:37:30.173876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 487/1367] [D loss: 0.130568, acc:  88%] [G loss: 6.475601] time: 0:37:34.736685\n",
      "[Epoch 0/200] [Batch 488/1367] [D loss: 0.229711, acc:  60%] [G loss: 12.080954] time: 0:37:39.254679\n",
      "[Epoch 0/200] [Batch 489/1367] [D loss: 0.055878, acc:  98%] [G loss: 11.749264] time: 0:37:43.824278\n",
      "[Epoch 0/200] [Batch 490/1367] [D loss: 0.088360, acc:  93%] [G loss: 11.994170] time: 0:37:48.404693\n",
      "[Epoch 0/200] [Batch 491/1367] [D loss: 0.098916, acc:  87%] [G loss: 8.931896] time: 0:37:52.976324\n",
      "[Epoch 0/200] [Batch 492/1367] [D loss: 0.129810, acc:  93%] [G loss: 12.212940] time: 0:37:57.577831\n",
      "[Epoch 0/200] [Batch 493/1367] [D loss: 0.132177, acc:  92%] [G loss: 8.654303] time: 0:38:02.140359\n",
      "[Epoch 0/200] [Batch 494/1367] [D loss: 0.198548, acc:  83%] [G loss: 10.283794] time: 0:38:06.700102\n",
      "[Epoch 0/200] [Batch 495/1367] [D loss: 0.693244, acc:  45%] [G loss: 12.766890] time: 0:38:11.275487\n",
      "[Epoch 0/200] [Batch 496/1367] [D loss: 0.196056, acc:  77%] [G loss: 19.372169] time: 0:38:15.863873\n",
      "[Epoch 0/200] [Batch 497/1367] [D loss: 0.708563, acc:  25%] [G loss: 11.713179] time: 0:38:20.451323\n",
      "[Epoch 0/200] [Batch 498/1367] [D loss: 0.656015, acc:  28%] [G loss: 10.817870] time: 0:38:25.047728\n",
      "[Epoch 0/200] [Batch 499/1367] [D loss: 0.755637, acc:  49%] [G loss: 8.588652] time: 0:38:29.597267\n",
      "[Epoch 0/200] [Batch 500/1367] [D loss: 1.139509, acc:  46%] [G loss: 7.321457] time: 0:38:34.158899\n",
      "[Epoch 0/200] [Batch 501/1367] [D loss: 0.239646, acc:  58%] [G loss: 14.869151] time: 0:38:38.756182\n",
      "[Epoch 0/200] [Batch 502/1367] [D loss: 0.275011, acc:  61%] [G loss: 13.188891] time: 0:38:43.327650\n",
      "[Epoch 0/200] [Batch 503/1367] [D loss: 0.149230, acc:  76%] [G loss: 15.549213] time: 0:38:47.917297\n",
      "[Epoch 0/200] [Batch 504/1367] [D loss: 0.092660, acc:  96%] [G loss: 15.979333] time: 0:38:52.503173\n",
      "[Epoch 0/200] [Batch 505/1367] [D loss: 0.095031, acc:  95%] [G loss: 21.918188] time: 0:38:57.070359\n",
      "[Epoch 0/200] [Batch 506/1367] [D loss: 0.113811, acc:  91%] [G loss: 11.115525] time: 0:39:01.619354\n",
      "[Epoch 0/200] [Batch 507/1367] [D loss: 0.472756, acc:  62%] [G loss: 18.689835] time: 0:39:06.218920\n",
      "[Epoch 0/200] [Batch 508/1367] [D loss: 0.375031, acc:  69%] [G loss: 15.939898] time: 0:39:10.813518\n",
      "[Epoch 0/200] [Batch 509/1367] [D loss: 0.337351, acc:  49%] [G loss: 5.990914] time: 0:39:15.381214\n",
      "[Epoch 0/200] [Batch 510/1367] [D loss: 0.272452, acc:  47%] [G loss: 4.104749] time: 0:39:19.955441\n",
      "[Epoch 0/200] [Batch 511/1367] [D loss: 0.119739, acc:  88%] [G loss: 11.948895] time: 0:39:24.556412\n",
      "[Epoch 0/200] [Batch 512/1367] [D loss: 0.148102, acc:  79%] [G loss: 9.612782] time: 0:39:29.122783\n",
      "[Epoch 0/200] [Batch 513/1367] [D loss: 0.342159, acc:  36%] [G loss: 7.285700] time: 0:39:33.691808\n",
      "[Epoch 0/200] [Batch 514/1367] [D loss: 0.170396, acc:  83%] [G loss: 8.712510] time: 0:39:38.256791\n",
      "[Epoch 0/200] [Batch 515/1367] [D loss: 0.094861, acc:  96%] [G loss: 11.303114] time: 0:39:42.838237\n",
      "[Epoch 0/200] [Batch 516/1367] [D loss: 0.073854, acc:  98%] [G loss: 9.715714] time: 0:39:47.444712\n",
      "[Epoch 0/200] [Batch 517/1367] [D loss: 0.071278, acc:  96%] [G loss: 5.496586] time: 0:39:52.021921\n",
      "[Epoch 0/200] [Batch 518/1367] [D loss: 0.163337, acc:  85%] [G loss: 19.663752] time: 0:39:56.612016\n",
      "[Epoch 0/200] [Batch 519/1367] [D loss: 0.052779, acc:  97%] [G loss: 9.189884] time: 0:40:01.270013\n",
      "[Epoch 0/200] [Batch 520/1367] [D loss: 0.097430, acc:  95%] [G loss: 12.515172] time: 0:40:05.836737\n",
      "[Epoch 0/200] [Batch 521/1367] [D loss: 0.225219, acc:  58%] [G loss: 9.744240] time: 0:40:10.398013\n",
      "[Epoch 0/200] [Batch 522/1367] [D loss: 0.277960, acc:  49%] [G loss: 10.072424] time: 0:40:14.965714\n",
      "[Epoch 0/200] [Batch 523/1367] [D loss: 0.276074, acc:  46%] [G loss: 11.335721] time: 0:40:19.530793\n",
      "[Epoch 0/200] [Batch 524/1367] [D loss: 0.348279, acc:  16%] [G loss: 11.891021] time: 0:40:24.109458\n",
      "[Epoch 0/200] [Batch 525/1367] [D loss: 0.253000, acc:  53%] [G loss: 9.919817] time: 0:40:28.691046\n",
      "[Epoch 0/200] [Batch 526/1367] [D loss: 0.094275, acc:  94%] [G loss: 7.934172] time: 0:40:33.275635\n",
      "[Epoch 0/200] [Batch 527/1367] [D loss: 0.188913, acc:  75%] [G loss: 11.230964] time: 0:40:37.844567\n",
      "[Epoch 0/200] [Batch 528/1367] [D loss: 0.303838, acc:  42%] [G loss: 14.987050] time: 0:40:42.406849\n",
      "[Epoch 0/200] [Batch 529/1367] [D loss: 0.163914, acc:  82%] [G loss: 11.387144] time: 0:40:46.967641\n",
      "[Epoch 0/200] [Batch 530/1367] [D loss: 0.078701, acc:  97%] [G loss: 10.760856] time: 0:40:51.555773\n",
      "[Epoch 0/200] [Batch 531/1367] [D loss: 0.111591, acc:  96%] [G loss: 5.986301] time: 0:40:56.144563\n",
      "[Epoch 0/200] [Batch 532/1367] [D loss: 0.093570, acc:  89%] [G loss: 7.771727] time: 0:41:00.691893\n",
      "[Epoch 0/200] [Batch 533/1367] [D loss: 0.201832, acc:  75%] [G loss: 5.718638] time: 0:41:05.258490\n",
      "[Epoch 0/200] [Batch 534/1367] [D loss: 0.257161, acc:  68%] [G loss: 10.963688] time: 0:41:09.815215\n",
      "[Epoch 0/200] [Batch 535/1367] [D loss: 0.052885, acc:  98%] [G loss: 12.234970] time: 0:41:14.356020\n",
      "[Epoch 0/200] [Batch 536/1367] [D loss: 0.074469, acc:  97%] [G loss: 10.308404] time: 0:41:18.933717\n",
      "[Epoch 0/200] [Batch 537/1367] [D loss: 0.056453, acc:  96%] [G loss: 12.494419] time: 0:41:23.510046\n",
      "[Epoch 0/200] [Batch 538/1367] [D loss: 0.336187, acc:  62%] [G loss: 12.822355] time: 0:41:28.069378\n",
      "[Epoch 0/200] [Batch 539/1367] [D loss: 0.227927, acc:  77%] [G loss: 12.749498] time: 0:41:32.647730\n",
      "[Epoch 0/200] [Batch 540/1367] [D loss: 0.234723, acc:  62%] [G loss: 13.257705] time: 0:41:37.250766\n",
      "[Epoch 0/200] [Batch 541/1367] [D loss: 0.365290, acc:  50%] [G loss: 6.328561] time: 0:41:41.821942\n",
      "[Epoch 0/200] [Batch 542/1367] [D loss: 0.294037, acc:  62%] [G loss: 7.630767] time: 0:41:46.367566\n",
      "[Epoch 0/200] [Batch 543/1367] [D loss: 0.167962, acc:  60%] [G loss: 10.152716] time: 0:41:50.924624\n",
      "[Epoch 0/200] [Batch 544/1367] [D loss: 0.142228, acc:  89%] [G loss: 10.874581] time: 0:41:55.511765\n",
      "[Epoch 0/200] [Batch 545/1367] [D loss: 0.164774, acc:  88%] [G loss: 6.287640] time: 0:42:00.072539\n",
      "[Epoch 0/200] [Batch 546/1367] [D loss: 0.276772, acc:  33%] [G loss: 8.083437] time: 0:42:04.647704\n",
      "[Epoch 0/200] [Batch 547/1367] [D loss: 0.306089, acc:  37%] [G loss: 11.566108] time: 0:42:09.202500\n",
      "[Epoch 0/200] [Batch 548/1367] [D loss: 0.152935, acc:  91%] [G loss: 5.735955] time: 0:42:13.762491\n",
      "[Epoch 0/200] [Batch 549/1367] [D loss: 0.150862, acc:  74%] [G loss: 7.624632] time: 0:42:18.322264\n",
      "[Epoch 0/200] [Batch 550/1367] [D loss: 0.351701, acc:  30%] [G loss: 9.355108] time: 0:42:22.883420\n",
      "[Epoch 0/200] [Batch 551/1367] [D loss: 0.248977, acc:  67%] [G loss: 11.156684] time: 0:42:27.464902\n",
      "[Epoch 0/200] [Batch 552/1367] [D loss: 0.147346, acc:  75%] [G loss: 13.216679] time: 0:42:32.038772\n",
      "[Epoch 0/200] [Batch 553/1367] [D loss: 0.199575, acc:  75%] [G loss: 12.180773] time: 0:42:36.622496\n",
      "[Epoch 0/200] [Batch 554/1367] [D loss: 0.081168, acc:  94%] [G loss: 15.102168] time: 0:42:41.207309\n",
      "[Epoch 0/200] [Batch 555/1367] [D loss: 0.089241, acc:  90%] [G loss: 12.293537] time: 0:42:45.785130\n",
      "[Epoch 0/200] [Batch 556/1367] [D loss: 0.084020, acc:  97%] [G loss: 5.672092] time: 0:42:50.347106\n",
      "[Epoch 0/200] [Batch 557/1367] [D loss: 0.169670, acc:  65%] [G loss: 5.133201] time: 0:42:54.899025\n",
      "[Epoch 0/200] [Batch 558/1367] [D loss: 0.175248, acc:  81%] [G loss: 10.946562] time: 0:42:59.475383\n",
      "[Epoch 0/200] [Batch 559/1367] [D loss: 0.294029, acc:  68%] [G loss: 10.105655] time: 0:43:04.034086\n",
      "[Epoch 0/200] [Batch 560/1367] [D loss: 0.167621, acc:  80%] [G loss: 11.774606] time: 0:43:08.585170\n",
      "[Epoch 0/200] [Batch 561/1367] [D loss: 0.131338, acc:  90%] [G loss: 10.431795] time: 0:43:13.145980\n",
      "[Epoch 0/200] [Batch 562/1367] [D loss: 0.098889, acc:  88%] [G loss: 10.499894] time: 0:43:17.721337\n",
      "[Epoch 0/200] [Batch 563/1367] [D loss: 0.462673, acc:   8%] [G loss: 3.254565] time: 0:43:22.268586\n",
      "[Epoch 0/200] [Batch 564/1367] [D loss: 0.103711, acc:  96%] [G loss: 14.963180] time: 0:43:26.834180\n",
      "[Epoch 0/200] [Batch 565/1367] [D loss: 0.068997, acc:  93%] [G loss: 8.018177] time: 0:43:31.417500\n",
      "[Epoch 0/200] [Batch 566/1367] [D loss: 0.112437, acc:  94%] [G loss: 9.393715] time: 0:43:36.010969\n",
      "[Epoch 0/200] [Batch 567/1367] [D loss: 0.396084, acc:  39%] [G loss: 9.964210] time: 0:43:40.589410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 568/1367] [D loss: 0.333642, acc:  30%] [G loss: 12.217206] time: 0:43:45.173229\n",
      "[Epoch 0/200] [Batch 569/1367] [D loss: 0.086744, acc:  94%] [G loss: 17.576910] time: 0:43:49.796776\n",
      "[Epoch 0/200] [Batch 570/1367] [D loss: 0.200946, acc:  74%] [G loss: 13.995552] time: 0:43:54.394248\n",
      "[Epoch 0/200] [Batch 571/1367] [D loss: 0.193306, acc:  78%] [G loss: 10.184498] time: 0:43:59.005567\n",
      "[Epoch 0/200] [Batch 572/1367] [D loss: 0.105780, acc:  94%] [G loss: 11.059899] time: 0:44:03.568973\n",
      "[Epoch 0/200] [Batch 573/1367] [D loss: 0.113235, acc:  95%] [G loss: 9.558752] time: 0:44:08.141264\n",
      "[Epoch 0/200] [Batch 574/1367] [D loss: 0.105472, acc:  92%] [G loss: 8.509269] time: 0:44:12.708854\n",
      "[Epoch 0/200] [Batch 575/1367] [D loss: 0.209527, acc:  71%] [G loss: 10.963487] time: 0:44:17.281842\n",
      "[Epoch 0/200] [Batch 576/1367] [D loss: 0.300115, acc:  59%] [G loss: 11.574969] time: 0:44:21.859127\n",
      "[Epoch 0/200] [Batch 577/1367] [D loss: 0.207988, acc:  56%] [G loss: 7.736421] time: 0:44:26.392960\n",
      "[Epoch 0/200] [Batch 578/1367] [D loss: 0.155143, acc:  81%] [G loss: 6.124803] time: 0:44:30.953051\n",
      "[Epoch 0/200] [Batch 579/1367] [D loss: 0.053467, acc:  93%] [G loss: 7.765728] time: 0:44:35.540421\n",
      "[Epoch 0/200] [Batch 580/1367] [D loss: 0.283926, acc:  82%] [G loss: 6.230275] time: 0:44:40.113187\n",
      "[Epoch 0/200] [Batch 581/1367] [D loss: 0.172538, acc:  87%] [G loss: 8.902557] time: 0:44:44.656231\n",
      "[Epoch 0/200] [Batch 582/1367] [D loss: 1.415524, acc:  15%] [G loss: 10.151876] time: 0:44:49.228959\n",
      "[Epoch 0/200] [Batch 583/1367] [D loss: 0.665269, acc:  37%] [G loss: 12.134832] time: 0:44:53.813480\n",
      "[Epoch 0/200] [Batch 584/1367] [D loss: 0.257081, acc:  83%] [G loss: 8.115195] time: 0:44:58.371120\n",
      "[Epoch 0/200] [Batch 585/1367] [D loss: 0.338326, acc:  83%] [G loss: 5.812400] time: 0:45:02.905852\n",
      "[Epoch 0/200] [Batch 586/1367] [D loss: 0.114582, acc:  88%] [G loss: 6.845089] time: 0:45:07.471878\n",
      "[Epoch 0/200] [Batch 587/1367] [D loss: 0.447961, acc:  58%] [G loss: 4.252944] time: 0:45:12.028877\n",
      "[Epoch 0/200] [Batch 588/1367] [D loss: 0.260082, acc:  44%] [G loss: 11.988353] time: 0:45:16.612612\n",
      "[Epoch 0/200] [Batch 589/1367] [D loss: 0.178883, acc:  76%] [G loss: 14.862861] time: 0:45:21.207881\n",
      "[Epoch 0/200] [Batch 590/1367] [D loss: 0.155876, acc:  84%] [G loss: 8.026375] time: 0:45:25.800317\n",
      "[Epoch 0/200] [Batch 591/1367] [D loss: 0.083688, acc:  97%] [G loss: 7.570637] time: 0:45:30.413635\n",
      "[Epoch 0/200] [Batch 592/1367] [D loss: 0.129816, acc:  93%] [G loss: 9.274678] time: 0:45:35.013581\n",
      "[Epoch 0/200] [Batch 593/1367] [D loss: 0.362461, acc:  42%] [G loss: 7.504134] time: 0:45:39.583799\n",
      "[Epoch 0/200] [Batch 594/1367] [D loss: 0.260634, acc:  71%] [G loss: 5.126204] time: 0:45:44.123096\n",
      "[Epoch 0/200] [Batch 595/1367] [D loss: 0.150431, acc:  89%] [G loss: 7.456244] time: 0:45:48.708958\n",
      "[Epoch 0/200] [Batch 596/1367] [D loss: 0.176434, acc:  69%] [G loss: 8.491832] time: 0:45:53.293145\n",
      "[Epoch 0/200] [Batch 597/1367] [D loss: 0.088295, acc:  98%] [G loss: 7.142574] time: 0:45:57.882251\n",
      "[Epoch 0/200] [Batch 598/1367] [D loss: 0.064850, acc:  96%] [G loss: 10.650958] time: 0:46:02.439381\n",
      "[Epoch 0/200] [Batch 599/1367] [D loss: 0.182756, acc:  72%] [G loss: 15.797100] time: 0:46:07.029458\n",
      "[Epoch 0/200] [Batch 600/1367] [D loss: 0.134829, acc:  91%] [G loss: 9.010892] time: 0:46:11.590727\n",
      "sampling\n",
      "[Epoch 0/200] [Batch 601/1367] [D loss: 0.116879, acc:  94%] [G loss: 13.232337] time: 0:46:17.390615\n",
      "[Epoch 0/200] [Batch 602/1367] [D loss: 0.208827, acc:  62%] [G loss: 12.155068] time: 0:46:21.962374\n",
      "[Epoch 0/200] [Batch 603/1367] [D loss: 0.083253, acc:  96%] [G loss: 12.301421] time: 0:46:26.551747\n",
      "[Epoch 0/200] [Batch 604/1367] [D loss: 0.145605, acc:  83%] [G loss: 13.984656] time: 0:46:31.112338\n",
      "[Epoch 0/200] [Batch 605/1367] [D loss: 0.034139, acc:  98%] [G loss: 7.951887] time: 0:46:35.671218\n",
      "[Epoch 0/200] [Batch 606/1367] [D loss: 0.041682, acc:  96%] [G loss: 11.099823] time: 0:46:40.233149\n",
      "[Epoch 0/200] [Batch 607/1367] [D loss: 0.019057, acc:  98%] [G loss: 5.731106] time: 0:46:44.823925\n",
      "[Epoch 0/200] [Batch 608/1367] [D loss: 0.105553, acc:  93%] [G loss: 6.123972] time: 0:46:49.421796\n",
      "[Epoch 0/200] [Batch 609/1367] [D loss: 0.050666, acc:  95%] [G loss: 12.893661] time: 0:46:53.993136\n",
      "[Epoch 0/200] [Batch 610/1367] [D loss: 0.041134, acc:  98%] [G loss: 8.268158] time: 0:46:58.546377\n",
      "[Epoch 0/200] [Batch 611/1367] [D loss: 0.087147, acc:  95%] [G loss: 6.770461] time: 0:47:03.105040\n",
      "[Epoch 0/200] [Batch 612/1367] [D loss: 0.021660, acc:  99%] [G loss: 7.204987] time: 0:47:07.672119\n",
      "[Epoch 0/200] [Batch 613/1367] [D loss: 0.070458, acc:  91%] [G loss: 6.220065] time: 0:47:12.265293\n",
      "[Epoch 0/200] [Batch 614/1367] [D loss: 0.361916, acc:  70%] [G loss: 5.519947] time: 0:47:16.855744\n",
      "[Epoch 0/200] [Batch 615/1367] [D loss: 0.049183, acc:  98%] [G loss: 7.119284] time: 0:47:21.417983\n",
      "[Epoch 0/200] [Batch 616/1367] [D loss: 0.113581, acc:  92%] [G loss: 4.732897] time: 0:47:25.969605\n",
      "[Epoch 0/200] [Batch 617/1367] [D loss: 0.174083, acc:  73%] [G loss: 5.319656] time: 0:47:30.530400\n",
      "[Epoch 0/200] [Batch 618/1367] [D loss: 0.203158, acc:  76%] [G loss: 6.282761] time: 0:47:35.109042\n",
      "[Epoch 0/200] [Batch 619/1367] [D loss: 0.289507, acc:  62%] [G loss: 5.828415] time: 0:47:39.701992\n",
      "[Epoch 0/200] [Batch 620/1367] [D loss: 0.194148, acc:  80%] [G loss: 11.331195] time: 0:47:44.265862\n",
      "[Epoch 0/200] [Batch 621/1367] [D loss: 0.207829, acc:  73%] [G loss: 15.868138] time: 0:47:48.849614\n",
      "[Epoch 0/200] [Batch 622/1367] [D loss: 0.197215, acc:  83%] [G loss: 11.599146] time: 0:47:53.437566\n",
      "[Epoch 0/200] [Batch 623/1367] [D loss: 0.160452, acc:  63%] [G loss: 10.496099] time: 0:47:58.011718\n",
      "[Epoch 0/200] [Batch 624/1367] [D loss: 0.123476, acc:  86%] [G loss: 8.875724] time: 0:48:02.566536\n",
      "[Epoch 0/200] [Batch 625/1367] [D loss: 0.054943, acc:  98%] [G loss: 3.351717] time: 0:48:07.117982\n",
      "[Epoch 0/200] [Batch 626/1367] [D loss: 0.059988, acc:  95%] [G loss: 5.084428] time: 0:48:11.662252\n",
      "[Epoch 0/200] [Batch 627/1367] [D loss: 0.278675, acc:  83%] [G loss: 5.201202] time: 0:48:16.219552\n",
      "[Epoch 0/200] [Batch 628/1367] [D loss: 0.041314, acc:  97%] [G loss: 7.448196] time: 0:48:20.775132\n",
      "[Epoch 0/200] [Batch 629/1367] [D loss: 0.267202, acc:  55%] [G loss: 6.431625] time: 0:48:25.321091\n",
      "[Epoch 0/200] [Batch 630/1367] [D loss: 0.095641, acc:  92%] [G loss: 11.095912] time: 0:48:29.884661\n",
      "[Epoch 0/200] [Batch 631/1367] [D loss: 0.051809, acc:  94%] [G loss: 7.148541] time: 0:48:34.436452\n",
      "[Epoch 0/200] [Batch 632/1367] [D loss: 0.671847, acc:  69%] [G loss: 7.766679] time: 0:48:38.993507\n",
      "[Epoch 0/200] [Batch 633/1367] [D loss: 0.691397, acc:  38%] [G loss: 10.627489] time: 0:48:43.546560\n",
      "[Epoch 0/200] [Batch 634/1367] [D loss: 0.608271, acc:   9%] [G loss: 11.474936] time: 0:48:48.122916\n",
      "[Epoch 0/200] [Batch 635/1367] [D loss: 0.498670, acc:  20%] [G loss: 5.876336] time: 0:48:52.664135\n",
      "[Epoch 0/200] [Batch 636/1367] [D loss: 0.324514, acc:  28%] [G loss: 9.166969] time: 0:48:57.245377\n",
      "[Epoch 0/200] [Batch 637/1367] [D loss: 0.297682, acc:  45%] [G loss: 3.664423] time: 0:49:01.789498\n",
      "[Epoch 0/200] [Batch 638/1367] [D loss: 0.119260, acc:  88%] [G loss: 7.763251] time: 0:49:06.360686\n",
      "[Epoch 0/200] [Batch 639/1367] [D loss: 0.229771, acc:  64%] [G loss: 6.756552] time: 0:49:10.926999\n",
      "[Epoch 0/200] [Batch 640/1367] [D loss: 0.352687, acc:  28%] [G loss: 7.357711] time: 0:49:15.485993\n",
      "[Epoch 0/200] [Batch 641/1367] [D loss: 0.094653, acc:  96%] [G loss: 7.096374] time: 0:49:20.039719\n",
      "[Epoch 0/200] [Batch 642/1367] [D loss: 0.198867, acc:  76%] [G loss: 10.914336] time: 0:49:24.612698\n",
      "[Epoch 0/200] [Batch 643/1367] [D loss: 0.200159, acc:  77%] [G loss: 5.986612] time: 0:49:29.160798\n",
      "[Epoch 0/200] [Batch 644/1367] [D loss: 0.103004, acc:  89%] [G loss: 12.906262] time: 0:49:33.729367\n",
      "[Epoch 0/200] [Batch 645/1367] [D loss: 0.045002, acc:  97%] [G loss: 8.274590] time: 0:49:38.281006\n",
      "[Epoch 0/200] [Batch 646/1367] [D loss: 0.096192, acc:  85%] [G loss: 12.126820] time: 0:49:42.860765\n",
      "[Epoch 0/200] [Batch 647/1367] [D loss: 0.076087, acc:  98%] [G loss: 7.877100] time: 0:49:47.406598\n",
      "[Epoch 0/200] [Batch 648/1367] [D loss: 0.277217, acc:  36%] [G loss: 7.967290] time: 0:49:51.982071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 649/1367] [D loss: 0.143249, acc:  94%] [G loss: 10.803240] time: 0:49:56.585741\n",
      "[Epoch 0/200] [Batch 650/1367] [D loss: 0.067887, acc:  93%] [G loss: 8.171731] time: 0:50:01.147387\n",
      "[Epoch 0/200] [Batch 651/1367] [D loss: 0.092081, acc:  90%] [G loss: 8.670677] time: 0:50:05.685158\n",
      "[Epoch 0/200] [Batch 652/1367] [D loss: 0.022622, acc:  99%] [G loss: 6.104499] time: 0:50:10.229344\n",
      "[Epoch 0/200] [Batch 653/1367] [D loss: 0.104663, acc:  90%] [G loss: 7.902576] time: 0:50:14.805951\n",
      "[Epoch 0/200] [Batch 654/1367] [D loss: 0.195434, acc:  62%] [G loss: 8.872333] time: 0:50:19.381372\n",
      "[Epoch 0/200] [Batch 655/1367] [D loss: 0.068429, acc:  97%] [G loss: 7.354537] time: 0:50:23.926797\n",
      "[Epoch 0/200] [Batch 656/1367] [D loss: 0.102613, acc:  97%] [G loss: 10.930370] time: 0:50:28.481233\n",
      "[Epoch 0/200] [Batch 657/1367] [D loss: 0.041448, acc:  99%] [G loss: 7.755072] time: 0:50:33.051924\n",
      "[Epoch 0/200] [Batch 658/1367] [D loss: 0.037407, acc:  97%] [G loss: 6.729511] time: 0:50:37.645786\n",
      "[Epoch 0/200] [Batch 659/1367] [D loss: 0.035393, acc:  98%] [G loss: 8.908261] time: 0:50:42.222322\n",
      "[Epoch 0/200] [Batch 660/1367] [D loss: 0.063146, acc:  98%] [G loss: 11.937895] time: 0:50:46.786240\n",
      "[Epoch 0/200] [Batch 661/1367] [D loss: 0.158732, acc:  88%] [G loss: 6.196842] time: 0:50:51.356307\n",
      "[Epoch 0/200] [Batch 662/1367] [D loss: 0.148357, acc:  68%] [G loss: 12.116332] time: 0:50:55.923986\n",
      "[Epoch 0/200] [Batch 663/1367] [D loss: 0.514726, acc:  38%] [G loss: 8.708658] time: 0:51:00.496462\n",
      "[Epoch 0/200] [Batch 664/1367] [D loss: 0.088547, acc:  93%] [G loss: 6.452362] time: 0:51:05.094572\n",
      "[Epoch 0/200] [Batch 665/1367] [D loss: 0.158122, acc:  85%] [G loss: 6.860546] time: 0:51:09.681949\n",
      "[Epoch 0/200] [Batch 666/1367] [D loss: 0.126461, acc:  96%] [G loss: 7.671643] time: 0:51:14.247613\n",
      "[Epoch 0/200] [Batch 667/1367] [D loss: 0.082001, acc:  97%] [G loss: 14.527658] time: 0:51:18.823630\n",
      "[Epoch 0/200] [Batch 668/1367] [D loss: 0.096472, acc:  90%] [G loss: 13.786897] time: 0:51:23.409640\n",
      "[Epoch 0/200] [Batch 669/1367] [D loss: 0.221076, acc:  66%] [G loss: 20.534935] time: 0:51:28.005559\n",
      "[Epoch 0/200] [Batch 670/1367] [D loss: 0.202089, acc:  73%] [G loss: 12.174398] time: 0:51:32.592840\n",
      "[Epoch 0/200] [Batch 671/1367] [D loss: 0.114977, acc:  89%] [G loss: 18.478191] time: 0:51:37.185226\n",
      "[Epoch 0/200] [Batch 672/1367] [D loss: 0.334764, acc:  42%] [G loss: 17.642483] time: 0:51:41.761172\n",
      "[Epoch 0/200] [Batch 673/1367] [D loss: 0.417936, acc:  29%] [G loss: 8.587559] time: 0:51:46.361725\n",
      "[Epoch 0/200] [Batch 674/1367] [D loss: 0.153207, acc:  76%] [G loss: 15.049560] time: 0:51:50.958557\n",
      "[Epoch 0/200] [Batch 675/1367] [D loss: 0.223790, acc:  80%] [G loss: 13.774304] time: 0:51:55.528580\n",
      "[Epoch 0/200] [Batch 676/1367] [D loss: 0.518558, acc:  67%] [G loss: 13.411046] time: 0:52:00.095206\n",
      "[Epoch 0/200] [Batch 677/1367] [D loss: 0.148139, acc:  85%] [G loss: 10.336664] time: 0:52:04.656204\n",
      "[Epoch 0/200] [Batch 678/1367] [D loss: 0.276734, acc:  41%] [G loss: 14.690230] time: 0:52:09.218851\n",
      "[Epoch 0/200] [Batch 679/1367] [D loss: 0.391320, acc:  53%] [G loss: 7.826707] time: 0:52:13.822856\n",
      "[Epoch 0/200] [Batch 680/1367] [D loss: 0.199421, acc:  59%] [G loss: 9.718197] time: 0:52:18.368430\n",
      "[Epoch 0/200] [Batch 681/1367] [D loss: 0.233584, acc:  62%] [G loss: 15.127619] time: 0:52:22.953433\n",
      "[Epoch 0/200] [Batch 682/1367] [D loss: 0.312741, acc:  61%] [G loss: 12.773941] time: 0:52:27.559872\n",
      "[Epoch 0/200] [Batch 683/1367] [D loss: 0.131157, acc:  82%] [G loss: 13.779358] time: 0:52:32.145812\n",
      "[Epoch 0/200] [Batch 684/1367] [D loss: 0.164543, acc:  71%] [G loss: 14.278942] time: 0:52:36.759839\n",
      "[Epoch 0/200] [Batch 685/1367] [D loss: 0.070789, acc:  93%] [G loss: 12.967729] time: 0:52:41.355773\n",
      "[Epoch 0/200] [Batch 686/1367] [D loss: 0.076257, acc:  89%] [G loss: 12.062654] time: 0:52:45.912046\n",
      "[Epoch 0/200] [Batch 687/1367] [D loss: 0.367114, acc:  32%] [G loss: 10.649069] time: 0:52:50.478957\n",
      "[Epoch 0/200] [Batch 688/1367] [D loss: 0.088179, acc:  97%] [G loss: 7.918892] time: 0:52:55.058080\n",
      "[Epoch 0/200] [Batch 689/1367] [D loss: 0.172252, acc:  70%] [G loss: 7.225608] time: 0:52:59.614895\n",
      "[Epoch 0/200] [Batch 690/1367] [D loss: 0.207827, acc:  67%] [G loss: 12.686959] time: 0:53:04.175537\n",
      "[Epoch 0/200] [Batch 691/1367] [D loss: 0.074197, acc:  97%] [G loss: 10.687307] time: 0:53:08.753730\n",
      "[Epoch 0/200] [Batch 692/1367] [D loss: 0.068986, acc:  95%] [G loss: 16.925297] time: 0:53:13.341049\n",
      "[Epoch 0/200] [Batch 693/1367] [D loss: 0.049646, acc:  98%] [G loss: 11.401286] time: 0:53:17.915467\n",
      "[Epoch 0/200] [Batch 694/1367] [D loss: 0.041122, acc:  98%] [G loss: 17.301344] time: 0:53:22.485484\n",
      "[Epoch 0/200] [Batch 695/1367] [D loss: 0.097514, acc:  86%] [G loss: 8.263010] time: 0:53:27.037820\n",
      "[Epoch 0/200] [Batch 696/1367] [D loss: 0.255843, acc:  67%] [G loss: 13.068487] time: 0:53:31.652173\n",
      "[Epoch 0/200] [Batch 697/1367] [D loss: 0.092850, acc:  87%] [G loss: 9.529194] time: 0:53:36.243448\n",
      "[Epoch 0/200] [Batch 698/1367] [D loss: 0.340966, acc:  45%] [G loss: 15.310476] time: 0:53:40.839933\n",
      "[Epoch 0/200] [Batch 699/1367] [D loss: 0.318525, acc:  38%] [G loss: 8.232049] time: 0:53:45.426723\n",
      "[Epoch 0/200] [Batch 700/1367] [D loss: 0.183898, acc:  69%] [G loss: 9.012140] time: 0:53:49.996343\n",
      "[Epoch 0/200] [Batch 701/1367] [D loss: 0.055373, acc:  97%] [G loss: 10.310922] time: 0:53:54.562611\n",
      "[Epoch 0/200] [Batch 702/1367] [D loss: 0.376288, acc:  70%] [G loss: 12.389136] time: 0:53:59.139243\n",
      "[Epoch 0/200] [Batch 703/1367] [D loss: 0.343244, acc:  70%] [G loss: 15.301517] time: 0:54:03.721921\n",
      "[Epoch 0/200] [Batch 704/1367] [D loss: 0.279526, acc:  56%] [G loss: 18.433229] time: 0:54:08.308754\n",
      "[Epoch 0/200] [Batch 705/1367] [D loss: 0.548957, acc:  28%] [G loss: 9.196909] time: 0:54:12.855087\n",
      "[Epoch 0/200] [Batch 706/1367] [D loss: 0.481132, acc:  48%] [G loss: 11.483047] time: 0:54:17.410415\n",
      "[Epoch 0/200] [Batch 707/1367] [D loss: 0.136302, acc:  80%] [G loss: 11.358193] time: 0:54:21.978319\n",
      "[Epoch 0/200] [Batch 708/1367] [D loss: 0.241356, acc:  64%] [G loss: 10.384651] time: 0:54:26.564934\n",
      "[Epoch 0/200] [Batch 709/1367] [D loss: 0.357891, acc:  40%] [G loss: 13.868056] time: 0:54:31.123227\n",
      "[Epoch 0/200] [Batch 710/1367] [D loss: 0.045899, acc:  98%] [G loss: 10.868978] time: 0:54:35.695318\n",
      "[Epoch 0/200] [Batch 711/1367] [D loss: 0.084407, acc:  89%] [G loss: 11.668767] time: 0:54:40.279630\n",
      "[Epoch 0/200] [Batch 712/1367] [D loss: 0.084409, acc:  88%] [G loss: 10.422317] time: 0:54:44.849908\n",
      "[Epoch 0/200] [Batch 713/1367] [D loss: 0.130697, acc:  87%] [G loss: 7.635818] time: 0:54:49.425053\n",
      "[Epoch 0/200] [Batch 714/1367] [D loss: 0.105649, acc:  97%] [G loss: 8.175505] time: 0:54:54.019106\n",
      "[Epoch 0/200] [Batch 715/1367] [D loss: 0.260138, acc:  80%] [G loss: 13.039356] time: 0:54:58.582963\n",
      "[Epoch 0/200] [Batch 716/1367] [D loss: 0.423251, acc:  47%] [G loss: 15.964188] time: 0:55:03.153731\n",
      "[Epoch 0/200] [Batch 717/1367] [D loss: 0.221616, acc:  78%] [G loss: 16.838289] time: 0:55:07.717027\n",
      "[Epoch 0/200] [Batch 718/1367] [D loss: 0.245113, acc:  68%] [G loss: 10.507221] time: 0:55:12.308614\n",
      "[Epoch 0/200] [Batch 719/1367] [D loss: 0.489610, acc:  18%] [G loss: 7.171313] time: 0:55:16.882114\n",
      "[Epoch 0/200] [Batch 720/1367] [D loss: 0.225029, acc:  60%] [G loss: 12.986057] time: 0:55:21.466543\n",
      "[Epoch 0/200] [Batch 721/1367] [D loss: 0.471310, acc:  34%] [G loss: 6.823653] time: 0:55:26.026284\n",
      "[Epoch 0/200] [Batch 722/1367] [D loss: 0.096206, acc:  92%] [G loss: 11.391743] time: 0:55:30.609710\n",
      "[Epoch 0/200] [Batch 723/1367] [D loss: 0.091471, acc:  92%] [G loss: 17.749113] time: 0:55:35.214061\n",
      "[Epoch 0/200] [Batch 724/1367] [D loss: 0.156907, acc:  75%] [G loss: 10.638346] time: 0:55:39.811314\n",
      "[Epoch 0/200] [Batch 725/1367] [D loss: 0.069104, acc:  92%] [G loss: 11.396031] time: 0:55:44.389830\n",
      "[Epoch 0/200] [Batch 726/1367] [D loss: 0.202004, acc:  72%] [G loss: 17.389957] time: 0:55:48.973908\n",
      "[Epoch 0/200] [Batch 727/1367] [D loss: 0.320839, acc:  54%] [G loss: 5.306588] time: 0:55:53.527618\n",
      "[Epoch 0/200] [Batch 728/1367] [D loss: 0.066253, acc:  98%] [G loss: 9.761927] time: 0:55:58.087007\n",
      "[Epoch 0/200] [Batch 729/1367] [D loss: 0.090220, acc:  99%] [G loss: 7.102426] time: 0:56:02.662394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 730/1367] [D loss: 0.046834, acc:  96%] [G loss: 11.810978] time: 0:56:07.222988\n",
      "[Epoch 0/200] [Batch 731/1367] [D loss: 0.153238, acc:  79%] [G loss: 7.656672] time: 0:56:11.784805\n",
      "[Epoch 0/200] [Batch 732/1367] [D loss: 0.088863, acc:  93%] [G loss: 7.165671] time: 0:56:16.385646\n",
      "[Epoch 0/200] [Batch 733/1367] [D loss: 0.301287, acc:  30%] [G loss: 7.171086] time: 0:56:20.917353\n",
      "[Epoch 0/200] [Batch 734/1367] [D loss: 0.293797, acc:  42%] [G loss: 10.768455] time: 0:56:25.488218\n",
      "[Epoch 0/200] [Batch 735/1367] [D loss: 0.054071, acc:  97%] [G loss: 12.920713] time: 0:56:30.064470\n",
      "[Epoch 0/200] [Batch 736/1367] [D loss: 0.082236, acc:  91%] [G loss: 10.076685] time: 0:56:34.636031\n",
      "[Epoch 0/200] [Batch 737/1367] [D loss: 0.143069, acc:  86%] [G loss: 13.406970] time: 0:56:39.202038\n",
      "[Epoch 0/200] [Batch 738/1367] [D loss: 0.164057, acc:  68%] [G loss: 9.607676] time: 0:56:43.758449\n",
      "[Epoch 0/200] [Batch 739/1367] [D loss: 0.247062, acc:  80%] [G loss: 8.475098] time: 0:56:48.338710\n",
      "[Epoch 0/200] [Batch 740/1367] [D loss: 0.530108, acc:  70%] [G loss: 8.584063] time: 0:56:52.904636\n",
      "[Epoch 0/200] [Batch 741/1367] [D loss: 0.099356, acc:  89%] [G loss: 9.173251] time: 0:56:57.476704\n",
      "[Epoch 0/200] [Batch 742/1367] [D loss: 0.531336, acc:  27%] [G loss: 8.488863] time: 0:57:02.046836\n",
      "[Epoch 0/200] [Batch 743/1367] [D loss: 0.209421, acc:  84%] [G loss: 15.733430] time: 0:57:06.625633\n",
      "[Epoch 0/200] [Batch 744/1367] [D loss: 0.144897, acc:  91%] [G loss: 11.629907] time: 0:57:11.225067\n",
      "[Epoch 0/200] [Batch 745/1367] [D loss: 0.190163, acc:  76%] [G loss: 17.257385] time: 0:57:15.800040\n",
      "[Epoch 0/200] [Batch 746/1367] [D loss: 0.282706, acc:  53%] [G loss: 16.273157] time: 0:57:20.370986\n",
      "[Epoch 0/200] [Batch 747/1367] [D loss: 0.125713, acc:  92%] [G loss: 15.059700] time: 0:57:24.984129\n",
      "[Epoch 0/200] [Batch 748/1367] [D loss: 0.125637, acc:  92%] [G loss: 15.875967] time: 0:57:29.587606\n",
      "[Epoch 0/200] [Batch 749/1367] [D loss: 0.412970, acc:  34%] [G loss: 9.477748] time: 0:57:34.159105\n",
      "[Epoch 0/200] [Batch 750/1367] [D loss: 0.281753, acc:  67%] [G loss: 8.691349] time: 0:57:38.718009\n",
      "[Epoch 0/200] [Batch 751/1367] [D loss: 0.240420, acc:  71%] [G loss: 16.631659] time: 0:57:43.305593\n",
      "[Epoch 0/200] [Batch 752/1367] [D loss: 0.195261, acc:  81%] [G loss: 11.373791] time: 0:57:47.873752\n",
      "[Epoch 0/200] [Batch 753/1367] [D loss: 0.260139, acc:  54%] [G loss: 13.168967] time: 0:57:52.431066\n",
      "[Epoch 0/200] [Batch 754/1367] [D loss: 0.110771, acc:  95%] [G loss: 9.599848] time: 0:57:57.010122\n",
      "[Epoch 0/200] [Batch 755/1367] [D loss: 0.218873, acc:  85%] [G loss: 6.581727] time: 0:58:01.546537\n",
      "[Epoch 0/200] [Batch 756/1367] [D loss: 0.530776, acc:  29%] [G loss: 9.400337] time: 0:58:06.136062\n",
      "[Epoch 0/200] [Batch 757/1367] [D loss: 0.192738, acc:  64%] [G loss: 10.082880] time: 0:58:10.701871\n",
      "[Epoch 0/200] [Batch 758/1367] [D loss: 0.357618, acc:  40%] [G loss: 10.604367] time: 0:58:15.298179\n",
      "[Epoch 0/200] [Batch 759/1367] [D loss: 0.046912, acc:  98%] [G loss: 15.877578] time: 0:58:19.879706\n",
      "[Epoch 0/200] [Batch 760/1367] [D loss: 0.435020, acc:  52%] [G loss: 7.619284] time: 0:58:24.441613\n",
      "[Epoch 0/200] [Batch 761/1367] [D loss: 0.333960, acc:  24%] [G loss: 9.399335] time: 0:58:28.996279\n",
      "[Epoch 0/200] [Batch 762/1367] [D loss: 0.103618, acc:  97%] [G loss: 8.805069] time: 0:58:33.542439\n",
      "[Epoch 0/200] [Batch 763/1367] [D loss: 0.055489, acc:  98%] [G loss: 9.164560] time: 0:58:38.147771\n",
      "[Epoch 0/200] [Batch 764/1367] [D loss: 0.057911, acc:  93%] [G loss: 10.322419] time: 0:58:42.735385\n",
      "[Epoch 0/200] [Batch 765/1367] [D loss: 0.267147, acc:  51%] [G loss: 14.147366] time: 0:58:47.297883\n",
      "[Epoch 0/200] [Batch 766/1367] [D loss: 0.177110, acc:  75%] [G loss: 16.567158] time: 0:58:51.877891\n",
      "[Epoch 0/200] [Batch 767/1367] [D loss: 0.375811, acc:  56%] [G loss: 11.657361] time: 0:58:56.450316\n",
      "[Epoch 0/200] [Batch 768/1367] [D loss: 0.086498, acc:  89%] [G loss: 11.698841] time: 0:59:01.015142\n",
      "[Epoch 0/200] [Batch 769/1367] [D loss: 0.076134, acc:  97%] [G loss: 10.677848] time: 0:59:05.593040\n",
      "[Epoch 0/200] [Batch 770/1367] [D loss: 0.144504, acc:  68%] [G loss: 10.144794] time: 0:59:10.161278\n",
      "[Epoch 0/200] [Batch 771/1367] [D loss: 0.029409, acc:  99%] [G loss: 10.412251] time: 0:59:14.726179\n",
      "[Epoch 0/200] [Batch 772/1367] [D loss: 0.041679, acc:  98%] [G loss: 11.953490] time: 0:59:19.289446\n",
      "[Epoch 0/200] [Batch 773/1367] [D loss: 0.076452, acc:  91%] [G loss: 13.225211] time: 0:59:23.871841\n",
      "[Epoch 0/200] [Batch 774/1367] [D loss: 0.113225, acc:  83%] [G loss: 16.164326] time: 0:59:28.457777\n",
      "[Epoch 0/200] [Batch 775/1367] [D loss: 0.230506, acc:  75%] [G loss: 9.907722] time: 0:59:33.035190\n",
      "[Epoch 0/200] [Batch 776/1367] [D loss: 0.185266, acc:  67%] [G loss: 7.694793] time: 0:59:37.632416\n",
      "[Epoch 0/200] [Batch 777/1367] [D loss: 0.112497, acc:  95%] [G loss: 16.978279] time: 0:59:42.251247\n",
      "[Epoch 0/200] [Batch 778/1367] [D loss: 0.052662, acc:  96%] [G loss: 12.797851] time: 0:59:46.824809\n",
      "[Epoch 0/200] [Batch 779/1367] [D loss: 0.032662, acc:  98%] [G loss: 12.256360] time: 0:59:51.394464\n",
      "[Epoch 0/200] [Batch 780/1367] [D loss: 0.022211, acc:  99%] [G loss: 10.162914] time: 0:59:55.959101\n",
      "[Epoch 0/200] [Batch 781/1367] [D loss: 0.045093, acc:  96%] [G loss: 11.612555] time: 1:00:00.537748\n",
      "[Epoch 0/200] [Batch 782/1367] [D loss: 0.046101, acc:  98%] [G loss: 8.886804] time: 1:00:05.156951\n",
      "[Epoch 0/200] [Batch 783/1367] [D loss: 0.075505, acc:  96%] [G loss: 9.698816] time: 1:00:09.748304\n",
      "[Epoch 0/200] [Batch 784/1367] [D loss: 0.294575, acc:  58%] [G loss: 6.454410] time: 1:00:14.324642\n",
      "[Epoch 0/200] [Batch 785/1367] [D loss: 0.130579, acc:  84%] [G loss: 14.011505] time: 1:00:18.927863\n",
      "[Epoch 0/200] [Batch 786/1367] [D loss: 0.238701, acc:  74%] [G loss: 10.479246] time: 1:00:23.536199\n",
      "[Epoch 0/200] [Batch 787/1367] [D loss: 0.130977, acc:  84%] [G loss: 9.074072] time: 1:00:28.111902\n",
      "[Epoch 0/200] [Batch 788/1367] [D loss: 0.233067, acc:  59%] [G loss: 10.498430] time: 1:00:32.686582\n",
      "[Epoch 0/200] [Batch 789/1367] [D loss: 0.543434, acc:  44%] [G loss: 16.855612] time: 1:00:37.251629\n",
      "[Epoch 0/200] [Batch 790/1367] [D loss: 0.288790, acc:  73%] [G loss: 16.696932] time: 1:00:41.835504\n",
      "[Epoch 0/200] [Batch 791/1367] [D loss: 0.340651, acc:  67%] [G loss: 9.650622] time: 1:00:46.419880\n",
      "[Epoch 0/200] [Batch 792/1367] [D loss: 0.181266, acc:  81%] [G loss: 13.830532] time: 1:00:50.978420\n",
      "[Epoch 0/200] [Batch 793/1367] [D loss: 0.539624, acc:  28%] [G loss: 9.172597] time: 1:00:55.548427\n",
      "[Epoch 0/200] [Batch 794/1367] [D loss: 0.136406, acc:  74%] [G loss: 9.499263] time: 1:01:00.100699\n",
      "[Epoch 0/200] [Batch 795/1367] [D loss: 0.038216, acc:  99%] [G loss: 10.922776] time: 1:01:04.689260\n",
      "[Epoch 0/200] [Batch 796/1367] [D loss: 0.032711, acc:  99%] [G loss: 10.216567] time: 1:01:09.285671\n",
      "[Epoch 0/200] [Batch 797/1367] [D loss: 0.027834, acc:  99%] [G loss: 10.746955] time: 1:01:13.870276\n",
      "[Epoch 0/200] [Batch 798/1367] [D loss: 0.047683, acc:  98%] [G loss: 10.031226] time: 1:01:18.435202\n",
      "[Epoch 0/200] [Batch 799/1367] [D loss: 0.148390, acc:  78%] [G loss: 13.043886] time: 1:01:23.031731\n",
      "[Epoch 0/200] [Batch 800/1367] [D loss: 0.067996, acc:  91%] [G loss: 11.775042] time: 1:01:27.605815\n",
      "sampling\n",
      "[Epoch 0/200] [Batch 801/1367] [D loss: 0.263335, acc:  59%] [G loss: 8.752242] time: 1:01:33.486221\n",
      "[Epoch 0/200] [Batch 802/1367] [D loss: 0.039854, acc:  96%] [G loss: 10.473533] time: 1:01:38.031810\n",
      "[Epoch 0/200] [Batch 803/1367] [D loss: 0.255535, acc:  82%] [G loss: 10.932236] time: 1:01:42.608084\n",
      "[Epoch 0/200] [Batch 804/1367] [D loss: 0.660889, acc:  54%] [G loss: 7.958185] time: 1:01:47.178935\n",
      "[Epoch 0/200] [Batch 805/1367] [D loss: 0.408429, acc:  37%] [G loss: 14.716375] time: 1:01:51.748362\n",
      "[Epoch 0/200] [Batch 806/1367] [D loss: 0.323927, acc:  30%] [G loss: 17.023838] time: 1:01:56.329084\n",
      "[Epoch 0/200] [Batch 807/1367] [D loss: 0.321647, acc:  36%] [G loss: 13.494197] time: 1:02:00.921830\n",
      "[Epoch 0/200] [Batch 808/1367] [D loss: 0.356486, acc:  42%] [G loss: 8.826041] time: 1:02:05.483447\n",
      "[Epoch 0/200] [Batch 809/1367] [D loss: 0.148060, acc:  90%] [G loss: 9.664489] time: 1:02:10.054924\n",
      "[Epoch 0/200] [Batch 810/1367] [D loss: 0.229693, acc:  63%] [G loss: 13.510886] time: 1:02:14.634420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 811/1367] [D loss: 0.227450, acc:  63%] [G loss: 10.556965] time: 1:02:19.207272\n",
      "[Epoch 0/200] [Batch 812/1367] [D loss: 0.086290, acc:  96%] [G loss: 6.520485] time: 1:02:23.768812\n",
      "[Epoch 0/200] [Batch 813/1367] [D loss: 0.055718, acc:  92%] [G loss: 8.823896] time: 1:02:28.333468\n",
      "[Epoch 0/200] [Batch 814/1367] [D loss: 0.596462, acc:  54%] [G loss: 11.422032] time: 1:02:32.906576\n",
      "[Epoch 0/200] [Batch 815/1367] [D loss: 0.097750, acc:  88%] [G loss: 10.720890] time: 1:02:37.481794\n",
      "[Epoch 0/200] [Batch 816/1367] [D loss: 0.063080, acc:  94%] [G loss: 13.070192] time: 1:02:42.047647\n",
      "[Epoch 0/200] [Batch 817/1367] [D loss: 0.020470, acc:  99%] [G loss: 12.887744] time: 1:02:46.661109\n",
      "[Epoch 0/200] [Batch 818/1367] [D loss: 0.066837, acc:  93%] [G loss: 10.340428] time: 1:02:51.250211\n",
      "[Epoch 0/200] [Batch 819/1367] [D loss: 0.063597, acc:  98%] [G loss: 5.823801] time: 1:02:55.794394\n",
      "[Epoch 0/200] [Batch 820/1367] [D loss: 0.033290, acc:  99%] [G loss: 9.168174] time: 1:03:00.359283\n",
      "[Epoch 0/200] [Batch 821/1367] [D loss: 0.237334, acc:  76%] [G loss: 9.456573] time: 1:03:04.928364\n",
      "[Epoch 0/200] [Batch 822/1367] [D loss: 0.071478, acc:  93%] [G loss: 8.543353] time: 1:03:09.512566\n",
      "[Epoch 0/200] [Batch 823/1367] [D loss: 0.191945, acc:  67%] [G loss: 10.242930] time: 1:03:14.092606\n",
      "[Epoch 0/200] [Batch 824/1367] [D loss: 0.225422, acc:  61%] [G loss: 19.682728] time: 1:03:18.690445\n",
      "[Epoch 0/200] [Batch 825/1367] [D loss: 0.282410, acc:  42%] [G loss: 10.792156] time: 1:03:23.304359\n",
      "[Epoch 0/200] [Batch 826/1367] [D loss: 0.238961, acc:  54%] [G loss: 10.891397] time: 1:03:27.877512\n",
      "[Epoch 0/200] [Batch 827/1367] [D loss: 0.166117, acc:  71%] [G loss: 9.835814] time: 1:03:32.447015\n",
      "[Epoch 0/200] [Batch 828/1367] [D loss: 0.152249, acc:  85%] [G loss: 13.835574] time: 1:03:37.026609\n",
      "[Epoch 0/200] [Batch 829/1367] [D loss: 0.179744, acc:  84%] [G loss: 22.913525] time: 1:03:41.592179\n",
      "[Epoch 0/200] [Batch 830/1367] [D loss: 0.168787, acc:  80%] [G loss: 10.425427] time: 1:03:46.161279\n",
      "[Epoch 0/200] [Batch 831/1367] [D loss: 0.219404, acc:  56%] [G loss: 14.621054] time: 1:03:50.722462\n",
      "[Epoch 0/200] [Batch 832/1367] [D loss: 0.267044, acc:  50%] [G loss: 15.581484] time: 1:03:55.266732\n",
      "[Epoch 0/200] [Batch 833/1367] [D loss: 0.115854, acc:  92%] [G loss: 10.221460] time: 1:03:59.834594\n",
      "[Epoch 0/200] [Batch 834/1367] [D loss: 0.167724, acc:  64%] [G loss: 9.927782] time: 1:04:04.384561\n",
      "[Epoch 0/200] [Batch 835/1367] [D loss: 0.041959, acc:  96%] [G loss: 7.042088] time: 1:04:08.935727\n",
      "[Epoch 0/200] [Batch 836/1367] [D loss: 0.110453, acc:  91%] [G loss: 9.207582] time: 1:04:13.505480\n",
      "[Epoch 0/200] [Batch 837/1367] [D loss: 0.070768, acc:  92%] [G loss: 6.765834] time: 1:04:18.079244\n",
      "[Epoch 0/200] [Batch 838/1367] [D loss: 0.097612, acc:  91%] [G loss: 8.069946] time: 1:04:22.649015\n",
      "[Epoch 0/200] [Batch 839/1367] [D loss: 0.073907, acc:  97%] [G loss: 8.510594] time: 1:04:27.224106\n",
      "[Epoch 0/200] [Batch 840/1367] [D loss: 0.147858, acc:  81%] [G loss: 8.951899] time: 1:04:31.787764\n",
      "[Epoch 0/200] [Batch 841/1367] [D loss: 0.217396, acc:  72%] [G loss: 14.378975] time: 1:04:36.373151\n",
      "[Epoch 0/200] [Batch 842/1367] [D loss: 0.431277, acc:  47%] [G loss: 4.153316] time: 1:04:40.905687\n",
      "[Epoch 0/200] [Batch 843/1367] [D loss: 0.107549, acc:  94%] [G loss: 9.812563] time: 1:04:45.464223\n",
      "[Epoch 0/200] [Batch 844/1367] [D loss: 0.071060, acc:  96%] [G loss: 10.894480] time: 1:04:50.032239\n",
      "[Epoch 0/200] [Batch 845/1367] [D loss: 0.154297, acc:  72%] [G loss: 12.586898] time: 1:04:54.607084\n",
      "[Epoch 0/200] [Batch 846/1367] [D loss: 0.072832, acc:  94%] [G loss: 4.940633] time: 1:04:59.181294\n",
      "[Epoch 0/200] [Batch 847/1367] [D loss: 0.120994, acc:  84%] [G loss: 9.798952] time: 1:05:03.760588\n",
      "[Epoch 0/200] [Batch 848/1367] [D loss: 0.191708, acc:  69%] [G loss: 17.612484] time: 1:05:08.365837\n",
      "[Epoch 0/200] [Batch 849/1367] [D loss: 0.240958, acc:  75%] [G loss: 13.460948] time: 1:05:12.940500\n",
      "[Epoch 0/200] [Batch 850/1367] [D loss: 0.120154, acc:  84%] [G loss: 15.688970] time: 1:05:17.508577\n",
      "[Epoch 0/200] [Batch 851/1367] [D loss: 0.101579, acc:  92%] [G loss: 8.928069] time: 1:05:22.059711\n",
      "[Epoch 0/200] [Batch 852/1367] [D loss: 0.336191, acc:  32%] [G loss: 11.627119] time: 1:05:26.631329\n",
      "[Epoch 0/200] [Batch 853/1367] [D loss: 0.131032, acc:  83%] [G loss: 9.006480] time: 1:05:31.195550\n",
      "[Epoch 0/200] [Batch 854/1367] [D loss: 0.168982, acc:  77%] [G loss: 8.292751] time: 1:05:35.773376\n",
      "[Epoch 0/200] [Batch 855/1367] [D loss: 0.131397, acc:  90%] [G loss: 9.851310] time: 1:05:40.362764\n",
      "[Epoch 0/200] [Batch 856/1367] [D loss: 0.057570, acc:  98%] [G loss: 11.121561] time: 1:05:44.942013\n",
      "[Epoch 0/200] [Batch 857/1367] [D loss: 0.456140, acc:  25%] [G loss: 7.577582] time: 1:05:49.532643\n",
      "[Epoch 0/200] [Batch 858/1367] [D loss: 0.286825, acc:  52%] [G loss: 5.348773] time: 1:05:54.084914\n",
      "[Epoch 0/200] [Batch 859/1367] [D loss: 0.483501, acc:  25%] [G loss: 6.825064] time: 1:05:58.645414\n",
      "[Epoch 0/200] [Batch 860/1367] [D loss: 0.732335, acc:  24%] [G loss: 8.129594] time: 1:06:03.213620\n",
      "[Epoch 0/200] [Batch 861/1367] [D loss: 0.337021, acc:  39%] [G loss: 11.153144] time: 1:06:07.795508\n",
      "[Epoch 0/200] [Batch 862/1367] [D loss: 0.423350, acc:  32%] [G loss: 6.956553] time: 1:06:12.353453\n",
      "[Epoch 0/200] [Batch 863/1367] [D loss: 0.217178, acc:  74%] [G loss: 10.481693] time: 1:06:16.918015\n",
      "[Epoch 0/200] [Batch 864/1367] [D loss: 0.083946, acc:  97%] [G loss: 12.162827] time: 1:06:21.494354\n",
      "[Epoch 0/200] [Batch 865/1367] [D loss: 0.031165, acc:  99%] [G loss: 15.392173] time: 1:06:26.061425\n",
      "[Epoch 0/200] [Batch 866/1367] [D loss: 0.064651, acc:  99%] [G loss: 8.004645] time: 1:06:30.630816\n",
      "[Epoch 0/200] [Batch 867/1367] [D loss: 0.033308, acc:  99%] [G loss: 5.554439] time: 1:06:35.204269\n",
      "[Epoch 0/200] [Batch 868/1367] [D loss: 0.163502, acc:  89%] [G loss: 7.254133] time: 1:06:39.790009\n",
      "[Epoch 0/200] [Batch 869/1367] [D loss: 0.052339, acc:  96%] [G loss: 8.109888] time: 1:06:44.350056\n",
      "[Epoch 0/200] [Batch 870/1367] [D loss: 0.080747, acc:  86%] [G loss: 7.612274] time: 1:06:48.902191\n",
      "[Epoch 0/200] [Batch 871/1367] [D loss: 0.301638, acc:  47%] [G loss: 9.175316] time: 1:06:53.475259\n",
      "[Epoch 0/200] [Batch 872/1367] [D loss: 0.502714, acc:   4%] [G loss: 9.605855] time: 1:06:58.053914\n",
      "[Epoch 0/200] [Batch 873/1367] [D loss: 0.179476, acc:  57%] [G loss: 6.425407] time: 1:07:02.637056\n",
      "[Epoch 0/200] [Batch 874/1367] [D loss: 0.317831, acc:  21%] [G loss: 8.606832] time: 1:07:07.197891\n",
      "[Epoch 0/200] [Batch 875/1367] [D loss: 0.294884, acc:  49%] [G loss: 6.950575] time: 1:07:11.761759\n",
      "[Epoch 0/200] [Batch 876/1367] [D loss: 0.248114, acc:  66%] [G loss: 11.238825] time: 1:07:16.341521\n",
      "[Epoch 0/200] [Batch 877/1367] [D loss: 0.591547, acc:   3%] [G loss: 10.172594] time: 1:07:20.922958\n",
      "[Epoch 0/200] [Batch 878/1367] [D loss: 0.332856, acc:  35%] [G loss: 5.336590] time: 1:07:25.475903\n",
      "[Epoch 0/200] [Batch 879/1367] [D loss: 0.236463, acc:  68%] [G loss: 16.509380] time: 1:07:30.054550\n",
      "[Epoch 0/200] [Batch 880/1367] [D loss: 0.185185, acc:  76%] [G loss: 5.627069] time: 1:07:34.616932\n",
      "[Epoch 0/200] [Batch 881/1367] [D loss: 0.233973, acc:  62%] [G loss: 6.794559] time: 1:07:39.188979\n",
      "[Epoch 0/200] [Batch 882/1367] [D loss: 0.152030, acc:  94%] [G loss: 12.613088] time: 1:07:43.789141\n",
      "[Epoch 0/200] [Batch 883/1367] [D loss: 0.209219, acc:  53%] [G loss: 7.350030] time: 1:07:48.371668\n",
      "[Epoch 0/200] [Batch 884/1367] [D loss: 0.177197, acc:  78%] [G loss: 9.455453] time: 1:07:52.931710\n",
      "[Epoch 0/200] [Batch 885/1367] [D loss: 0.096822, acc:  94%] [G loss: 11.978112] time: 1:07:57.511004\n",
      "[Epoch 0/200] [Batch 886/1367] [D loss: 0.187276, acc:  70%] [G loss: 15.318000] time: 1:08:02.092203\n",
      "[Epoch 0/200] [Batch 887/1367] [D loss: 0.240311, acc:  67%] [G loss: 8.267112] time: 1:08:06.657479\n",
      "[Epoch 0/200] [Batch 888/1367] [D loss: 0.018282, acc:  99%] [G loss: 9.919612] time: 1:08:11.226386\n",
      "[Epoch 0/200] [Batch 889/1367] [D loss: 0.153576, acc:  93%] [G loss: 13.121699] time: 1:08:15.791975\n",
      "[Epoch 0/200] [Batch 890/1367] [D loss: 0.170099, acc:  71%] [G loss: 6.647007] time: 1:08:20.340641\n",
      "[Epoch 0/200] [Batch 891/1367] [D loss: 0.108559, acc:  96%] [G loss: 10.009262] time: 1:08:24.887549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 892/1367] [D loss: 0.158807, acc:  84%] [G loss: 6.719377] time: 1:08:29.424958\n",
      "[Epoch 0/200] [Batch 893/1367] [D loss: 0.185842, acc:  75%] [G loss: 6.973210] time: 1:08:33.970088\n",
      "[Epoch 0/200] [Batch 894/1367] [D loss: 0.132074, acc:  81%] [G loss: 8.476695] time: 1:08:38.537439\n",
      "[Epoch 0/200] [Batch 895/1367] [D loss: 0.094157, acc:  86%] [G loss: 5.891695] time: 1:08:43.086049\n",
      "[Epoch 0/200] [Batch 896/1367] [D loss: 0.081309, acc:  90%] [G loss: 7.581056] time: 1:08:47.646659\n",
      "[Epoch 0/200] [Batch 897/1367] [D loss: 0.069234, acc:  94%] [G loss: 8.278257] time: 1:08:52.183590\n",
      "[Epoch 0/200] [Batch 898/1367] [D loss: 0.157257, acc:  91%] [G loss: 7.282662] time: 1:08:56.722967\n",
      "[Epoch 0/200] [Batch 899/1367] [D loss: 0.468792, acc:  51%] [G loss: 5.409813] time: 1:09:01.270545\n",
      "[Epoch 0/200] [Batch 900/1367] [D loss: 0.424215, acc:  53%] [G loss: 6.605793] time: 1:09:05.832979\n",
      "[Epoch 0/200] [Batch 901/1367] [D loss: 0.249067, acc:  63%] [G loss: 12.432085] time: 1:09:10.413258\n",
      "[Epoch 0/200] [Batch 902/1367] [D loss: 0.265598, acc:  50%] [G loss: 12.948492] time: 1:09:14.982758\n",
      "[Epoch 0/200] [Batch 903/1367] [D loss: 0.083527, acc:  96%] [G loss: 7.115769] time: 1:09:19.520329\n",
      "[Epoch 0/200] [Batch 904/1367] [D loss: 0.369910, acc:  35%] [G loss: 8.878135] time: 1:09:24.099264\n",
      "[Epoch 0/200] [Batch 905/1367] [D loss: 0.137844, acc:  85%] [G loss: 11.239783] time: 1:09:28.682746\n",
      "[Epoch 0/200] [Batch 906/1367] [D loss: 0.305972, acc:  77%] [G loss: 7.598871] time: 1:09:33.261315\n",
      "[Epoch 0/200] [Batch 907/1367] [D loss: 0.785094, acc:  28%] [G loss: 6.000210] time: 1:09:37.799772\n",
      "[Epoch 0/200] [Batch 908/1367] [D loss: 0.597783, acc:   3%] [G loss: 7.584158] time: 1:09:42.369529\n",
      "[Epoch 0/200] [Batch 909/1367] [D loss: 0.496798, acc:  20%] [G loss: 4.833166] time: 1:09:46.938254\n",
      "[Epoch 0/200] [Batch 910/1367] [D loss: 0.449703, acc:  24%] [G loss: 6.363309] time: 1:09:51.513964\n",
      "[Epoch 0/200] [Batch 911/1367] [D loss: 0.344527, acc:  34%] [G loss: 6.234303] time: 1:09:56.114813\n",
      "[Epoch 0/200] [Batch 912/1367] [D loss: 0.143293, acc:  88%] [G loss: 11.991179] time: 1:10:00.674792\n",
      "[Epoch 0/200] [Batch 913/1367] [D loss: 0.093134, acc:  93%] [G loss: 15.479959] time: 1:10:05.248562\n",
      "[Epoch 0/200] [Batch 914/1367] [D loss: 0.157496, acc:  75%] [G loss: 13.520935] time: 1:10:09.840942\n",
      "[Epoch 0/200] [Batch 915/1367] [D loss: 0.120654, acc:  85%] [G loss: 10.145669] time: 1:10:14.420659\n",
      "[Epoch 0/200] [Batch 916/1367] [D loss: 0.090160, acc:  96%] [G loss: 13.069054] time: 1:10:18.991543\n",
      "[Epoch 0/200] [Batch 917/1367] [D loss: 0.045928, acc:  99%] [G loss: 15.694893] time: 1:10:23.591651\n",
      "[Epoch 0/200] [Batch 918/1367] [D loss: 0.091546, acc:  93%] [G loss: 21.600595] time: 1:10:28.193712\n",
      "[Epoch 0/200] [Batch 919/1367] [D loss: 0.074072, acc:  93%] [G loss: 14.135139] time: 1:10:32.767544\n",
      "[Epoch 0/200] [Batch 920/1367] [D loss: 0.139654, acc:  93%] [G loss: 11.747504] time: 1:10:37.336534\n",
      "[Epoch 0/200] [Batch 921/1367] [D loss: 0.067300, acc:  99%] [G loss: 7.368784] time: 1:10:41.899432\n",
      "[Epoch 0/200] [Batch 922/1367] [D loss: 0.119160, acc:  96%] [G loss: 15.998663] time: 1:10:46.472365\n",
      "[Epoch 0/200] [Batch 923/1367] [D loss: 0.110567, acc:  97%] [G loss: 7.629484] time: 1:10:51.069491\n",
      "[Epoch 0/200] [Batch 924/1367] [D loss: 0.052235, acc:  98%] [G loss: 11.470604] time: 1:10:55.648181\n",
      "[Epoch 0/200] [Batch 925/1367] [D loss: 0.345695, acc:  66%] [G loss: 8.989984] time: 1:11:00.213926\n",
      "[Epoch 0/200] [Batch 926/1367] [D loss: 0.366620, acc:  37%] [G loss: 10.252498] time: 1:11:04.800527\n",
      "[Epoch 0/200] [Batch 927/1367] [D loss: 0.316015, acc:  49%] [G loss: 7.400646] time: 1:11:09.467302\n",
      "[Epoch 0/200] [Batch 928/1367] [D loss: 0.097588, acc:  90%] [G loss: 12.098076] time: 1:11:14.070517\n",
      "[Epoch 0/200] [Batch 929/1367] [D loss: 0.084259, acc:  89%] [G loss: 11.955008] time: 1:11:18.652996\n",
      "[Epoch 0/200] [Batch 930/1367] [D loss: 0.052747, acc:  93%] [G loss: 13.599073] time: 1:11:23.232613\n",
      "[Epoch 0/200] [Batch 931/1367] [D loss: 0.031095, acc:  99%] [G loss: 12.688751] time: 1:11:27.806969\n",
      "[Epoch 0/200] [Batch 932/1367] [D loss: 0.101084, acc:  89%] [G loss: 9.479378] time: 1:11:32.394863\n",
      "[Epoch 0/200] [Batch 933/1367] [D loss: 0.264753, acc:  58%] [G loss: 11.390528] time: 1:11:36.976776\n",
      "[Epoch 0/200] [Batch 934/1367] [D loss: 0.255758, acc:  72%] [G loss: 10.221915] time: 1:11:41.547234\n",
      "[Epoch 0/200] [Batch 935/1367] [D loss: 0.646415, acc:  25%] [G loss: 8.866130] time: 1:11:46.145050\n",
      "[Epoch 0/200] [Batch 936/1367] [D loss: 0.135723, acc:  77%] [G loss: 10.103818] time: 1:11:50.744154\n",
      "[Epoch 0/200] [Batch 937/1367] [D loss: 0.565342, acc:  29%] [G loss: 5.590260] time: 1:11:55.369843\n",
      "[Epoch 0/200] [Batch 938/1367] [D loss: 0.509892, acc:  27%] [G loss: 13.872005] time: 1:11:59.974406\n",
      "[Epoch 0/200] [Batch 939/1367] [D loss: 0.287581, acc:  34%] [G loss: 10.173167] time: 1:12:04.555715\n",
      "[Epoch 0/200] [Batch 940/1367] [D loss: 0.177580, acc:  84%] [G loss: 9.501311] time: 1:12:09.161400\n",
      "[Epoch 0/200] [Batch 941/1367] [D loss: 0.237265, acc:  39%] [G loss: 7.695319] time: 1:12:13.758958\n",
      "[Epoch 0/200] [Batch 942/1367] [D loss: 0.203282, acc:  69%] [G loss: 8.373307] time: 1:12:18.322340\n",
      "[Epoch 0/200] [Batch 943/1367] [D loss: 0.316198, acc:  33%] [G loss: 9.577866] time: 1:12:22.897309\n",
      "[Epoch 0/200] [Batch 944/1367] [D loss: 0.155138, acc:  91%] [G loss: 8.581873] time: 1:12:27.466228\n",
      "[Epoch 0/200] [Batch 945/1367] [D loss: 0.132923, acc:  80%] [G loss: 15.838295] time: 1:12:32.067753\n",
      "[Epoch 0/200] [Batch 946/1367] [D loss: 0.250846, acc:  71%] [G loss: 20.071880] time: 1:12:36.661676\n",
      "[Epoch 0/200] [Batch 947/1367] [D loss: 0.229568, acc:  75%] [G loss: 14.649719] time: 1:12:41.241988\n",
      "[Epoch 0/200] [Batch 948/1367] [D loss: 0.072506, acc:  95%] [G loss: 12.844426] time: 1:12:45.792248\n",
      "[Epoch 0/200] [Batch 949/1367] [D loss: 0.306510, acc:  61%] [G loss: 17.114443] time: 1:12:50.344263\n",
      "[Epoch 0/200] [Batch 950/1367] [D loss: 0.426568, acc:  40%] [G loss: 10.480207] time: 1:12:54.917273\n",
      "[Epoch 0/200] [Batch 951/1367] [D loss: 0.108004, acc:  87%] [G loss: 15.707599] time: 1:12:59.516800\n",
      "[Epoch 0/200] [Batch 952/1367] [D loss: 0.349986, acc:  27%] [G loss: 3.035029] time: 1:13:04.093479\n",
      "[Epoch 0/200] [Batch 953/1367] [D loss: 0.514704, acc:  46%] [G loss: 7.946319] time: 1:13:08.654493\n",
      "[Epoch 0/200] [Batch 954/1367] [D loss: 0.334389, acc:  36%] [G loss: 9.392083] time: 1:13:13.229370\n",
      "[Epoch 0/200] [Batch 955/1367] [D loss: 0.120221, acc:  93%] [G loss: 8.790351] time: 1:13:17.804795\n",
      "[Epoch 0/200] [Batch 956/1367] [D loss: 0.287679, acc:  71%] [G loss: 10.111572] time: 1:13:22.382523\n",
      "[Epoch 0/200] [Batch 957/1367] [D loss: 0.480402, acc:  65%] [G loss: 12.217817] time: 1:13:26.971526\n",
      "[Epoch 0/200] [Batch 958/1367] [D loss: 0.242709, acc:  71%] [G loss: 10.295794] time: 1:13:31.564939\n",
      "[Epoch 0/200] [Batch 959/1367] [D loss: 0.088886, acc:  97%] [G loss: 12.721901] time: 1:13:36.128666\n",
      "[Epoch 0/200] [Batch 960/1367] [D loss: 0.266011, acc:  39%] [G loss: 11.596603] time: 1:13:40.703016\n",
      "[Epoch 0/200] [Batch 961/1367] [D loss: 0.237246, acc:  42%] [G loss: 12.740597] time: 1:13:45.274310\n",
      "[Epoch 0/200] [Batch 962/1367] [D loss: 0.220955, acc:  51%] [G loss: 9.724096] time: 1:13:49.841934\n",
      "[Epoch 0/200] [Batch 963/1367] [D loss: 0.233404, acc:  69%] [G loss: 13.233405] time: 1:13:54.421326\n",
      "[Epoch 0/200] [Batch 964/1367] [D loss: 0.383710, acc:  58%] [G loss: 10.718842] time: 1:13:59.010219\n",
      "[Epoch 0/200] [Batch 965/1367] [D loss: 0.047511, acc:  97%] [G loss: 13.924375] time: 1:14:03.639125\n",
      "[Epoch 0/200] [Batch 966/1367] [D loss: 0.091592, acc:  96%] [G loss: 12.236790] time: 1:14:08.221896\n",
      "[Epoch 0/200] [Batch 967/1367] [D loss: 0.034550, acc:  99%] [G loss: 17.444000] time: 1:14:12.829306\n",
      "[Epoch 0/200] [Batch 968/1367] [D loss: 0.020107, acc:  99%] [G loss: 12.320287] time: 1:14:17.424325\n",
      "[Epoch 0/200] [Batch 969/1367] [D loss: 0.047413, acc:  98%] [G loss: 16.592487] time: 1:14:22.006428\n",
      "[Epoch 0/200] [Batch 970/1367] [D loss: 0.044301, acc:  98%] [G loss: 9.312327] time: 1:14:26.557564\n",
      "[Epoch 0/200] [Batch 971/1367] [D loss: 0.087707, acc:  88%] [G loss: 17.965755] time: 1:14:31.128016\n",
      "[Epoch 0/200] [Batch 972/1367] [D loss: 0.218575, acc:  70%] [G loss: 12.727438] time: 1:14:35.730993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 973/1367] [D loss: 0.106009, acc:  85%] [G loss: 13.024168] time: 1:14:40.313054\n",
      "[Epoch 0/200] [Batch 974/1367] [D loss: 0.053690, acc:  98%] [G loss: 15.270263] time: 1:14:44.887277\n",
      "[Epoch 0/200] [Batch 975/1367] [D loss: 0.509828, acc:  53%] [G loss: 14.057272] time: 1:14:49.452710\n",
      "[Epoch 0/200] [Batch 976/1367] [D loss: 0.293413, acc:  42%] [G loss: 10.497435] time: 1:14:54.015199\n",
      "[Epoch 0/200] [Batch 977/1367] [D loss: 0.214152, acc:  74%] [G loss: 9.411991] time: 1:14:58.579985\n",
      "[Epoch 0/200] [Batch 978/1367] [D loss: 0.108785, acc:  95%] [G loss: 14.416832] time: 1:15:03.146407\n",
      "[Epoch 0/200] [Batch 979/1367] [D loss: 0.437717, acc:  49%] [G loss: 10.603339] time: 1:15:07.704168\n",
      "[Epoch 0/200] [Batch 980/1367] [D loss: 0.337688, acc:  32%] [G loss: 6.371926] time: 1:15:12.241542\n",
      "[Epoch 0/200] [Batch 981/1367] [D loss: 0.276628, acc:  45%] [G loss: 9.330203] time: 1:15:16.831986\n",
      "[Epoch 0/200] [Batch 982/1367] [D loss: 0.388271, acc:  38%] [G loss: 8.460641] time: 1:15:21.406253\n",
      "[Epoch 0/200] [Batch 983/1367] [D loss: 0.344404, acc:  43%] [G loss: 11.022778] time: 1:15:25.965598\n",
      "[Epoch 0/200] [Batch 984/1367] [D loss: 0.167830, acc:  81%] [G loss: 8.745129] time: 1:15:30.542093\n",
      "[Epoch 0/200] [Batch 985/1367] [D loss: 0.138724, acc:  88%] [G loss: 6.181003] time: 1:15:35.084862\n",
      "[Epoch 0/200] [Batch 986/1367] [D loss: 0.050354, acc:  99%] [G loss: 9.556612] time: 1:15:39.665928\n",
      "[Epoch 0/200] [Batch 987/1367] [D loss: 0.311392, acc:  76%] [G loss: 8.261532] time: 1:15:44.237558\n",
      "[Epoch 0/200] [Batch 988/1367] [D loss: 0.461006, acc:  62%] [G loss: 21.782425] time: 1:15:48.817159\n",
      "[Epoch 0/200] [Batch 989/1367] [D loss: 0.183590, acc:  82%] [G loss: 11.328047] time: 1:15:53.373615\n",
      "[Epoch 0/200] [Batch 990/1367] [D loss: 0.453814, acc:  23%] [G loss: 10.781408] time: 1:15:57.941694\n",
      "[Epoch 0/200] [Batch 991/1367] [D loss: 0.377966, acc:  44%] [G loss: 7.802135] time: 1:16:02.494840\n",
      "[Epoch 0/200] [Batch 992/1367] [D loss: 0.164309, acc:  74%] [G loss: 7.175755] time: 1:16:07.090358\n",
      "[Epoch 0/200] [Batch 993/1367] [D loss: 0.184576, acc:  63%] [G loss: 8.249457] time: 1:16:11.677575\n",
      "[Epoch 0/200] [Batch 994/1367] [D loss: 0.172047, acc:  69%] [G loss: 9.662504] time: 1:16:16.253577\n",
      "[Epoch 0/200] [Batch 995/1367] [D loss: 0.107581, acc:  96%] [G loss: 14.115830] time: 1:16:20.811158\n",
      "[Epoch 0/200] [Batch 996/1367] [D loss: 0.122538, acc:  87%] [G loss: 8.840540] time: 1:16:25.384972\n",
      "[Epoch 0/200] [Batch 997/1367] [D loss: 0.024731, acc:  99%] [G loss: 7.889484] time: 1:16:29.975206\n",
      "[Epoch 0/200] [Batch 998/1367] [D loss: 0.246585, acc:  52%] [G loss: 6.304619] time: 1:16:34.541281\n",
      "[Epoch 0/200] [Batch 999/1367] [D loss: 0.086603, acc:  92%] [G loss: 11.546374] time: 1:16:39.109529\n",
      "[Epoch 0/200] [Batch 1000/1367] [D loss: 0.159259, acc:  82%] [G loss: 6.729467] time: 1:16:43.655016\n",
      "sampling\n",
      "[Epoch 0/200] [Batch 1001/1367] [D loss: 0.051011, acc:  98%] [G loss: 9.219651] time: 1:16:49.562142\n",
      "[Epoch 0/200] [Batch 1002/1367] [D loss: 0.122035, acc:  92%] [G loss: 6.918089] time: 1:16:54.114882\n",
      "[Epoch 0/200] [Batch 1003/1367] [D loss: 0.133310, acc:  85%] [G loss: 8.023357] time: 1:16:58.679695\n",
      "[Epoch 0/200] [Batch 1004/1367] [D loss: 0.391743, acc:  53%] [G loss: 14.964231] time: 1:17:03.261163\n",
      "[Epoch 0/200] [Batch 1005/1367] [D loss: 0.119112, acc:  87%] [G loss: 9.136755] time: 1:17:07.828058\n",
      "[Epoch 0/200] [Batch 1006/1367] [D loss: 0.534211, acc:  47%] [G loss: 17.605688] time: 1:17:12.425999\n",
      "[Epoch 0/200] [Batch 1007/1367] [D loss: 0.422672, acc:  59%] [G loss: 5.668004] time: 1:17:16.984147\n",
      "[Epoch 0/200] [Batch 1008/1367] [D loss: 0.234783, acc:  76%] [G loss: 10.829440] time: 1:17:21.531431\n",
      "[Epoch 0/200] [Batch 1009/1367] [D loss: 0.526884, acc:  51%] [G loss: 10.779220] time: 1:17:26.103922\n",
      "[Epoch 0/200] [Batch 1010/1367] [D loss: 0.338883, acc:  27%] [G loss: 9.416166] time: 1:17:30.691323\n",
      "[Epoch 0/200] [Batch 1011/1367] [D loss: 0.273539, acc:  41%] [G loss: 8.992618] time: 1:17:35.286207\n",
      "[Epoch 0/200] [Batch 1012/1367] [D loss: 0.177027, acc:  82%] [G loss: 10.785449] time: 1:17:39.848724\n",
      "[Epoch 0/200] [Batch 1013/1367] [D loss: 0.304591, acc:  58%] [G loss: 18.303892] time: 1:17:44.429082\n",
      "[Epoch 0/200] [Batch 1014/1367] [D loss: 0.369580, acc:  47%] [G loss: 12.058888] time: 1:17:49.002835\n",
      "[Epoch 0/200] [Batch 1015/1367] [D loss: 0.321420, acc:  80%] [G loss: 5.464342] time: 1:17:53.544824\n",
      "[Epoch 0/200] [Batch 1016/1367] [D loss: 0.231528, acc:  81%] [G loss: 6.475609] time: 1:17:58.104025\n",
      "[Epoch 0/200] [Batch 1017/1367] [D loss: 0.517542, acc:  17%] [G loss: 5.765208] time: 1:18:02.657223\n",
      "[Epoch 0/200] [Batch 1018/1367] [D loss: 0.358874, acc:  36%] [G loss: 11.397411] time: 1:18:07.207974\n",
      "[Epoch 0/200] [Batch 1019/1367] [D loss: 0.169826, acc:  82%] [G loss: 16.990490] time: 1:18:11.820662\n",
      "[Epoch 0/200] [Batch 1020/1367] [D loss: 0.289914, acc:  53%] [G loss: 16.455482] time: 1:18:16.419144\n",
      "[Epoch 0/200] [Batch 1021/1367] [D loss: 0.145290, acc:  86%] [G loss: 12.419449] time: 1:18:20.998654\n",
      "[Epoch 0/200] [Batch 1022/1367] [D loss: 0.164261, acc:  78%] [G loss: 13.865811] time: 1:18:25.555949\n",
      "[Epoch 0/200] [Batch 1023/1367] [D loss: 0.177124, acc:  78%] [G loss: 10.040454] time: 1:18:30.138434\n",
      "[Epoch 0/200] [Batch 1024/1367] [D loss: 0.038331, acc:  97%] [G loss: 7.585030] time: 1:18:34.693534\n",
      "[Epoch 0/200] [Batch 1025/1367] [D loss: 0.092246, acc:  84%] [G loss: 16.155434] time: 1:18:39.270692\n",
      "[Epoch 0/200] [Batch 1026/1367] [D loss: 0.131793, acc:  81%] [G loss: 11.426045] time: 1:18:43.842926\n",
      "[Epoch 0/200] [Batch 1027/1367] [D loss: 0.216466, acc:  57%] [G loss: 9.976045] time: 1:18:48.427595\n",
      "[Epoch 0/200] [Batch 1028/1367] [D loss: 0.115523, acc:  97%] [G loss: 11.092648] time: 1:18:52.988021\n",
      "[Epoch 0/200] [Batch 1029/1367] [D loss: 0.064088, acc:  97%] [G loss: 7.877254] time: 1:18:57.526063\n",
      "[Epoch 0/200] [Batch 1030/1367] [D loss: 0.041509, acc:  94%] [G loss: 9.644657] time: 1:19:02.094688\n",
      "[Epoch 0/200] [Batch 1031/1367] [D loss: 0.168968, acc:  73%] [G loss: 9.670909] time: 1:19:06.696633\n",
      "[Epoch 0/200] [Batch 1032/1367] [D loss: 0.130118, acc:  83%] [G loss: 10.746052] time: 1:19:11.273902\n",
      "[Epoch 0/200] [Batch 1033/1367] [D loss: 0.146002, acc:  78%] [G loss: 12.722618] time: 1:19:15.864685\n",
      "[Epoch 0/200] [Batch 1034/1367] [D loss: 0.271680, acc:  55%] [G loss: 10.701556] time: 1:19:20.428578\n",
      "[Epoch 0/200] [Batch 1035/1367] [D loss: 0.116188, acc:  94%] [G loss: 17.469374] time: 1:19:25.033699\n",
      "[Epoch 0/200] [Batch 1036/1367] [D loss: 0.166936, acc:  87%] [G loss: 10.987035] time: 1:19:29.621412\n",
      "[Epoch 0/200] [Batch 1037/1367] [D loss: 0.065451, acc:  97%] [G loss: 10.947913] time: 1:19:34.220063\n",
      "[Epoch 0/200] [Batch 1038/1367] [D loss: 0.044357, acc:  97%] [G loss: 13.546689] time: 1:19:38.813379\n",
      "[Epoch 0/200] [Batch 1039/1367] [D loss: 0.256428, acc:  63%] [G loss: 10.073619] time: 1:19:43.408311\n",
      "[Epoch 0/200] [Batch 1040/1367] [D loss: 0.139277, acc:  83%] [G loss: 10.517774] time: 1:19:47.986996\n",
      "[Epoch 0/200] [Batch 1041/1367] [D loss: 0.103898, acc:  85%] [G loss: 9.569400] time: 1:19:52.575513\n",
      "[Epoch 0/200] [Batch 1042/1367] [D loss: 0.355288, acc:  55%] [G loss: 6.676664] time: 1:19:57.123443\n",
      "[Epoch 0/200] [Batch 1043/1367] [D loss: 0.181834, acc:  76%] [G loss: 8.905769] time: 1:20:01.670027\n",
      "[Epoch 0/200] [Batch 1044/1367] [D loss: 0.048586, acc:  95%] [G loss: 11.551844] time: 1:20:06.259226\n",
      "[Epoch 0/200] [Batch 1045/1367] [D loss: 0.112962, acc:  94%] [G loss: 15.996126] time: 1:20:10.842917\n",
      "[Epoch 0/200] [Batch 1046/1367] [D loss: 0.078991, acc:  98%] [G loss: 10.144154] time: 1:20:15.453091\n",
      "[Epoch 0/200] [Batch 1047/1367] [D loss: 0.033279, acc:  98%] [G loss: 15.924703] time: 1:20:20.043317\n",
      "[Epoch 0/200] [Batch 1048/1367] [D loss: 0.270373, acc:  32%] [G loss: 11.695028] time: 1:20:24.600541\n",
      "[Epoch 0/200] [Batch 1049/1367] [D loss: 0.064551, acc:  93%] [G loss: 16.770590] time: 1:20:29.160321\n",
      "[Epoch 0/200] [Batch 1050/1367] [D loss: 0.413020, acc:  60%] [G loss: 9.348932] time: 1:20:33.726388\n",
      "[Epoch 0/200] [Batch 1051/1367] [D loss: 0.156177, acc:  85%] [G loss: 11.224041] time: 1:20:38.296882\n",
      "[Epoch 0/200] [Batch 1052/1367] [D loss: 0.424580, acc:  63%] [G loss: 13.734273] time: 1:20:42.924929\n",
      "[Epoch 0/200] [Batch 1053/1367] [D loss: 0.077741, acc:  93%] [G loss: 8.836375] time: 1:20:47.540639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 1054/1367] [D loss: 0.124120, acc:  81%] [G loss: 15.862440] time: 1:20:52.160412\n",
      "[Epoch 0/200] [Batch 1055/1367] [D loss: 0.059843, acc:  97%] [G loss: 11.776044] time: 1:20:56.743238\n",
      "[Epoch 0/200] [Batch 1056/1367] [D loss: 0.052443, acc:  97%] [G loss: 10.342655] time: 1:21:01.318634\n",
      "[Epoch 0/200] [Batch 1057/1367] [D loss: 0.105861, acc:  84%] [G loss: 9.942234] time: 1:21:05.919357\n",
      "[Epoch 0/200] [Batch 1058/1367] [D loss: 0.047686, acc:  98%] [G loss: 8.974470] time: 1:21:10.486154\n",
      "[Epoch 0/200] [Batch 1059/1367] [D loss: 0.275151, acc:  38%] [G loss: 11.342820] time: 1:21:15.056283\n",
      "[Epoch 0/200] [Batch 1060/1367] [D loss: 0.236102, acc:  65%] [G loss: 7.892952] time: 1:21:19.641743\n",
      "[Epoch 0/200] [Batch 1061/1367] [D loss: 0.241297, acc:  55%] [G loss: 8.827368] time: 1:21:24.205997\n",
      "[Epoch 0/200] [Batch 1062/1367] [D loss: 0.124510, acc:  86%] [G loss: 8.707329] time: 1:21:28.777189\n",
      "[Epoch 0/200] [Batch 1063/1367] [D loss: 0.259543, acc:  50%] [G loss: 8.405074] time: 1:21:33.341027\n",
      "[Epoch 0/200] [Batch 1064/1367] [D loss: 0.155076, acc:  76%] [G loss: 11.640731] time: 1:21:37.914493\n",
      "[Epoch 0/200] [Batch 1065/1367] [D loss: 0.164131, acc:  86%] [G loss: 9.151623] time: 1:21:42.476331\n",
      "[Epoch 0/200] [Batch 1066/1367] [D loss: 0.073938, acc:  95%] [G loss: 10.169792] time: 1:21:47.057287\n",
      "[Epoch 0/200] [Batch 1067/1367] [D loss: 0.146297, acc:  86%] [G loss: 9.372501] time: 1:21:51.671100\n",
      "[Epoch 0/200] [Batch 1068/1367] [D loss: 0.169250, acc:  82%] [G loss: 15.283302] time: 1:21:56.266236\n",
      "[Epoch 0/200] [Batch 1069/1367] [D loss: 0.472564, acc:  12%] [G loss: 7.903773] time: 1:22:00.842809\n",
      "[Epoch 0/200] [Batch 1070/1367] [D loss: 0.347274, acc:  51%] [G loss: 11.232789] time: 1:22:05.457433\n",
      "[Epoch 0/200] [Batch 1071/1367] [D loss: 0.213016, acc:  44%] [G loss: 8.086367] time: 1:22:10.022250\n",
      "[Epoch 0/200] [Batch 1072/1367] [D loss: 0.096484, acc:  79%] [G loss: 14.466464] time: 1:22:14.618384\n",
      "[Epoch 0/200] [Batch 1073/1367] [D loss: 0.272086, acc:  55%] [G loss: 6.983882] time: 1:22:19.189853\n",
      "[Epoch 0/200] [Batch 1074/1367] [D loss: 0.613144, acc:  46%] [G loss: 15.422712] time: 1:22:23.786450\n",
      "[Epoch 0/200] [Batch 1075/1367] [D loss: 0.162922, acc:  90%] [G loss: 5.236894] time: 1:22:28.332453\n",
      "[Epoch 0/200] [Batch 1076/1367] [D loss: 0.242469, acc:  60%] [G loss: 6.677036] time: 1:22:32.899202\n",
      "[Epoch 0/200] [Batch 1077/1367] [D loss: 0.811575, acc:  17%] [G loss: 7.237336] time: 1:22:37.504283\n",
      "[Epoch 0/200] [Batch 1078/1367] [D loss: 0.393432, acc:  39%] [G loss: 9.885198] time: 1:22:42.076018\n",
      "[Epoch 0/200] [Batch 1079/1367] [D loss: 0.170897, acc:  74%] [G loss: 14.295547] time: 1:22:46.661794\n",
      "[Epoch 0/200] [Batch 1080/1367] [D loss: 0.293118, acc:  64%] [G loss: 20.142052] time: 1:22:51.256563\n",
      "[Epoch 0/200] [Batch 1081/1367] [D loss: 0.252239, acc:  58%] [G loss: 18.629688] time: 1:22:55.858939\n",
      "[Epoch 0/200] [Batch 1082/1367] [D loss: 0.174044, acc:  77%] [G loss: 18.521345] time: 1:23:00.484603\n",
      "[Epoch 0/200] [Batch 1083/1367] [D loss: 0.046653, acc:  97%] [G loss: 13.698673] time: 1:23:05.062192\n",
      "[Epoch 0/200] [Batch 1084/1367] [D loss: 0.024418, acc:  99%] [G loss: 9.563510] time: 1:23:09.641551\n",
      "[Epoch 0/200] [Batch 1085/1367] [D loss: 0.168783, acc:  82%] [G loss: 9.107306] time: 1:23:14.228794\n",
      "[Epoch 0/200] [Batch 1086/1367] [D loss: 0.202545, acc:  77%] [G loss: 9.303443] time: 1:23:18.810456\n",
      "[Epoch 0/200] [Batch 1087/1367] [D loss: 0.049892, acc:  95%] [G loss: 9.436188] time: 1:23:23.360962\n",
      "[Epoch 0/200] [Batch 1088/1367] [D loss: 0.334247, acc:  51%] [G loss: 5.791720] time: 1:23:27.910774\n",
      "[Epoch 0/200] [Batch 1089/1367] [D loss: 0.208689, acc:  81%] [G loss: 5.173157] time: 1:23:32.454988\n",
      "[Epoch 0/200] [Batch 1090/1367] [D loss: 0.450837, acc:  24%] [G loss: 7.830920] time: 1:23:36.989840\n",
      "[Epoch 0/200] [Batch 1091/1367] [D loss: 0.349508, acc:  38%] [G loss: 15.727167] time: 1:23:41.544462\n",
      "[Epoch 0/200] [Batch 1092/1367] [D loss: 0.570935, acc:  34%] [G loss: 8.128466] time: 1:23:46.091765\n",
      "[Epoch 0/200] [Batch 1093/1367] [D loss: 0.691596, acc:  64%] [G loss: 9.885448] time: 1:23:50.672699\n",
      "[Epoch 0/200] [Batch 1094/1367] [D loss: 0.401177, acc:  61%] [G loss: 9.481523] time: 1:23:55.224666\n",
      "[Epoch 0/200] [Batch 1095/1367] [D loss: 0.254689, acc:  50%] [G loss: 10.094536] time: 1:23:59.785923\n",
      "[Epoch 0/200] [Batch 1096/1367] [D loss: 0.272073, acc:  41%] [G loss: 8.478230] time: 1:24:04.364882\n",
      "[Epoch 0/200] [Batch 1097/1367] [D loss: 0.149993, acc:  92%] [G loss: 7.317089] time: 1:24:08.945844\n",
      "[Epoch 0/200] [Batch 1098/1367] [D loss: 0.321717, acc:  35%] [G loss: 9.103929] time: 1:24:13.529040\n",
      "[Epoch 0/200] [Batch 1099/1367] [D loss: 0.157199, acc:  82%] [G loss: 9.015993] time: 1:24:18.102080\n",
      "[Epoch 0/200] [Batch 1100/1367] [D loss: 0.146954, acc:  82%] [G loss: 7.758825] time: 1:24:22.672717\n",
      "[Epoch 0/200] [Batch 1101/1367] [D loss: 0.101245, acc:  96%] [G loss: 5.885536] time: 1:24:27.218324\n",
      "[Epoch 0/200] [Batch 1102/1367] [D loss: 0.074950, acc:  99%] [G loss: 7.700263] time: 1:24:31.771858\n",
      "[Epoch 0/200] [Batch 1103/1367] [D loss: 0.102758, acc:  98%] [G loss: 7.205822] time: 1:24:36.334913\n",
      "[Epoch 0/200] [Batch 1104/1367] [D loss: 0.141690, acc:  86%] [G loss: 13.094578] time: 1:24:40.896940\n",
      "[Epoch 0/200] [Batch 1105/1367] [D loss: 0.063054, acc:  94%] [G loss: 12.553277] time: 1:24:45.495733\n",
      "[Epoch 0/200] [Batch 1106/1367] [D loss: 0.060137, acc:  94%] [G loss: 7.422827] time: 1:24:50.053738\n",
      "[Epoch 0/200] [Batch 1107/1367] [D loss: 0.085629, acc:  96%] [G loss: 8.974688] time: 1:24:54.631794\n",
      "[Epoch 0/200] [Batch 1108/1367] [D loss: 0.168929, acc:  73%] [G loss: 11.214235] time: 1:24:59.232353\n",
      "[Epoch 0/200] [Batch 1109/1367] [D loss: 0.312544, acc:  46%] [G loss: 18.766228] time: 1:25:03.816091\n",
      "[Epoch 0/200] [Batch 1110/1367] [D loss: 0.129691, acc:  86%] [G loss: 16.594715] time: 1:25:08.403442\n",
      "[Epoch 0/200] [Batch 1111/1367] [D loss: 0.325276, acc:  50%] [G loss: 14.645107] time: 1:25:12.954638\n",
      "[Epoch 0/200] [Batch 1112/1367] [D loss: 0.098528, acc:  97%] [G loss: 14.781918] time: 1:25:17.536414\n",
      "[Epoch 0/200] [Batch 1113/1367] [D loss: 0.107441, acc:  95%] [G loss: 5.414651] time: 1:25:22.094144\n",
      "[Epoch 0/200] [Batch 1114/1367] [D loss: 0.115650, acc:  93%] [G loss: 5.512395] time: 1:25:26.656405\n",
      "[Epoch 0/200] [Batch 1115/1367] [D loss: 0.217092, acc:  57%] [G loss: 7.527039] time: 1:25:31.236032\n",
      "[Epoch 0/200] [Batch 1116/1367] [D loss: 0.073382, acc:  92%] [G loss: 14.827511] time: 1:25:35.800933\n",
      "[Epoch 0/200] [Batch 1117/1367] [D loss: 0.603209, acc:  61%] [G loss: 16.325060] time: 1:25:40.370868\n",
      "[Epoch 0/200] [Batch 1118/1367] [D loss: 0.222890, acc:  78%] [G loss: 9.363409] time: 1:25:44.929893\n",
      "[Epoch 0/200] [Batch 1119/1367] [D loss: 0.039879, acc:  99%] [G loss: 5.934949] time: 1:25:49.488580\n",
      "[Epoch 0/200] [Batch 1120/1367] [D loss: 0.352467, acc:  25%] [G loss: 13.965622] time: 1:25:54.085808\n",
      "[Epoch 0/200] [Batch 1121/1367] [D loss: 0.124290, acc:  83%] [G loss: 10.502542] time: 1:25:58.670367\n",
      "[Epoch 0/200] [Batch 1122/1367] [D loss: 0.229018, acc:  80%] [G loss: 6.068434] time: 1:26:03.231520\n",
      "[Epoch 0/200] [Batch 1123/1367] [D loss: 0.025748, acc:  98%] [G loss: 13.010829] time: 1:26:07.807397\n",
      "[Epoch 0/200] [Batch 1124/1367] [D loss: 0.401125, acc:  27%] [G loss: 3.691357] time: 1:26:12.370713\n",
      "[Epoch 0/200] [Batch 1125/1367] [D loss: 0.084895, acc:  93%] [G loss: 8.032940] time: 1:26:16.945667\n",
      "[Epoch 0/200] [Batch 1126/1367] [D loss: 0.035732, acc:  97%] [G loss: 7.110633] time: 1:26:21.483329\n",
      "[Epoch 0/200] [Batch 1127/1367] [D loss: 0.093763, acc:  89%] [G loss: 6.357792] time: 1:26:26.064327\n",
      "[Epoch 0/200] [Batch 1128/1367] [D loss: 0.100920, acc:  94%] [G loss: 15.514832] time: 1:26:30.623329\n",
      "[Epoch 0/200] [Batch 1129/1367] [D loss: 0.064350, acc:  95%] [G loss: 14.029653] time: 1:26:35.169774\n",
      "[Epoch 0/200] [Batch 1130/1367] [D loss: 0.066760, acc:  92%] [G loss: 14.183515] time: 1:26:39.719578\n",
      "[Epoch 0/200] [Batch 1131/1367] [D loss: 0.295998, acc:  49%] [G loss: 5.531561] time: 1:26:44.320801\n",
      "[Epoch 0/200] [Batch 1132/1367] [D loss: 0.467121, acc:  46%] [G loss: 13.414601] time: 1:26:48.893179\n",
      "[Epoch 0/200] [Batch 1133/1367] [D loss: 0.551083, acc:  69%] [G loss: 5.534459] time: 1:26:53.422303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 1134/1367] [D loss: 0.593317, acc:  12%] [G loss: 7.914034] time: 1:26:58.004917\n",
      "[Epoch 0/200] [Batch 1135/1367] [D loss: 0.274641, acc:  46%] [G loss: 8.623729] time: 1:27:02.559090\n",
      "[Epoch 0/200] [Batch 1136/1367] [D loss: 0.313777, acc:  24%] [G loss: 5.586843] time: 1:27:07.121727\n",
      "[Epoch 0/200] [Batch 1137/1367] [D loss: 0.305626, acc:  35%] [G loss: 10.691217] time: 1:27:11.698546\n",
      "[Epoch 0/200] [Batch 1138/1367] [D loss: 0.228334, acc:  65%] [G loss: 15.948476] time: 1:27:16.285867\n",
      "[Epoch 0/200] [Batch 1139/1367] [D loss: 0.195261, acc:  64%] [G loss: 17.654358] time: 1:27:20.905811\n",
      "[Epoch 0/200] [Batch 1140/1367] [D loss: 0.198665, acc:  73%] [G loss: 12.071599] time: 1:27:25.488921\n",
      "[Epoch 0/200] [Batch 1141/1367] [D loss: 0.144124, acc:  79%] [G loss: 11.767385] time: 1:27:30.073050\n",
      "[Epoch 0/200] [Batch 1142/1367] [D loss: 0.161271, acc:  89%] [G loss: 7.115562] time: 1:27:34.629881\n",
      "[Epoch 0/200] [Batch 1143/1367] [D loss: 0.028838, acc:  99%] [G loss: 8.605224] time: 1:27:39.184023\n",
      "[Epoch 0/200] [Batch 1144/1367] [D loss: 0.032730, acc:  98%] [G loss: 10.824972] time: 1:27:43.730128\n",
      "[Epoch 0/200] [Batch 1145/1367] [D loss: 0.200583, acc:  81%] [G loss: 15.131883] time: 1:27:48.308298\n",
      "[Epoch 0/200] [Batch 1146/1367] [D loss: 0.246500, acc:  82%] [G loss: 8.754098] time: 1:27:52.844840\n",
      "[Epoch 0/200] [Batch 1147/1367] [D loss: 0.325028, acc:  58%] [G loss: 9.273539] time: 1:27:57.420245\n",
      "[Epoch 0/200] [Batch 1148/1367] [D loss: 0.165137, acc:  75%] [G loss: 15.477326] time: 1:28:01.984343\n",
      "[Epoch 0/200] [Batch 1149/1367] [D loss: 0.472350, acc:  31%] [G loss: 10.567686] time: 1:28:06.556014\n",
      "[Epoch 0/200] [Batch 1150/1367] [D loss: 0.182876, acc:  72%] [G loss: 17.357960] time: 1:28:11.133990\n",
      "[Epoch 0/200] [Batch 1151/1367] [D loss: 0.147628, acc:  68%] [G loss: 8.191433] time: 1:28:15.747985\n",
      "[Epoch 0/200] [Batch 1152/1367] [D loss: 0.104827, acc:  84%] [G loss: 13.187206] time: 1:28:20.360463\n",
      "[Epoch 0/200] [Batch 1153/1367] [D loss: 0.068405, acc:  94%] [G loss: 10.196284] time: 1:28:24.962895\n",
      "[Epoch 0/200] [Batch 1154/1367] [D loss: 0.195230, acc:  77%] [G loss: 8.875837] time: 1:28:29.565348\n",
      "[Epoch 0/200] [Batch 1155/1367] [D loss: 0.141005, acc:  94%] [G loss: 11.278195] time: 1:28:34.134727\n",
      "[Epoch 0/200] [Batch 1156/1367] [D loss: 0.352632, acc:  30%] [G loss: 6.769935] time: 1:28:38.699559\n",
      "[Epoch 0/200] [Batch 1157/1367] [D loss: 0.270938, acc:  64%] [G loss: 9.717596] time: 1:28:43.294168\n",
      "[Epoch 0/200] [Batch 1158/1367] [D loss: 0.133548, acc:  85%] [G loss: 6.449800] time: 1:28:47.865797\n",
      "[Epoch 0/200] [Batch 1159/1367] [D loss: 0.428514, acc:  76%] [G loss: 10.943722] time: 1:28:52.428545\n",
      "[Epoch 0/200] [Batch 1160/1367] [D loss: 0.171341, acc:  89%] [G loss: 6.347901] time: 1:28:56.995961\n",
      "[Epoch 0/200] [Batch 1161/1367] [D loss: 0.578454, acc:   7%] [G loss: 6.467777] time: 1:29:01.561735\n",
      "[Epoch 0/200] [Batch 1162/1367] [D loss: 0.150690, acc:  74%] [G loss: 10.962353] time: 1:29:06.134863\n",
      "[Epoch 0/200] [Batch 1163/1367] [D loss: 0.172928, acc:  84%] [G loss: 9.215034] time: 1:29:10.688511\n",
      "[Epoch 0/200] [Batch 1164/1367] [D loss: 0.203264, acc:  68%] [G loss: 8.579664] time: 1:29:15.225213\n",
      "[Epoch 0/200] [Batch 1165/1367] [D loss: 0.187099, acc:  60%] [G loss: 6.633924] time: 1:29:19.777466\n",
      "[Epoch 0/200] [Batch 1166/1367] [D loss: 0.210418, acc:  63%] [G loss: 21.320309] time: 1:29:24.372488\n",
      "[Epoch 0/200] [Batch 1167/1367] [D loss: 0.522862, acc:  70%] [G loss: 12.502861] time: 1:29:28.935361\n",
      "[Epoch 0/200] [Batch 1168/1367] [D loss: 0.133354, acc:  89%] [G loss: 7.053711] time: 1:29:33.478803\n",
      "[Epoch 0/200] [Batch 1169/1367] [D loss: 0.154657, acc:  89%] [G loss: 14.600979] time: 1:29:38.045182\n",
      "[Epoch 0/200] [Batch 1170/1367] [D loss: 0.078592, acc:  94%] [G loss: 11.320797] time: 1:29:42.627845\n",
      "[Epoch 0/200] [Batch 1171/1367] [D loss: 0.104490, acc:  88%] [G loss: 12.318393] time: 1:29:47.219761\n",
      "[Epoch 0/200] [Batch 1172/1367] [D loss: 0.124062, acc:  82%] [G loss: 10.185860] time: 1:29:51.786393\n",
      "[Epoch 0/200] [Batch 1173/1367] [D loss: 0.058146, acc:  92%] [G loss: 7.653958] time: 1:29:56.392019\n",
      "[Epoch 0/200] [Batch 1174/1367] [D loss: 0.158544, acc:  71%] [G loss: 7.049670] time: 1:30:00.955006\n",
      "[Epoch 0/200] [Batch 1175/1367] [D loss: 0.043894, acc:  96%] [G loss: 10.566241] time: 1:30:05.528966\n",
      "[Epoch 0/200] [Batch 1176/1367] [D loss: 0.259419, acc:  73%] [G loss: 7.963330] time: 1:30:10.099964\n",
      "[Epoch 0/200] [Batch 1177/1367] [D loss: 0.305619, acc:  64%] [G loss: 14.272453] time: 1:30:14.675503\n",
      "[Epoch 0/200] [Batch 1178/1367] [D loss: 0.122783, acc:  88%] [G loss: 14.341997] time: 1:30:19.242831\n",
      "[Epoch 0/200] [Batch 1179/1367] [D loss: 0.115522, acc:  93%] [G loss: 7.668968] time: 1:30:23.791446\n",
      "[Epoch 0/200] [Batch 1180/1367] [D loss: 0.139232, acc:  91%] [G loss: 11.114598] time: 1:30:28.357521\n",
      "[Epoch 0/200] [Batch 1181/1367] [D loss: 0.719119, acc:  16%] [G loss: 7.038098] time: 1:30:32.915270\n",
      "[Epoch 0/200] [Batch 1182/1367] [D loss: 0.201294, acc:  62%] [G loss: 10.749367] time: 1:30:37.502452\n",
      "[Epoch 0/200] [Batch 1183/1367] [D loss: 0.207722, acc:  60%] [G loss: 11.304912] time: 1:30:42.064838\n",
      "[Epoch 0/200] [Batch 1184/1367] [D loss: 0.102361, acc:  92%] [G loss: 13.997089] time: 1:30:46.656557\n",
      "[Epoch 0/200] [Batch 1185/1367] [D loss: 0.052922, acc:  94%] [G loss: 14.341858] time: 1:30:51.216952\n",
      "[Epoch 0/200] [Batch 1186/1367] [D loss: 0.034857, acc:  99%] [G loss: 16.187519] time: 1:30:55.803996\n",
      "[Epoch 0/200] [Batch 1187/1367] [D loss: 0.075254, acc:  97%] [G loss: 12.334299] time: 1:31:00.380700\n",
      "[Epoch 0/200] [Batch 1188/1367] [D loss: 0.232631, acc:  52%] [G loss: 11.541036] time: 1:31:04.969198\n",
      "[Epoch 0/200] [Batch 1189/1367] [D loss: 0.164094, acc:  67%] [G loss: 10.671286] time: 1:31:09.567977\n",
      "[Epoch 0/200] [Batch 1190/1367] [D loss: 0.363022, acc:  52%] [G loss: 4.562503] time: 1:31:14.112527\n",
      "[Epoch 0/200] [Batch 1191/1367] [D loss: 0.150293, acc:  92%] [G loss: 3.729596] time: 1:31:18.668163\n",
      "[Epoch 0/200] [Batch 1192/1367] [D loss: 0.114553, acc:  92%] [G loss: 10.046104] time: 1:31:23.255181\n",
      "[Epoch 0/200] [Batch 1193/1367] [D loss: 0.294473, acc:  38%] [G loss: 9.623536] time: 1:31:27.836851\n",
      "[Epoch 0/200] [Batch 1194/1367] [D loss: 0.158156, acc:  80%] [G loss: 9.603048] time: 1:31:32.429096\n",
      "[Epoch 0/200] [Batch 1195/1367] [D loss: 0.147425, acc:  78%] [G loss: 9.867572] time: 1:31:37.041649\n",
      "[Epoch 0/200] [Batch 1196/1367] [D loss: 0.056312, acc:  94%] [G loss: 10.995130] time: 1:31:41.650495\n",
      "[Epoch 0/200] [Batch 1197/1367] [D loss: 0.056577, acc:  98%] [G loss: 7.234955] time: 1:31:46.258384\n",
      "[Epoch 0/200] [Batch 1198/1367] [D loss: 0.089731, acc:  98%] [G loss: 9.555125] time: 1:31:50.815383\n",
      "[Epoch 0/200] [Batch 1199/1367] [D loss: 0.174420, acc:  70%] [G loss: 10.586136] time: 1:31:55.387040\n",
      "[Epoch 0/200] [Batch 1200/1367] [D loss: 0.075740, acc:  98%] [G loss: 8.484994] time: 1:31:59.943702\n",
      "sampling\n",
      "[Epoch 0/200] [Batch 1201/1367] [D loss: 0.210570, acc:  69%] [G loss: 13.336008] time: 1:32:05.770780\n",
      "[Epoch 0/200] [Batch 1202/1367] [D loss: 0.083650, acc:  87%] [G loss: 9.610329] time: 1:32:10.324546\n",
      "[Epoch 0/200] [Batch 1203/1367] [D loss: 0.029682, acc:  98%] [G loss: 9.317865] time: 1:32:14.902165\n",
      "[Epoch 0/200] [Batch 1204/1367] [D loss: 0.083212, acc:  98%] [G loss: 11.858783] time: 1:32:19.512061\n",
      "[Epoch 0/200] [Batch 1205/1367] [D loss: 0.073323, acc:  98%] [G loss: 11.145512] time: 1:32:24.062774\n",
      "[Epoch 0/200] [Batch 1206/1367] [D loss: 0.039131, acc:  98%] [G loss: 13.032495] time: 1:32:28.635950\n",
      "[Epoch 0/200] [Batch 1207/1367] [D loss: 0.134860, acc:  80%] [G loss: 9.741305] time: 1:32:33.205938\n",
      "[Epoch 0/200] [Batch 1208/1367] [D loss: 0.073028, acc:  91%] [G loss: 11.257081] time: 1:32:37.792175\n",
      "[Epoch 0/200] [Batch 1209/1367] [D loss: 0.380929, acc:  27%] [G loss: 13.487236] time: 1:32:42.356738\n",
      "[Epoch 0/200] [Batch 1210/1367] [D loss: 0.217510, acc:  57%] [G loss: 17.106256] time: 1:32:46.915743\n",
      "[Epoch 0/200] [Batch 1211/1367] [D loss: 0.360529, acc:  49%] [G loss: 10.064220] time: 1:32:51.475316\n",
      "[Epoch 0/200] [Batch 1212/1367] [D loss: 0.272016, acc:  56%] [G loss: 10.305831] time: 1:32:56.051863\n",
      "[Epoch 0/200] [Batch 1213/1367] [D loss: 0.108348, acc:  95%] [G loss: 8.215673] time: 1:33:00.631935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 1214/1367] [D loss: 0.401436, acc:  31%] [G loss: 10.008895] time: 1:33:05.240295\n",
      "[Epoch 0/200] [Batch 1215/1367] [D loss: 0.130095, acc:  76%] [G loss: 9.867996] time: 1:33:09.815754\n",
      "[Epoch 0/200] [Batch 1216/1367] [D loss: 0.054363, acc:  98%] [G loss: 13.818246] time: 1:33:14.379848\n",
      "[Epoch 0/200] [Batch 1217/1367] [D loss: 0.042394, acc:  97%] [G loss: 15.997311] time: 1:33:18.940079\n",
      "[Epoch 0/200] [Batch 1218/1367] [D loss: 0.050883, acc:  94%] [G loss: 11.581503] time: 1:33:23.524232\n",
      "[Epoch 0/200] [Batch 1219/1367] [D loss: 0.047291, acc:  98%] [G loss: 13.951685] time: 1:33:28.096518\n",
      "[Epoch 0/200] [Batch 1220/1367] [D loss: 0.140148, acc:  75%] [G loss: 10.681619] time: 1:33:32.686602\n",
      "[Epoch 0/200] [Batch 1221/1367] [D loss: 0.481500, acc:  17%] [G loss: 10.508895] time: 1:33:37.268732\n",
      "[Epoch 0/200] [Batch 1222/1367] [D loss: 0.194630, acc:  75%] [G loss: 10.923018] time: 1:33:41.833060\n",
      "[Epoch 0/200] [Batch 1223/1367] [D loss: 0.359147, acc:  49%] [G loss: 7.023874] time: 1:33:46.403054\n",
      "[Epoch 0/200] [Batch 1224/1367] [D loss: 0.347707, acc:  36%] [G loss: 7.852812] time: 1:33:50.976712\n",
      "[Epoch 0/200] [Batch 1225/1367] [D loss: 0.178932, acc:  84%] [G loss: 11.673765] time: 1:33:55.545987\n",
      "[Epoch 0/200] [Batch 1226/1367] [D loss: 0.275876, acc:  57%] [G loss: 10.977947] time: 1:34:00.165662\n",
      "[Epoch 0/200] [Batch 1227/1367] [D loss: 0.299369, acc:  30%] [G loss: 8.280340] time: 1:34:04.750686\n",
      "[Epoch 0/200] [Batch 1228/1367] [D loss: 0.140573, acc:  91%] [G loss: 12.913625] time: 1:34:09.316146\n",
      "[Epoch 0/200] [Batch 1229/1367] [D loss: 0.190227, acc:  83%] [G loss: 5.473657] time: 1:34:13.874995\n",
      "[Epoch 0/200] [Batch 1230/1367] [D loss: 0.210362, acc:  65%] [G loss: 9.664385] time: 1:34:18.446473\n",
      "[Epoch 0/200] [Batch 1231/1367] [D loss: 0.407505, acc:  34%] [G loss: 7.225159] time: 1:34:23.024282\n",
      "[Epoch 0/200] [Batch 1232/1367] [D loss: 0.271937, acc:  37%] [G loss: 11.993112] time: 1:34:27.599766\n",
      "[Epoch 0/200] [Batch 1233/1367] [D loss: 0.155038, acc:  89%] [G loss: 11.523443] time: 1:34:32.182048\n",
      "[Epoch 0/200] [Batch 1234/1367] [D loss: 0.417908, acc:  34%] [G loss: 9.013127] time: 1:34:36.758416\n",
      "[Epoch 0/200] [Batch 1235/1367] [D loss: 0.106856, acc:  92%] [G loss: 14.617669] time: 1:34:41.333990\n",
      "[Epoch 0/200] [Batch 1236/1367] [D loss: 0.109660, acc:  93%] [G loss: 8.836572] time: 1:34:45.875803\n",
      "[Epoch 0/200] [Batch 1237/1367] [D loss: 0.146712, acc:  92%] [G loss: 6.745929] time: 1:34:50.450731\n",
      "[Epoch 0/200] [Batch 1238/1367] [D loss: 0.191150, acc:  69%] [G loss: 5.127817] time: 1:34:55.008405\n",
      "[Epoch 0/200] [Batch 1239/1367] [D loss: 0.064995, acc:  96%] [G loss: 11.429958] time: 1:34:59.582509\n",
      "[Epoch 0/200] [Batch 1240/1367] [D loss: 0.142213, acc:  77%] [G loss: 16.975901] time: 1:35:04.157498\n",
      "[Epoch 0/200] [Batch 1241/1367] [D loss: 0.324124, acc:  70%] [G loss: 11.397121] time: 1:35:08.710158\n",
      "[Epoch 0/200] [Batch 1242/1367] [D loss: 0.119051, acc:  88%] [G loss: 10.543171] time: 1:35:13.266321\n",
      "[Epoch 0/200] [Batch 1243/1367] [D loss: 0.068249, acc:  95%] [G loss: 10.576062] time: 1:35:17.815038\n",
      "[Epoch 0/200] [Batch 1244/1367] [D loss: 1.113128, acc:  73%] [G loss: 6.283455] time: 1:35:22.387186\n",
      "[Epoch 0/200] [Batch 1245/1367] [D loss: 0.608684, acc:  64%] [G loss: 14.118254] time: 1:35:26.973437\n",
      "[Epoch 0/200] [Batch 1246/1367] [D loss: 0.314910, acc:  51%] [G loss: 5.972038] time: 1:35:31.507660\n",
      "[Epoch 0/200] [Batch 1247/1367] [D loss: 0.630666, acc:  16%] [G loss: 9.867796] time: 1:35:36.081446\n",
      "[Epoch 0/200] [Batch 1248/1367] [D loss: 0.555813, acc:  24%] [G loss: 11.647486] time: 1:35:40.655664\n",
      "[Epoch 0/200] [Batch 1249/1367] [D loss: 0.203347, acc:  59%] [G loss: 12.759428] time: 1:35:45.239313\n",
      "[Epoch 0/200] [Batch 1250/1367] [D loss: 0.466487, acc:  59%] [G loss: 18.741449] time: 1:35:49.833333\n",
      "[Epoch 0/200] [Batch 1251/1367] [D loss: 0.276059, acc:  73%] [G loss: 10.754893] time: 1:35:54.400675\n",
      "[Epoch 0/200] [Batch 1252/1367] [D loss: 0.293482, acc:  43%] [G loss: 12.284971] time: 1:35:58.980958\n",
      "[Epoch 0/200] [Batch 1253/1367] [D loss: 0.113852, acc:  97%] [G loss: 9.241956] time: 1:36:03.555677\n",
      "[Epoch 0/200] [Batch 1254/1367] [D loss: 0.172044, acc:  91%] [G loss: 8.779386] time: 1:36:08.102701\n",
      "[Epoch 0/200] [Batch 1255/1367] [D loss: 0.170595, acc:  90%] [G loss: 9.091584] time: 1:36:12.690005\n",
      "[Epoch 0/200] [Batch 1256/1367] [D loss: 0.151623, acc:  89%] [G loss: 8.018258] time: 1:36:17.263729\n",
      "[Epoch 0/200] [Batch 1257/1367] [D loss: 0.332132, acc:  23%] [G loss: 9.575752] time: 1:36:21.817091\n",
      "[Epoch 0/200] [Batch 1258/1367] [D loss: 0.066627, acc:  98%] [G loss: 7.679597] time: 1:36:26.365237\n",
      "[Epoch 0/200] [Batch 1259/1367] [D loss: 0.274668, acc:  47%] [G loss: 9.001022] time: 1:36:30.945461\n",
      "[Epoch 0/200] [Batch 1260/1367] [D loss: 0.070622, acc:  96%] [G loss: 7.831738] time: 1:36:35.546601\n",
      "[Epoch 0/200] [Batch 1261/1367] [D loss: 0.046007, acc:  96%] [G loss: 8.187213] time: 1:36:40.093509\n",
      "[Epoch 0/200] [Batch 1262/1367] [D loss: 0.046941, acc:  94%] [G loss: 7.669595] time: 1:36:44.647978\n",
      "[Epoch 0/200] [Batch 1263/1367] [D loss: 0.188198, acc:  64%] [G loss: 11.833917] time: 1:36:49.229791\n",
      "[Epoch 0/200] [Batch 1264/1367] [D loss: 0.246336, acc:  65%] [G loss: 11.889895] time: 1:36:53.807311\n",
      "[Epoch 0/200] [Batch 1265/1367] [D loss: 0.163257, acc:  87%] [G loss: 8.432234] time: 1:36:58.389314\n",
      "[Epoch 0/200] [Batch 1266/1367] [D loss: 0.133846, acc:  91%] [G loss: 7.400448] time: 1:37:02.929724\n",
      "[Epoch 0/200] [Batch 1267/1367] [D loss: 0.183456, acc:  89%] [G loss: 10.094231] time: 1:37:07.488255\n",
      "[Epoch 0/200] [Batch 1268/1367] [D loss: 1.247204, acc:  69%] [G loss: 7.175823] time: 1:37:12.048257\n",
      "[Epoch 0/200] [Batch 1269/1367] [D loss: 0.287841, acc:  81%] [G loss: 12.123416] time: 1:37:16.631555\n",
      "[Epoch 0/200] [Batch 1270/1367] [D loss: 0.436947, acc:  29%] [G loss: 13.319624] time: 1:37:21.190252\n",
      "[Epoch 0/200] [Batch 1271/1367] [D loss: 0.264030, acc:  55%] [G loss: 9.343520] time: 1:37:25.761040\n",
      "[Epoch 0/200] [Batch 1272/1367] [D loss: 0.121222, acc:  97%] [G loss: 9.669730] time: 1:37:30.317206\n",
      "[Epoch 0/200] [Batch 1273/1367] [D loss: 0.200363, acc:  62%] [G loss: 11.508028] time: 1:37:34.880893\n",
      "[Epoch 0/200] [Batch 1274/1367] [D loss: 0.069115, acc:  98%] [G loss: 6.936703] time: 1:37:39.486912\n",
      "[Epoch 0/200] [Batch 1275/1367] [D loss: 0.289876, acc:  62%] [G loss: 4.394940] time: 1:37:44.049068\n",
      "[Epoch 0/200] [Batch 1276/1367] [D loss: 0.463175, acc:  34%] [G loss: 8.005305] time: 1:37:48.623204\n",
      "[Epoch 0/200] [Batch 1277/1367] [D loss: 0.100031, acc:  95%] [G loss: 9.187750] time: 1:37:53.173974\n",
      "[Epoch 0/200] [Batch 1278/1367] [D loss: 0.124535, acc:  89%] [G loss: 10.753950] time: 1:37:57.740510\n",
      "[Epoch 0/200] [Batch 1279/1367] [D loss: 0.042556, acc:  98%] [G loss: 12.725870] time: 1:38:02.340785\n",
      "[Epoch 0/200] [Batch 1280/1367] [D loss: 0.056930, acc:  98%] [G loss: 15.433067] time: 1:38:06.946919\n",
      "[Epoch 0/200] [Batch 1281/1367] [D loss: 0.038068, acc:  99%] [G loss: 7.260308] time: 1:38:11.519504\n",
      "[Epoch 0/200] [Batch 1282/1367] [D loss: 0.067869, acc:  99%] [G loss: 6.856505] time: 1:38:16.107909\n",
      "[Epoch 0/200] [Batch 1283/1367] [D loss: 0.049878, acc:  98%] [G loss: 11.848886] time: 1:38:20.677031\n",
      "[Epoch 0/200] [Batch 1284/1367] [D loss: 0.082058, acc:  95%] [G loss: 13.763928] time: 1:38:25.229590\n",
      "[Epoch 0/200] [Batch 1285/1367] [D loss: 0.188507, acc:  70%] [G loss: 17.032459] time: 1:38:29.845665\n",
      "[Epoch 0/200] [Batch 1286/1367] [D loss: 0.039120, acc:  99%] [G loss: 5.926415] time: 1:38:34.428052\n",
      "[Epoch 0/200] [Batch 1287/1367] [D loss: 0.125927, acc:  90%] [G loss: 9.681419] time: 1:38:38.996695\n",
      "[Epoch 0/200] [Batch 1288/1367] [D loss: 0.231001, acc:  86%] [G loss: 6.670171] time: 1:38:43.542035\n",
      "[Epoch 0/200] [Batch 1289/1367] [D loss: 0.163706, acc:  77%] [G loss: 7.603557] time: 1:38:48.099863\n",
      "[Epoch 0/200] [Batch 1290/1367] [D loss: 0.074786, acc:  95%] [G loss: 10.770706] time: 1:38:52.662636\n",
      "[Epoch 0/200] [Batch 1291/1367] [D loss: 0.048120, acc:  96%] [G loss: 7.370426] time: 1:38:57.251489\n",
      "[Epoch 0/200] [Batch 1292/1367] [D loss: 0.129214, acc:  85%] [G loss: 7.448719] time: 1:39:01.820641\n",
      "[Epoch 0/200] [Batch 1293/1367] [D loss: 0.185562, acc:  67%] [G loss: 5.876132] time: 1:39:06.381248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 1294/1367] [D loss: 0.035631, acc:  99%] [G loss: 5.590810] time: 1:39:10.942065\n",
      "[Epoch 0/200] [Batch 1295/1367] [D loss: 0.189077, acc:  81%] [G loss: 8.201765] time: 1:39:15.519189\n",
      "[Epoch 0/200] [Batch 1296/1367] [D loss: 0.246654, acc:  49%] [G loss: 6.873387] time: 1:39:20.081591\n",
      "[Epoch 0/200] [Batch 1297/1367] [D loss: 0.144886, acc:  82%] [G loss: 6.256438] time: 1:39:24.658476\n",
      "[Epoch 0/200] [Batch 1298/1367] [D loss: 0.116262, acc:  86%] [G loss: 5.906776] time: 1:39:29.198762\n",
      "[Epoch 0/200] [Batch 1299/1367] [D loss: 0.125451, acc:  84%] [G loss: 12.571708] time: 1:39:33.763820\n",
      "[Epoch 0/200] [Batch 1300/1367] [D loss: 0.163798, acc:  82%] [G loss: 7.805021] time: 1:39:38.331738\n",
      "[Epoch 0/200] [Batch 1301/1367] [D loss: 0.224852, acc:  57%] [G loss: 6.674045] time: 1:39:42.880669\n",
      "[Epoch 0/200] [Batch 1302/1367] [D loss: 0.068973, acc:  90%] [G loss: 14.575066] time: 1:39:47.465628\n",
      "[Epoch 0/200] [Batch 1303/1367] [D loss: 0.123275, acc:  86%] [G loss: 10.490838] time: 1:39:52.034435\n",
      "[Epoch 0/200] [Batch 1304/1367] [D loss: 0.150641, acc:  74%] [G loss: 9.200209] time: 1:39:56.594292\n",
      "[Epoch 0/200] [Batch 1305/1367] [D loss: 0.094420, acc:  94%] [G loss: 9.351901] time: 1:40:01.149710\n",
      "[Epoch 0/200] [Batch 1306/1367] [D loss: 0.426887, acc:  44%] [G loss: 3.192917] time: 1:40:05.702896\n",
      "[Epoch 0/200] [Batch 1307/1367] [D loss: 0.164967, acc:  64%] [G loss: 5.819475] time: 1:40:10.273090\n",
      "[Epoch 0/200] [Batch 1308/1367] [D loss: 0.185700, acc:  90%] [G loss: 5.440030] time: 1:40:14.813799\n",
      "[Epoch 0/200] [Batch 1309/1367] [D loss: 0.248526, acc:  51%] [G loss: 5.052321] time: 1:40:19.401491\n",
      "[Epoch 0/200] [Batch 1310/1367] [D loss: 0.619272, acc:   6%] [G loss: 5.711673] time: 1:40:23.950025\n",
      "[Epoch 0/200] [Batch 1311/1367] [D loss: 0.064073, acc:  90%] [G loss: 8.506656] time: 1:40:28.531384\n",
      "[Epoch 0/200] [Batch 1312/1367] [D loss: 0.228669, acc:  85%] [G loss: 4.974990] time: 1:40:33.071839\n",
      "[Epoch 0/200] [Batch 1313/1367] [D loss: 0.188747, acc:  58%] [G loss: 4.777215] time: 1:40:37.632191\n",
      "[Epoch 0/200] [Batch 1314/1367] [D loss: 0.102445, acc:  91%] [G loss: 11.500004] time: 1:40:42.185709\n",
      "[Epoch 0/200] [Batch 1315/1367] [D loss: 0.229044, acc:  71%] [G loss: 14.852279] time: 1:40:46.771889\n",
      "[Epoch 0/200] [Batch 1316/1367] [D loss: 0.491586, acc:  48%] [G loss: 5.214271] time: 1:40:51.321443\n",
      "[Epoch 0/200] [Batch 1317/1367] [D loss: 0.195685, acc:  59%] [G loss: 12.617038] time: 1:40:55.880876\n",
      "[Epoch 0/200] [Batch 1318/1367] [D loss: 0.056732, acc:  98%] [G loss: 5.435340] time: 1:41:00.440264\n",
      "[Epoch 0/200] [Batch 1319/1367] [D loss: 0.106486, acc:  87%] [G loss: 7.600476] time: 1:41:04.994406\n",
      "[Epoch 0/200] [Batch 1320/1367] [D loss: 0.132303, acc:  88%] [G loss: 5.003248] time: 1:41:09.559750\n",
      "[Epoch 0/200] [Batch 1321/1367] [D loss: 0.128652, acc:  70%] [G loss: 4.126703] time: 1:41:14.139569\n",
      "[Epoch 0/200] [Batch 1322/1367] [D loss: 0.142319, acc:  71%] [G loss: 7.221439] time: 1:41:18.698388\n",
      "[Epoch 0/200] [Batch 1323/1367] [D loss: 0.310059, acc:  52%] [G loss: 6.292312] time: 1:41:23.238032\n",
      "[Epoch 0/200] [Batch 1324/1367] [D loss: 0.220741, acc:  44%] [G loss: 10.580627] time: 1:41:27.796518\n",
      "[Epoch 0/200] [Batch 1325/1367] [D loss: 0.329961, acc:  42%] [G loss: 8.169096] time: 1:41:32.364271\n",
      "[Epoch 0/200] [Batch 1326/1367] [D loss: 0.043668, acc:  97%] [G loss: 9.133005] time: 1:41:36.923557\n",
      "[Epoch 0/200] [Batch 1327/1367] [D loss: 0.064777, acc:  99%] [G loss: 8.657698] time: 1:41:41.498066\n",
      "[Epoch 0/200] [Batch 1328/1367] [D loss: 0.072275, acc:  98%] [G loss: 9.466801] time: 1:41:46.060353\n",
      "[Epoch 0/200] [Batch 1329/1367] [D loss: 0.116006, acc:  87%] [G loss: 9.203949] time: 1:41:50.625442\n",
      "[Epoch 0/200] [Batch 1330/1367] [D loss: 0.121564, acc:  86%] [G loss: 10.353261] time: 1:41:55.213194\n",
      "[Epoch 0/200] [Batch 1331/1367] [D loss: 0.080096, acc:  99%] [G loss: 11.477906] time: 1:41:59.783416\n",
      "[Epoch 0/200] [Batch 1332/1367] [D loss: 0.036015, acc:  98%] [G loss: 9.704227] time: 1:42:04.343989\n",
      "[Epoch 0/200] [Batch 1333/1367] [D loss: 0.032049, acc:  97%] [G loss: 8.431248] time: 1:42:08.915229\n",
      "[Epoch 0/200] [Batch 1334/1367] [D loss: 0.020155, acc:  98%] [G loss: 5.435637] time: 1:42:13.467356\n",
      "[Epoch 0/200] [Batch 1335/1367] [D loss: 0.073817, acc:  98%] [G loss: 12.768882] time: 1:42:18.040592\n",
      "[Epoch 0/200] [Batch 1336/1367] [D loss: 0.055697, acc:  99%] [G loss: 11.035200] time: 1:42:22.609738\n",
      "[Epoch 0/200] [Batch 1337/1367] [D loss: 0.016248, acc:  99%] [G loss: 4.979649] time: 1:42:27.177110\n",
      "[Epoch 0/200] [Batch 1338/1367] [D loss: 0.261526, acc:  51%] [G loss: 8.736940] time: 1:42:31.749172\n",
      "[Epoch 0/200] [Batch 1339/1367] [D loss: 0.084362, acc:  91%] [G loss: 9.546546] time: 1:42:36.312465\n",
      "[Epoch 0/200] [Batch 1340/1367] [D loss: 0.074577, acc:  91%] [G loss: 8.147492] time: 1:42:40.863033\n",
      "[Epoch 0/200] [Batch 1341/1367] [D loss: 0.080971, acc:  97%] [G loss: 7.516446] time: 1:42:45.405602\n",
      "[Epoch 0/200] [Batch 1342/1367] [D loss: 0.373338, acc:  26%] [G loss: 7.014145] time: 1:42:49.969173\n",
      "[Epoch 0/200] [Batch 1343/1367] [D loss: 0.115208, acc:  92%] [G loss: 12.396318] time: 1:42:54.562685\n",
      "[Epoch 0/200] [Batch 1344/1367] [D loss: 0.239257, acc:  53%] [G loss: 6.781642] time: 1:42:59.161976\n",
      "[Epoch 0/200] [Batch 1345/1367] [D loss: 0.343842, acc:  50%] [G loss: 6.612355] time: 1:43:03.707978\n",
      "[Epoch 0/200] [Batch 1346/1367] [D loss: 0.220455, acc:  79%] [G loss: 7.329832] time: 1:43:08.264623\n",
      "[Epoch 0/200] [Batch 1347/1367] [D loss: 0.137567, acc:  87%] [G loss: 10.738031] time: 1:43:12.838505\n",
      "[Epoch 0/200] [Batch 1348/1367] [D loss: 0.379548, acc:  43%] [G loss: 10.594936] time: 1:43:17.419587\n",
      "[Epoch 0/200] [Batch 1349/1367] [D loss: 0.103137, acc:  91%] [G loss: 11.624868] time: 1:43:21.983593\n",
      "[Epoch 0/200] [Batch 1350/1367] [D loss: 0.092540, acc:  95%] [G loss: 14.671620] time: 1:43:26.558881\n",
      "[Epoch 0/200] [Batch 1351/1367] [D loss: 0.474537, acc:  10%] [G loss: 17.338125] time: 1:43:31.122346\n",
      "[Epoch 0/200] [Batch 1352/1367] [D loss: 0.193976, acc:  70%] [G loss: 15.162738] time: 1:43:35.703707\n",
      "[Epoch 0/200] [Batch 1353/1367] [D loss: 0.333660, acc:  38%] [G loss: 14.995729] time: 1:43:40.286946\n",
      "[Epoch 0/200] [Batch 1354/1367] [D loss: 0.539848, acc:  47%] [G loss: 8.443787] time: 1:43:44.854746\n",
      "[Epoch 0/200] [Batch 1355/1367] [D loss: 0.182002, acc:  85%] [G loss: 11.708897] time: 1:43:49.432333\n",
      "[Epoch 0/200] [Batch 1356/1367] [D loss: 0.178436, acc:  68%] [G loss: 17.497610] time: 1:43:53.985268\n",
      "[Epoch 0/200] [Batch 1357/1367] [D loss: 0.188780, acc:  62%] [G loss: 10.139602] time: 1:43:58.553934\n",
      "[Epoch 0/200] [Batch 1358/1367] [D loss: 0.100679, acc:  97%] [G loss: 13.078154] time: 1:44:03.127797\n",
      "[Epoch 0/200] [Batch 1359/1367] [D loss: 0.085032, acc:  98%] [G loss: 11.428719] time: 1:44:07.720101\n",
      "[Epoch 0/200] [Batch 1360/1367] [D loss: 0.052584, acc:  98%] [G loss: 12.314002] time: 1:44:12.278091\n",
      "[Epoch 0/200] [Batch 1361/1367] [D loss: 0.150921, acc:  94%] [G loss: 9.438415] time: 1:44:16.824383\n",
      "[Epoch 0/200] [Batch 1362/1367] [D loss: 0.167792, acc:  85%] [G loss: 11.856822] time: 1:44:21.413325\n",
      "[Epoch 0/200] [Batch 1363/1367] [D loss: 0.100612, acc:  90%] [G loss: 13.173483] time: 1:44:26.013900\n",
      "[Epoch 0/200] [Batch 1364/1367] [D loss: 0.233469, acc:  66%] [G loss: 13.329961] time: 1:44:30.617203\n",
      "[Epoch 0/200] [Batch 1365/1367] [D loss: 0.120974, acc:  84%] [G loss: 13.027225] time: 1:44:35.212272\n",
      "[Epoch 0/200] [Batch 1366/1367] [D loss: 0.084680, acc:  93%] [G loss: 12.371405] time: 1:44:39.826130\n",
      "[Epoch 1/200] [Batch 0/1367] [D loss: 0.044278, acc:  97%] [G loss: 13.552712] time: 1:44:44.420762\n",
      "sampling\n",
      "[Epoch 1/200] [Batch 1/1367] [D loss: 0.291461, acc:  65%] [G loss: 8.182338] time: 1:44:50.230633\n",
      "[Epoch 1/200] [Batch 2/1367] [D loss: 0.029042, acc:  97%] [G loss: 9.145951] time: 1:44:54.801113\n",
      "[Epoch 1/200] [Batch 3/1367] [D loss: 0.103041, acc:  96%] [G loss: 7.636103] time: 1:44:59.383130\n"
     ]
    }
   ],
   "source": [
    "gan.train(epochs=200, batch_size=4, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = SiameseGanImageIterator(gan.generator, 'data/128x128/cropped/nude/backgrounds', \n",
    "                                                'data/128x128/cropped/nude/persons',\n",
    "                                                'data/128x128/cropped/dressed/backgrounds',\n",
    "                                                'data/128x128/cropped/dressed/persons',\n",
    "                                          batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = iterator.get_valid_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 128, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valids = iterator.get_valid_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=384x128 at 0x7F3001570908>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valids = iterator.get_valid_imgs()\n",
    "back = valids[1]\n",
    "truth = valids[0]\n",
    "generated = gan.generator.predict(back)[0]\n",
    "show_encoded_array(np.concatenate([back[0],generated, truth[0]], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "backed = generated\n",
    "for row in range(generated.shape[0]):\n",
    "    for pix in range(generated.shape[1]):\n",
    "        if int(generated[row][pix][0]*255) < 0.001 and int(generated[row][pix][1]*255) < 0.001 and int(generated[row][pix][2]*255) < 0.001:\n",
    "            backed[row][pix] = back[0][row][pix]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator.load_weights('generator.ckpt')\n",
    "gan.discriminator.load_weights('discriminator.ckpt')\n",
    "gan.combined.load_weights('combined.cktp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator.save('generator.ckpt')\n",
    "gan.discriminator.save('discriminator.ckpt')\n",
    "gan.combined.save('combined.cktp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
